[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "",
    "text": "Bem-vindo ao mini curso de Análise de Dados Funcionais com R! Neste curso, vamos explorar uma abordagem poderosa para lidar com conjuntos de dados complexos e dinâmicos, conhecidos como dados funcionais.\nA análise de dados funcionais oferece uma maneira flexível e abrangente de lidar com informações que evoluem ao longo do tempo ou em outros domínios contínuos. Essa abordagem é especialmente útil quando nossos dados são observações repetidas de uma mesma entidade ao longo de uma variável independente, como tempo, espaço ou qualquer outra dimensão contínua.\nNeste curso, baseamos nosso trabalho em um curso anterior da Universidade da Carolina do Norte, datado de 2016. No entanto, fizemos algumas adaptações significativas. Em vez de utilizar o R base e as funções de plotagem do pacote fda, escolhemos trabalhar com o pacote tidyverse, uma coleção de pacotes R que visa simplificar o processo de manipulação, transformação e visualização de dados.\nAo adotar o tidyverse, utilizaremos pacotes como dplyr e tidyr para realizar a manipulação e preparação dos dados de forma eficiente. Além disso, aproveitaremos os recursos do pacote ggplot2 para criar visualizações elegantes e informativas dos nossos dados funcionais.\nDurante o curso, você aprenderá os conceitos fundamentais da análise de dados funcionais, explorando métodos estatísticos, técnicas de visualização e interpretação dos resultados. Nossa abordagem prática permitirá que você aplique o conhecimento adquirido em situações reais, com exemplos e exercícios baseados em casos do mundo real.\nNão é necessário ter conhecimento prévio sobre análise de dados funcionais ou sobre o pacote tidyverse. Faremos uma introdução gradual a esses conceitos, permitindo que você acompanhe o curso independentemente do seu nível de experiência.\nEste mini curso é uma excelente oportunidade para expandir suas habilidades em análise de dados e R, adquirindo uma abordagem moderna e eficiente para lidar com dados funcionais. Esperamos que você aproveite essa jornada e que ela o capacite a aplicar esse conhecimento em seus próprios projetos e pesquisas.\nVamos começar explorando os fundamentos da análise de dados funcionais com R e o poder do tidyverse. Prepare-se para mergulhar em um mundo de dados dinâmicos e descobrir novas maneiras de extrair informações valiosas a partir deles!\n(link para o curso original)\n\nMódulo 1: Introdução aos Dados Funcionais\n\nMódulo 1-1: R e RStudio (não coloquei essa parte)\nMódulo 1-2: Introdução a conjuntos de dados\nMódulo 1-3: Introdução a conjuntos de dados (continuação)\nMódulo 1-4: Resumo e Discussão\n\n\n\nMódulo 2: Modelagem de Dados Funcionais com Expansões de Base Pré-definidas\n\nMódulo 2-1: Funções de base\nMódulo 2-2: Regressão linear em funções de base\nMódulo 2-3: Seleção do número de funções de base (validação cruzada)\nMódulo 2-4: Suavização com penalidade de rugosidade\nMódulo 2-5: Resumo e Discussão\n\n\n\nMódulo 3: Modelagem de Dados Funcionais usando Componentes Principais Funcionais\n\nMódulo 3-1: Análise de Componentes Principais Funcionais para dados altamente - frequentes\nMódulo 3-2: Análise de Componentes Principais Funcionais para dados funcionais esparsos\nMódulo 3-3: Gráficos interativos para Análise de Componentes Principais Funcionais\nMódulo 3-4: Resumo e Discussão\n\n\n\nMódulo 4: Modelos Lineares Funcionais\n\nMódulo 4-1: Regressão escalar em função\nMódulo 4-2: Regressão de função em escalar (visualização)\nMódulo 4-3: Regressão de função em função\nMódulo 4-4: Resumo, Discussão e Atividades em Grupo\n\n\n\nMódulo 5: Dados Funcionais Correlacionados\n\nMódulo 5-1: Análise de Dados Funcionais Longitudinais e Gráficos interativos\nMódulo 5-2: Discussão\n\n\n\nResposta dos exercícios propostos"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#instalando-os-pacotes-que-serão-utilizados",
    "href": "index.html#instalando-os-pacotes-que-serão-utilizados",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "1 Instalando os pacotes que serão utilizados",
    "text": "1 Instalando os pacotes que serão utilizados\n\nif(!require(pacman)) install.packages(c(\"pacman\", # instalando os pacotes necessários\n                                        \"tidyverse\",\n                                        \"lme4\",\n                                        \"mgcv\",\n                                        \"refund\",\n                                        \"face\",\n                                        \"fda\", \n                                        \"rgl\",\n                                        \"fields\",\n                                        \"refund.shiny\",\n                                        \"janitor\"))\n\nCarregando os pacotes simultaneamente utilizando o pacote pacman\n\npacman::p_load(tidyverse, lme4, mgcv, refund, face, fda, rgl, fields, refund.shiny, janitor)\n\nPacotes para estimação dos modelos\n\nlme4 - modelos lineares, lineares generalizados, e modelos mistos não lineares\nmgcv - modelos aditivos generalizados (mistos); suavização semi-paramétrica\nrefund - modelos de regressão usando dados funcionais\nface - para estimativa rápida de covariância para dados funcionais esparsos\nfda - para análise de dados funcionais\n\nPacotes para manipulação e visualização dos dados\n\ntidyverse - auxilia na importação, organização, manipulação e visualização de dados\nrgl - gráficos 3d\nrefund.shiny - gráficos interativos para análise de dados funcionais"
  },
  {
    "objectID": "index.html#introdução-aos-dados-funcionais-e-longitudinais",
    "href": "index.html#introdução-aos-dados-funcionais-e-longitudinais",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "2 Introdução aos Dados Funcionais e longitudinais",
    "text": "2 Introdução aos Dados Funcionais e longitudinais\nDados Funcionais: Dados funcionais são uma forma de representar informações observadas ao longo de uma dimensão contínua, geralmente o tempo. Em vez de ter observações pontuais em momentos específicos, os dados funcionais registram a evolução de uma variável ao longo de uma curva contínua, muitas vezes representada como uma função. Essa função pode ser descrita por uma série de pontos discretos ou através de uma representação contínua, como uma curva suave. Dados funcionais são frequentemente utilizados em áreas como análise de séries temporais, análise de imagens médicas, processamento de sinais, entre outras.\nDados Longitudinais: Dados longitudinais referem-se a dados coletados repetidamente de um mesmo indivíduo, objeto ou unidade de estudo ao longo do tempo. Nesse tipo de dado, o foco está na observação das mudanças que ocorrem em uma variável ou conjunto de variáveis ao longo de diferentes momentos. Os dados longitudinais permitem analisar tendências, padrões de crescimento, estabilidade ou mudanças em uma população ao longo do tempo. São comumente utilizados em estudos longitudinais, pesquisas de acompanhamento de indivíduos, estudos de coorte e estudos de desenvolvimento, entre outros.\nEm resumo, dados funcionais representam a evolução de uma variável em uma dimensão contínua, enquanto dados longitudinais referem-se a observações repetidas de uma mesma unidade de estudo ao longo do tempo. Ambos os tipos de dados são valiosos para análises estatísticas e científicas em diferentes áreas de estudo."
  },
  {
    "objectID": "index.html#dados-utilizados-no-curso",
    "href": "index.html#dados-utilizados-no-curso",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "3 Dados utilizados no curso",
    "text": "3 Dados utilizados no curso\nPara este curso, exploraremos e analisaremos os seguintes conjuntos de dados (onde encontrar):\n\nDados de imagem de tensor de difusão (“DTI” pacote refund)\nDados meteorológicos canadenses (“CanadianWeather” pacote fda)\nDados de crescimento de Berkeley (“growth” pacote fda)\nDados CD4 (“cd4” pacote refund)\n\n\n3.1 Dados de imagem de tensor de difusão\nOs dados de imagem de tensor de difusão (DTI - Diffusion Tensor Imaging) são amplamente utilizados em estudos sobre esclerose múltipla (EM) e em outras áreas de pesquisa neurocientífica. A DTI é uma técnica de ressonância magnética que permite avaliar a microestrutura e a conectividade das fibras nervosas no cérebro.\nNa esclerose múltipla, a DTI pode ser usada para detectar e quantificar as alterações nas propriedades de difusão da água nos tecidos cerebrais. Essas alterações podem ser indicativas de danos ou desmielinização das fibras nervosas, que são características da EM.\nOs principais parâmetros medidos na DTI incluem a difusão isotrópica (representada pela medida do coeficiente de difusão isotrópica - ADC) e a difusão anisotrópica (representada pelos valores de fração anisotrópica - FA). A FA é particularmente importante na DTI, pois fornece informações sobre a direcionalidade das fibras nervosas e sua integridade estrutural.\nAo analisar os dados de DTI na EM, os pesquisadores podem investigar as alterações na integridade das fibras nervosas, a presença de lesões ou placas desmielinizantes e o impacto dessas alterações na conectividade e no funcionamento cerebral dos pacientes com EM. Além disso, a DTI também pode ser usada para avaliar a progressão da doença ao longo do tempo e monitorar a eficácia de intervenções terapêuticas.\n\n# Os dados de DTI vem do pacote refund\ndata(\"DTI\")\nnames(DTI)\n\n[1] \"ID\"         \"visit\"      \"visit.time\" \"Nscans\"     \"case\"      \n[6] \"sex\"        \"pasat\"      \"cca\"        \"rcst\"      \n\n\nAqui vamos considerar apenas os casos completos, de pacientes com EM em suas prinmeiras visitas.\n\n# filtrando apenas os casos completos ----\nDTI2 <- DTI %>% \n  drop_na() %>% \n  filter(visit == 1 & case == 1)\n\n# verificando as dimensoes dos dados de CCA\n# F1 para mais infomações sobre o formato dos dados\ndim(DTI2$cca)\n\n[1] 66 93\n\n# arrumando os dados para o gráfico ----\nDTI2$cca %>% \n  t() %>% \n  as_tibble() %>% \n janitor::clean_names() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"x\")) %>% \n  \n  ggplot(aes(tract, value, group = name, colour = name))+ #gráfico de todos os 66 casos completos\n  geom_line()+\n  theme(legend.position = \"none\")+\n  labs(x=\"tract\",\n       y =\"Anisotropia Fracionária (AF)\",\n       title = \"imagem de tensor de difusão:CCA\")\n\n\n\n\n\n\n\n# gráfico da média dos dados ----\n\nDTI2$cca %>% \n  t() %>% \n  as_tibble() %>% \n janitor::clean_names() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"x\")) %>% \n  group_by(tract) %>% \n  mutate(\n    avg = mean(value)\n  ) %>% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 1)+\n  theme(legend.position = \"none\")+\n  labs(x=\"tract\",\n       y =\"Anisotropia Fracionária (AF)\",\n       title = \"imagem de tensor de difusão:CCA\")\n\n\n\n\n\n\n\n\nAlém das medidas de AF, o conjunto de dados DTI também inclui as pontuações do Teste de Adição Auditiva em Série (PASAT) dos pacientes, que medem suas funções cognitivas. Os gráficos a seguir ajudam a visualizar a relação entre as medições de AF e as pontuações do PASAT, codificadas por cores.\n\n#[code by R. Todd Ogden and Jeff Goldsmith]\n\ntract <- 1:93\ncolfct <- as.numeric(cut(DTI2$pasat, 40))\n\npar(mar=c(1,1,0,0), cex.axis=1, cex.lab=1)\nclrs <- rev(colorRampPalette(c(\"blue\", \"green\", \"yellow\", \"red\"))(40))    \n\nproj = persp(x = tract, y = seq(min(DTI2$pasat), max(DTI2$pasat), l=length(DTI2$pasat)),  z=t(DTI2$cca),\n  xlab=\"tract\", ylab=\"PASAT\", zlab=\"AF\", col=NA, border=NA,\n  ticktype = \"detailed\", axes=TRUE, theta=30, phi=30)\n\no <- rev(order(DTI2$pasat))\nfor(i in o){\n  lines(trans3d(x = tract, y=rep(DTI2$pasat[i], ncol(DTI2$cca)),  z=DTI2$cca[i,], pmat=proj), col=clrs[colfct[i]])\n}\n\n\n\n\n\n\n\n\nQuestões de interesse:\n\nComo as medições de AF ao longo do CCA variam entre os indivíduos com EM?\nQual é a associação entre medições de AF e funções cognitivas (pontuações PASAT)?\nO perfil típico de AF varia entre pacientes com EM e indivíduos saudáveis?\n\n\n\n3.2 Dados meteorológicos do Canadá\nO banco de dados Canadian Weather contém informações sobre as temperaturas e precipitações diárias registradas em 35 estações meteorológicas localizadas no Canadá. Os dados abrangem um período de 365 dias, fornecendo uma série temporal de temperaturas para cada estação (Ramsay and Silverman, 2002).\n\n# Arrumando os dados diários de temperatura para todas as estações\ndaily_data_temp <- CanadianWeather$dailyAv %>% \n  as_tibble() %>% \n  dplyr::select(contains(\"Temperature\")) %>% \n  rownames_to_column(var = \"day\") %>% \n  mutate(\n    day = as.numeric(day)\n  )  \n \n\n# Criando o gráfico dos dados de teperatura diária\ndaily_data_temp %>% \n  pivot_longer(cols = c(2:ncol(daily_data_temp))) %>% \n  group_by(day) %>% \n  ggplot(aes(day,value, color = name ))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Temperatura c°\", title = \"Temperaturas diárias\")\n\n\n\n\n\n\n\n# Criando o gráfico com o comportamento médio dos dados\ndaily_data_temp %>% \n  pivot_longer(cols = c(2:ncol(daily_data_temp))) %>% \n  group_by(day) %>% \n  mutate(\n    avg = mean(value)\n  ) %>% \n  ggplot(aes(day,value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(day, avg), color = \"red\", linewidth = 2, linetype = 1)+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Temperatura c°\", title = \"Temperaturas diárias - c/média\")\n\n\n\n\n\n\n\n\n\n# Arrumando os dados diários de preciptação para todas as estações\ndaily_data_prec <- CanadianWeather$dailyAv %>% \n  as_tibble() %>% \n  dplyr::select(contains(\"Precipitation\")) %>% \n  rownames_to_column(var = \"day\") %>% \n  mutate(\n    day = as.numeric(day)\n  )  \n\n\n# Criando o gráfico dos dados de precipitação diária\ndaily_data_prec %>% \n  pivot_longer(cols = c(2:ncol(daily_data_prec))) %>% \n  group_by(day) %>% \n  ggplot(aes(day,value, color = name ))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Precipitação mm\", title = \"Precipitação diária\")\n\n\n\n\n\n\n\n# Criando o gráfico com o comportamento médio dos dados\ndaily_data_prec %>% \n  pivot_longer(cols = c(2:ncol(daily_data_prec))) %>% \n  group_by(day) %>% \n  mutate(\n    avg = mean(value)\n  ) %>% \n  ggplot(aes(day,value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(day, avg), color = \"red\", linewidth = 2, linetype = 1)+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Precipitação mm\", title = \"Precipitação diária c/média\")\n\n\n\n\n\n\n\n\nQuestões de interesse:\n\nDescrever características que caracterizam os perfis diários de temperatura.\nQual é a associação entre a precipitação média anual e a temperatura diária no Canadá?\nE quanto à associação entre as precipitações diárias e os perfis de temperatura.\n\n\n\n3.3 Dados CD4\nAs células imunes (células CD4) são tipicamente atacadas pelo HIV (vírus da imunodeficiência humana); A contagem de células CD4 por mm de sangue é um substituto útil da progressão do HIV. O estudo Multicenter Aids Cohort (disponível no pacote de reembolso) coletou as contagens de células CD4 de 366 indivíduos afetados entre -18 e 42 meses desde a soroconversão (diagnóstico de HIV).\n\ndata(cd4)\nview(cd4)\n\ncd4_tidy <- cd4 %>%\n  as_tibble() %>% \n rowid_to_column(var = \"affected\") %>% \n  pivot_longer(cols = 2:62, names_to = \"months\", values_to = \"count-mm\" ) %>% \n  mutate(\n    months = as.numeric(months)\n  )\n\ncd4_tidy %>% \n  group_by(months) %>% \n  drop_na() %>% \n  ggplot(aes(months, `count-mm`, group = affected, color = affected))+\n  geom_line()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nQuestões de interesse:\nComo as contagens de CD4 variam ao longo do tempo na população, bem como no nível individual?\nDescreva as principais direções nas quais as contagens de CD4 variam.\n\n\n3.4 Dados de crescimento de Berkeley\nO conjunto de dados Berkeley Growth Data (também conhecido como Berkeley Growth Study) é um conjunto de dados histórico amplamente utilizado em pesquisas sobre crescimento e desenvolvimento infantil, contendo em alturas de 39 meninos e 54 meninas de 1 a 18 anos e as idades em que foram coletados. Esse conjunto de dados foi coletado como parte do Berkeley Growth Study, que ocorreu na Universidade da Califórnia, Berkeley, durante o século XX.\nO Berkeley Growth Data contém informações longitudinais de medidas antropométricas (como altura e peso) de um grande número de crianças desde o nascimento até a idade adulta. As medidas foram coletadas em intervalos regulares ao longo de vários anos, permitindo uma análise detalhada do crescimento e desenvolvimento ao longo do tempo.\n\ndata(\"growth\")\nview(growth)\n\n# Separando os dados referentes aos meninos\ngrowth_boy <- growth$hgtm %>%\n  as_tibble() %>% \n   mutate(\n     age = growth$age\n     )\n# Separando os dados referentes as meninas\ngrowth_girl <- growth$hgtf %>%\n  as_tibble() %>% \n  mutate(\n    age = growth$age\n  )\n\n# unificando os dois conuntos e criando uma variável gênero\ngrowth_df <- left_join(\n  growth_boy,\n  growth_girl,\n  by = \"age\") %>% \n  pivot_longer(\n    cols = contains(c(\"boy\",\"girl\")),\n    names_to = \"kids\",\n    values_to = \"height\") %>% \n   mutate(\n    gender = if_else(stringr::str_starts(kids, \"boy\"), \"M\", \"F\")\n  )\n\n\n# curva de crescimento \ngrowth_df %>%  \n   group_by(age) %>% \n  ggplot(aes(age, height, group = kids, color = gender))+\n  geom_line()\n\n\n\n\n\n\n\n# taxa de crescimento \ngrowth_df %>%  \n  group_by(kids, gender) %>% \n  mutate(\n    g_rate = c(0,diff(height))\n  ) %>%\n  ggplot(aes(age, g_rate, group = kids,color = gender))+\n  geom_line()\n\n\n\n\n\n\n\n\nQuestões de interesse:\nComo a altura varia em meninos e meninas?\nO género tem impacto no processo de crescimento de uma criança?"
  },
  {
    "objectID": "index.html#resumo",
    "href": "index.html#resumo",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "4 Resumo :",
    "text": "4 Resumo :\nIntrodução dos quatro conjuntos de dados com características funcionais (ou longitudinais).\nDiscutiu possíveis questões científicas para cada conjunto de dados."
  },
  {
    "objectID": "dia02.html#várias-funções-base",
    "href": "dia02.html#várias-funções-base",
    "title": "Dia-02",
    "section": "1 Várias funções base:",
    "text": "1 Várias funções base:\nAs funções base, também conhecidas como funções de base, são utilizadas em análise estatística e modelagem para descrever ou aproximar relações complexas entre variáveis. Elas são usadas como componentes de modelos estatísticos e permitem representar de forma flexível e adequada diferentes padrões de dados.\nAs funções base são frequentemente aplicadas em análise de regressão, onde o objetivo é encontrar uma função que relacione uma variável dependente a uma ou mais variáveis independentes. Elas formam uma base para representar o comportamento geral dos dados e permitem expressar as relações entre as variáveis de forma mais simples e interpretável.\nExistem diferentes tipos de funções base, sendo as mais comuns:\n\nFunções polinomiais: São funções base construídas a partir de polinômios. Geralmente são utilizadas funções polinomiais de grau baixo, como polinômios de primeira e segunda ordem, para ajustar curvas lineares e quadráticas, respectivamente.\nFunções splines: As funções splines dividem o domínio de interesse em segmentos menores e utilizam polinômios de grau baixo em cada segmento para modelar os dados. Isso permite uma flexibilidade maior na representação de padrões complexos e não lineares.\nFunções de onda: São funções que se repetem ao longo de um domínio e podem ser usadas para modelar fenômenos periódicos. Exemplos comuns são as funções seno e cosseno.\nFunções de base radial: São funções que dependem da distância entre pontos de dados e um centro definido. Essas funções são frequentemente utilizadas em problemas de interpolação ou para modelar dados com padrões radiais.\n\nAo escolher as funções base adequadas, é possível capturar diferentes características dos dados e ajustar modelos que se ajustem melhor aos padrões observados. A seleção correta das funções base é um aspecto importante na modelagem estatística, e depende do conhecimento do pesquisador sobre o problema em questão, bem como das características dos dados disponíveis.\n\n1.1 Biblioteca fda\nA biblioteca fda (Functional Data Analysis) é um pacote em R que fornece funcionalidades para análise de dados funcionais. A análise de dados funcionais é uma área da estatística que lida com dados observados como funções contínuas ao longo de uma dimensão, como séries temporais, curvas suaves ou imagens médicas.\nO fda permite a manipulação, visualização, modelagem e análise de dados funcionais de maneira eficiente. Alguns recursos e funcionalidades principais do fda incluem:\n\nRepresentação de dados funcionais: O pacote oferece estruturas de dados adequadas para representar e armazenar dados funcionais, como objetos fd (functional data) e fdata (functional data array).\nVisualização de dados funcionais: O fda inclui funções para plotagem e visualização de dados funcionais, permitindo a criação de gráficos de séries temporais, curvas suaves e outras representações visuais adequadas para dados funcionais.\nAnálise exploratória: O pacote fornece métodos estatísticos para análise exploratória de dados funcionais, incluindo medidas descritivas, como média, mediana e desvio padrão funcional, além de ferramentas para análise de variabilidade e análise de componentes principais funcionais.\nModelagem e ajuste de dados funcionais: O fda oferece métodos para modelar dados funcionais, incluindo regressão funcional, análise de covariância funcional e análise de séries temporais funcionais. Isso permite ajustar modelos estatísticos apropriados aos dados funcionais.\nAlinhamento e registro de dados funcionais: O pacote inclui funções para alinhar e registrar dados funcionais, que são úteis quando se deseja comparar ou combinar diferentes curvas ou séries temporais funcionais.\n\nA biblioteca fda é uma ferramenta poderosa para análise de dados funcionais e pode ser usada em uma variedade de aplicações, como medicina, economia, engenharia e muitas outras áreas em que os dados são naturalmente representados como funções contínuas.\n\n\n1.2 Bases Monomiais\nA função base monomial, do pacote fda, requer o domínio e o número da base. Por exemplo, a base monomial com K=6 funções de base definidas no intervalo [0,1] pode ser construída da seguinte forma.\n\nbbasis_obj = create.monomial.basis(rangeval=c(0,1), nbasis = 6)\n\nIsso retornará uma saída como “funções”. Para avaliar as bases criadas, em uma grade de s pontos, podemos:\n\nx <- seq(0,1,length.out=100) # grade de pontos\n\nbbasisevals <- eval.basis(x, bbasis_obj) # avaliando as bases na grade de pontos\n\nPara visualizar as bases monomiais:\n\nx1 <- x %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nbbasisevals1 <- bbasisevals %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nbasis_plot <- left_join(x1,bbasisevals1, by = \"rowname\")\n\n\nbasis_plot %>%\n    pivot_longer(\n    cols = starts_with(\"monomial\"),\n    names_to = \"degree\",\n    values_to = \"values\") %>%\n  group_by(value) %>% \n    ggplot(aes(value, values, group = degree, color = degree))+\n  geom_line()+\n  labs(x = \"X\", y = \"fns bases\", title = \"Bases monomiais com k=6\")\n\n\n\n\n\n\n\n\n\n\n1.3 Bases de Fourier\nA análise de Fourier requer que sejam definidos o domínio, o período de oscilação e o número de funções de base.\n\nx <- seq(0,1,length.out=100)\n\nfbasis_obj <- create.fourier.basis(rangeval=c(0,1), \n                                 nbasis=65, period = 1)\n\nfbasisevals <- eval.basis(x, fbasis_obj)\n\nPara visualizar as bases de Fourier:\n\nx1 <- x %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nfbasisevals1 <- fbasisevals %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nbasis_plot <- left_join(x1,fbasisevals1, by = \"rowname\")\n\n# plotando as bases de fourier\nbasis_plot %>%\n dplyr::select(2:5) %>% # escolhendo o número de bases no plot - 2:5 três bases\npivot_longer(\n    cols = starts_with(c(\"sin\",\"co\")),\n    names_to = \"degree\",\n    values_to = \"values\") %>%\n  group_by(value) %>% \n  ggplot(aes(value, values, group = degree, color = degree))+\n  geom_line()+\n  labs(x = \"X\", y = \"fns bases\", title = \"Três primeiras bases de Fourie\")\n\n\n\n\n\n\n\n\n\n\n1.4 Bases B-Spline\nA base B-spline requer o domínio, as funções de base numérica e o grau.\n\nx <- seq(0,1,length.out=100)\n\nbsbasis_obj <- create.bspline.basis(rangeval=c(0,1),\n                                    nbasis=10, norder=4)\n\nbsbasisevals <- eval.basis(x, bsbasis_obj)\n\nPara visualizar as bases de B-spline:\n\nx1 <- x %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nbsbasisevals1 <- bsbasisevals %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nbasis_plot <- left_join(x1,bsbasisevals1, by = \"rowname\")\n\n# plotando B-Spline basis\nbasis_plot %>%\ndplyr::select(2:12) %>% \n  pivot_longer(\n    cols = starts_with(\"bspl\"),\n    names_to = \"degree\",\n    values_to = \"values\") %>%\n  group_by(value) %>% \n  ggplot(aes(value, values, group = degree, color = degree))+\n  geom_line()+\n  labs(x = \"X\", y = \"fns bases\", title = \"B-spline cúbica com k=10\")+\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nTestando diferentes valores de nbasis e/ou norder.\n\n# norder =2\nx <- seq(0,1,length.out=100)\n\nbsbasis_obj <- create.bspline.basis(rangeval=c(0,1),\n                                    nbasis=10, norder=2)\n\nbsbasisevals <- eval.basis(x, bsbasis_obj)\n\nx1 <- x %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nbsbasisevals1 <- bsbasisevals %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nbasis_plot <- left_join(x1,bsbasisevals1, by = \"rowname\")\n\n# plotando B-Spline basis\nbasis_plot %>%\ndplyr::select(2:12) %>% \n  pivot_longer(\n    cols = starts_with(\"bspl\"),\n    names_to = \"degree\",\n    values_to = \"values\") %>%\n  group_by(value) %>% \n  ggplot(aes(value, values, group = degree, color = degree))+\n  geom_line()+\n  theme(\n    legend.position = \"none\"\n  )+\n  labs(x = \"X\", y = \"fns bases\", title = \"B-spline linear com k=10\")\n\n\n\n\n\n\n\n\n\n\n1.5 Outras bases\nO pacote fda também oferece a possibilidade de construir outros tipos de bases. Para visualizar a lista de bases disponíveis, você pode digitar ?create.+“tab”."
  },
  {
    "objectID": "dia02.html#tópicos-dia-2",
    "href": "dia02.html#tópicos-dia-2",
    "title": "Dia-02",
    "section": "Tópicos dia 2:",
    "text": "Tópicos dia 2:\n\nConstruir diferentes funções de base em R\nCriar dados funcionais\nRegressão linear em funções de base (OLS)\nComo escolher o número de funções de base?\nComo explicar a suavidade na curva subjacente?\n\nmétodos do kernel\nSuavização penalizada e seleção do parâmetro de suavização\n\n\n[Os Código são cortesia de Giles Hooker]"
  },
  {
    "objectID": "dia02.html#o-objeto-dados-funcionais-fd-do-pacote-fda",
    "href": "dia02.html#o-objeto-dados-funcionais-fd-do-pacote-fda",
    "title": "Dia-02",
    "section": "2 O objeto dados funcionais (FD) do pacote fda",
    "text": "2 O objeto dados funcionais (FD) do pacote fda\nDefinido por uma base de funções e um vetor de coeficientes aleatórios correspondentes, a função fd desempenha o papel de construtor de objetos de dados funcionais.\n\nnb <- 10\ncoef <- rnorm(nb)\nbsbasis_obj <- create.bspline.basis(rangeval=c(0,1),\n                                  nbasis=nb, norder=4)\n\nfd_obj <- fd(coef, bsbasis_obj)\nx <- seq(0,1,length.out = 100)\nfd_eval <- eval.fd(x, fd_obj)\n\nPara visualizar as bases de aleatórias:\n\nx1 <- x %>% \n  as_tibble() %>% \n  rownames_to_column()\n#\nfd_eval1 <- fd_eval %>% \n  as_tibble() %>% \n  rownames_to_column()\n#\nbasis_plot <- left_join(x1,fd_eval1, by = \"rowname\")\n#\n# plotando B-Spline basis\nbasis_plot %>%\n   pivot_longer(\n    cols = starts_with(\"reps\"),\n    names_to = \"degree\",\n    values_to = \"values\") %>%\n  group_by(value) %>% \n  ggplot(aes(value, values, group = degree, color = degree))+\n  geom_line()+\n  theme(\n    legend.position = \"none\"\n  )+\n  labs(x = \"X\", y = \"fns bases\", title = \"Função aleatória usando B-spline\")\n\n\n\n\n\n\n\n\nTente executar o código acima várias vezes. Tente também executar o código com um número diferente de funções base, por exemplo nb <- 5, nb <- 15, nb <- 30.\nPara gerar (n = 20) múltiplas funções aleatórias você pode criar uma matriz de coeficientes aleatórios com dimensão de #bases por n; por exemplo.\n\nn <- 20 ; nb <- 10\ncoef = matrix(rnorm(n*nb),nb,n)\n# dim(coefs)   # [1]  10 20\n\nbsbasis_obj <- create.bspline.basis(rangeval=c(0,1),\n                                    nbasis=nb, norder=4)\nfd_obj <- fd(coef, bsbasis_obj)\nx <- seq(0,1,length.out = 100)\nfd_eval <- eval.fd(x, fd_obj)\n\nPara visualizar as funções aleatórias:\n\nx1 <- x %>% \n  as_tibble() %>% \n  rownames_to_column()\n\n\nfd_eval1 <- fd_eval %>% \n  as_tibble() %>% \n  rownames_to_column()\n\n\nbasis_plot <- left_join(x1,fd_eval1, by = \"rowname\")\n\n\nbasis_plot %>%\n  # select(2:12) %>% # escolhendo o número de bases no plot - 2:5 três bases\n  pivot_longer(\n    cols = starts_with(\"reps\"),\n    names_to = \"degree\",\n    values_to = \"values\") %>%\n  group_by(value) %>% \n  ggplot(aes(value, values, group = degree, color = degree))+\n  geom_line()+\n  theme(\n    legend.position = \"none\"\n  )+\n  labs(x = \"X\", y = \"fns bases\", title = \"20 Funções aleatórias usando B-spline\")\n\n\n\n\n\n\n\n\nAlgumas outras funções úteis do pacote fda:\n\nderiv.fd: fornece a derivada de um objeto fd (o valor retornado também é um objeto fd); por exemplo. tente drfd_obj <- deriv.fd(fd_obj, 1); eval.fd(drfd_obj, x).\npodemos fazer aritmética com os objetos fd; por exemplo. eval.fd(fd_obj[1]+fd_obj[2], x)\noperações estatísticas; por exemplo. mean(fd_obj) ou std.fd(fd_obj)"
  },
  {
    "objectID": "dia02.html#linear-regression-on-basis-functions-ols",
    "href": "dia02.html#linear-regression-on-basis-functions-ols",
    "title": "Dia-02",
    "section": "3 Linear regression on basis functions (OLS)",
    "text": "3 Linear regression on basis functions (OLS)\nAqui, tentaremos usar diferentes funções básicas para suavizar os dados meteorológicos canadenses, que estão disponíveis no pacote fda.\n(Ilustrado com base de Fourier)\nVamos nos concentrar na precipitação média diária de Vancouver (transformação log), representada por apenas uma das curvas observadas.\n\ndata(\"CanadianWeather\")\n\n#str(CanadianWeather)\n\n#view(CanadianWeather$dailyAv)\n\nmonths <- row.names(CanadianWeather$dailyAv) %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nprecipitation_log <- CanadianWeather$dailyAv %>% \n  as_tibble() %>% \n dplyr::select(contains(c(\"log\"))) %>% \n  rownames_to_column() \n\n\nprecipitation_log <- left_join(months, precipitation_log, by = \"rowname\") %>% \n dplyr::select(-rowname)\n\nprecipitation_log %>% \ndplyr::select(value, dplyr::contains(\"Vancouver\")) %>% \n  separate_wider_position(value,c(month = 3,day=2)) %>% \n  mutate(\n    day_month = as.numeric(day),\n    day_year = 1:nrow(precipitation_log)\n  ) %>% \n  ggplot(aes(day_year, Vancouver.log10precip))+\n  geom_point()+\n  geom_line()+\n  labs(x = \"dia\", y = \"precipitação-log\", title = \"log da precipitação média diária de vancouver\")\n\n\n\n\n\n\n\n\nAgora vamos tentar suavizar os pontos observados usando a base de Fourier. Para usar a base de Fourier, precisamos definir o domínio, o período e o número da base.\n\nday <- 1:365\nrangval=range(day)\nperiod = 365 \n\nnbasis = 3 \nfbasis=create.fourier.basis(rangval, nbasis=nbasis, period=period)  \nbvals = eval.basis(day, fbasis)\nXbasis =bvals\n\nVamos ajustar um modelo de regressão linear utilizando as funções de base criadas (OLS).\n\nlm.fit = lm(Vancouver.log10precip ~ 0 + Xbasis, data = precipitation_log) \n\n# Visualizando o modelo\n\nlm.fit %>%\n broom::augment() %>% \n  mutate(\n    day_year = 1:365\n  ) %>% \n  ggplot(aes(day_year, `Vancouver.log10precip`))+\n  geom_point()+\n  geom_line(aes(day_year, .fitted), color = \"red\", linewidth = 2, linetype = 1)+\n  labs(x = \"dia\", y = \"precipitação-log\", title = \"Fourier-nbasis = 3\")\n\n\n\n\n\n\n\n\nCalculando a segunda derivada da curva ajustada.\n\nyfitfd = fd(lm.fit$coefficients,fbasis)  #obtain FD object\nyfit2D = eval.fd(day, yfitfd, 2) # evaluate the 2nd deriv. of the fit at day\n\nyfit2D %>% \n  bind_cols(day) %>%\n  ggplot(aes(`...2`, `...1` ))+\n  geom_line()+\n  theme_classic()+\n  labs(x = \"dia\", y = \"2D precipitação-log\", title = glue::glue(\"Média dos quadrados da 2D log-precp. = \", \n               round(mean(yfit2D^2),2)))\n\nNew names:\n• `` -> `...1`\n• `` -> `...2`"
  },
  {
    "objectID": "dia02.html#regressão-linear-em-funções-de-base-ols",
    "href": "dia02.html#regressão-linear-em-funções-de-base-ols",
    "title": "Dia-02",
    "section": "3 Regressão linear em funções de base (OLS)",
    "text": "3 Regressão linear em funções de base (OLS)\nAqui, tentaremos usar diferentes funções básicas para suavizar os dados meteorológicos canadenses, que estão disponíveis no pacote fda.\n(Ilustrado com base de Fourier)\nVamos nos concentrar na precipitação média diária de Vancouver (transformação log), representada por apenas uma das curvas observadas.\n\ndata(\"CanadianWeather\")\n\n#str(CanadianWeather)\n\n#view(CanadianWeather$dailyAv)\n\nmonths <- row.names(CanadianWeather$dailyAv) %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nprecipitation_log <- CanadianWeather$dailyAv %>% \n  as_tibble() %>% \n dplyr::select(contains(c(\"log\"))) %>% \n  rownames_to_column() \n\n\nprecipitation_log <- left_join(months, precipitation_log, by = \"rowname\") %>% \n dplyr::select(-rowname)\n\nprecipitation_log %>% \ndplyr::select(value, dplyr::contains(\"Vancouver\")) %>% \n  separate_wider_position(value,c(month = 3,day=2)) %>% \n  mutate(\n    day_month = as.numeric(day),\n    day_year = 1:nrow(precipitation_log)\n  ) %>% \n  ggplot(aes(day_year, Vancouver.log10precip))+\n  geom_point()+\n  geom_line()+\n  labs(x = \"dia\", y = \"precipitação-log\", title = \"log da precipitação média diária de vancouver\")\n\n\n\n\n\n\n\n\nAgora vamos tentar suavizar os pontos observados usando a base de Fourier. Para usar a base de Fourier, precisamos definir o domínio, o período e o número da base.\n\nday <- 1:365\nrangval=range(day)\nperiod = 365 \n\nnbasis = 3 \nfbasis=create.fourier.basis(rangval, nbasis=nbasis, period=period)  \nbvals = eval.basis(day, fbasis)\nXbasis =bvals\n\nVamos ajustar um modelo de regressão linear utilizando as funções de base criadas (OLS).\n\nlm.fit = lm(Vancouver.log10precip ~ 0 + Xbasis, data = precipitation_log) \n\n# Visualizando o modelo\n\nlm.fit %>%\n broom::augment() %>% \n  mutate(\n    day_year = 1:365\n  ) %>% \n  ggplot(aes(day_year, `Vancouver.log10precip`))+\n  geom_point()+\n  geom_line(aes(day_year, .fitted), color = \"red\", linewidth = 2, linetype = 1)+\n  labs(x = \"dia\", y = \"precipitação-log\", title = \"Fourier-nbasis = 3\")\n\n\n\n\n\n\n\n\nCalculando a segunda derivada da curva ajustada.\n\nyfitfd = fd(lm.fit$coefficients,fbasis)  #obtain FD object\nyfit2D = eval.fd(day, yfitfd, 2) # evaluate the 2nd deriv. of the fit at day\n\nyfit2D %>% \n  bind_cols(day) %>%\n  ggplot(aes(`...2`, `...1` ))+\n  geom_line()+\n  theme_classic()+\n  labs(x = \"dia\", y = \"2D precipitação-log\", title = glue::glue(\"Média dos quadrados da 2D log-precp. = \", \n               round(mean(yfit2D^2),2)))\n\nNew names:\n• `` -> `...1`\n• `` -> `...2`\n\n\n\n\n\n\n\n\n\nAgora tente o código com nbasis = 13, nbasis = 27 e nbasis = 365. Qual é o efeito de usar diferentes números de bases?"
  },
  {
    "objectID": "dia02.html#como-determinar-o-número-de-funções-de-base",
    "href": "dia02.html#como-determinar-o-número-de-funções-de-base",
    "title": "Dia-02",
    "section": "4 Como determinar o número de funções de base?",
    "text": "4 Como determinar o número de funções de base?\nEstudo de simulação (compensação de viés-variância)\nPara saber mais sobre o estudo de simulação, aqui estão os slides preparados pelo Dr. Davidian.\nO exercício de simulação a seguir ilustra a compensação de viés-variância. Vamos nos concentrar em observar o efeito de diferentes escolhas de número de funções de base no viés, variância e erro quadrático médio (MSE).\nO viés e a variância do estimador \\(\\hat{X}(t)\\) são dados por:\n\\[\n\\text{Bias}(\\hat{X}(t)) = E\\{\\hat{X}(t)\\} - X(t), e\n\\]\n\\[\n\\text{Var}(\\hat{X}(t)) =\\mathbb{E}\\left[ \\left( \\hat{X}(t) - \\mathbb{E}\\left[ \\hat{X}(t) \\right] \\right)^2 \\right],\n\\]\nrespectivamente. Para t fixo, o MSE é definido como:\n\\[\n\\text{MSE}\\left\\{\\hat{X}(t)\\right\\} = \\mathbb{E}\\left[ \\left( \\hat{X}(t) - \\mathbb{E}\\left[X(t) \\right] \\right)^2 \\right] = \\text{Bias}^2\\{\\hat{X}(t)\\} + \\text{Var}\\{\\hat{X}(t)\\}\n\\]\nPara uma curva \\(Y(\\cdot)\\) gostaríamos de selecionar o número de funções de base para minimizar o erro quadrado médio integrado, \\(\\int TMSE\\{\\hat{X}(t)\\} dt.\\)\nPodemos estudar essas estatísticas por meio de simulação. Neste caso, iremos gerar conjuntos de dados simulados que imitam as medições de precipitação realizadas em Vancouver \\((l=26)\\).\nA seguir está um breve esboço do experimento de simulação:\n\nSuavize as medições de precipitação de Vancouver, \\(Y(t_j)\\), usando B-spline e obtenha\n\n\na curva ajustada, \\(\\hat{X}_0(\\cdot)\\).\nOs resíduos, \\(\\epsilon_j = Y(t_j) - \\hat{X}_0(t_j)\\).\n\n\nAssuma que a curva ajustada, \\(\\hat{X}_0(\\cdot)\\), é a “verdadeira” curva subjacente, \\(X(\\cdot)\\).\nGere um conjunto de dados simulados adicionando resíduos, \\(epsilon^*_j\\), reamostrados aleatoriamente à curva subjacente “verdadeira”,\\(X(\\cdot)\\); em outras palavras, obtenha \\(Y^*_j\\) por meio da adição dos resíduos reamostrados à curva subjacente \\(Y^*_j = X(t_j) + \\epsilon^*_j\\).\n\nRepita o Passo 3 para gerar muitos conjuntos de dados simulados!\nAjuste cada conjunto de dados simulados usando uma base de Fourier com K funções de base para várias escolhas de K.\nPara cada K, calcule o viés, a variância e o MSE.\nA seguir, passarei pelo código de simulação para um conjunto de dados simulado. Primeiro, definimos a resposta de interesse.\n\ndata(\"CanadianWeather\")\n\n#str(CanadianWeather)\n\n#view(CanadianWeather$dailyAv)\n\nmonths <- row.names(CanadianWeather$dailyAv) %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nprecipitation <- CanadianWeather$dailyAv %>% \n  as_tibble() %>% \n  dplyr::select(contains(c(\"Precipitation\"))) %>% \n  rownames_to_column() \n\n\nprecipitation <- left_join(months, precipitation, by = \"rowname\") %>% \n  dplyr::select(-rowname)\n\nPlotando os dados de precipitação\n\nvancouver_precip <- precipitation %>% \n     dplyr::select(value, dplyr::contains(\"Vancouver\")) %>% \n      separate_wider_position(value,c(month = 3,day=2)) %>% \n      mutate(\n         day_year = 1:nrow(precipitation)\n      ) %>% \n      rename(\n        precipitation_mm = Vancouver.Precipitation.mm\n      ) %>% \n      janitor::clean_names() \n      \nvancouver_precip %>% \n  ggplot(aes(day_year, precipitation_mm))+\n  geom_point()\n\n\n\n\n\n\n\n\nEm seguida, construímos um estimador suave utilizando funções de base B-spline cúbicas penalizadas (Passo 1). Vamos considerar a linha preta como a ‘verdade’. Nesse caso, utilizamos a função gam (do pacote mgcv) para a suavização, a qual será explicada em breve.\n\nlibrary(mgcv)\nday <- 1:365\nfit <- gam(vancouver_precip$precipitation_mm ~s(day, k = 10, bs = \"cr\"), method=\"REML\")\n\nPor simplicidade vamos plotar essa curva ajustada usando o pacote ggplot2.\n\nvancouver_precip %>% \n  ggplot(aes(day_year, precipitation_mm))+\n  geom_point()+\n  geom_smooth(method = lm, formula = y ~ splines::bs(x, 10), se = FALSE)\n\n\n\n\n\n\n\n\nPara gerar um conjunto de dados simulados (Passos 2-3)\nVerificando os resíduos do modelo gerado\n\nfit %>% \nbroom::augment() %>% \n  ggplot(aes(.resid))+\n  geom_histogram(bins = 15)+\n  theme_light()\n\n\n\n\n\n\n\n\n\nplot_df <- fit %>% \n      broom::augment() %>%\n        as_tibble() %>% \n        janitor::clean_names()\n\nset.seed(420)\nplot_df %>% \n  mutate(\n    Eps.star = sample(resid, size = 365, replace = TRUE),\n    Y.star = fitted + Eps.star\n  ) %>% \n  ggplot(aes(day, Y.star))+\n  geom_point(alpha = 0.7)+\n  geom_smooth(method = lm, formula = y ~ splines::bs(x, 10), se = FALSE,linewidth = 2)+\n  geom_point(aes(day,vancouver_precip_precipitation_mm), color = \"red\", alpha = 0.7)+\n  geom_point(aes(day,fitted), color = \"green\")\n\n\n\n\n\n\n\n# salvando os dados simulados\nsimulated_precip<- plot_df %>% \n          mutate(\n            Eps.star = sample(resid, size = 365, replace = TRUE),\n            Y.star = fitted + Eps.star\n          )\n\nOs pontos em vermelho são os dados originais, em preto os simulados. A linha em azul é o modelo ajustado aos dados ariginais e a linha em verda o modelo ajustado aos dados simulados.\nUma vez que geramos o conjunto de dados simulados, faremos a suavização utilizando a função de base de Fourier com K funções de base, para várias escolhas de K. Aqui consideramos 7 valores diferentes; \\(K = 2 \\cdot \\text{c}(2:8) + 1\\)\n\nK.vec = 2*c(2:8)+1;    # we consider 7 different values of k.\n# K.vec\nt.day <- 1:365\nm <- length(plot_df$resid)\n\nfbasis=create.fourier.basis(rangeval = c(1, 365), nbasis=max(K.vec), period=365)\nbvals = eval.basis(t.day,fbasis)\n\n\nXfit = array(0, c(m, length(K.vec)))\nindex=0\nfor (K in K.vec){\n  index=index+1\n  Xbasis = bvals[, 1:K]\n  lm.fit = lm(Y.star~0+Xbasis, data = simulated_precip)\n  Xfit[,index] = as.vector(lm.fit$fitted.values)\n}\n\n\nXfit_df <- Xfit %>% \n  as_tibble() %>% \n  rename(\n    model_5 = V1,\n    model_7 = V2,\n    model_9 = V3,\n    model_11 = V4,\n    model_13 = V5,\n    model_15 = V6,\n    model_17 = V7\n  ) %>% \n  mutate(\n    day = 1:365\n  )\n\n\nmodels_plot <- left_join(simulated_precip, Xfit_df, by = \"day\") \n\nmodels_plot %>% \n dplyr::select(day,Y.star,vancouver_precip_precipitation_mm,contains(\"model\")) %>% \n  pivot_longer(cols = contains(\"model\")) %>% \n  group_by(day) %>% \n  ggplot(aes(day,value, group = name, color = name))+\n  geom_line(linewidth = 1.5)+\n  geom_point(aes(day, Y.star), color = \"blue\", alpha = 0.7)+\n  geom_point(aes(day, vancouver_precip_precipitation_mm), color = \"red\", alpha = 0.7)\n\n\n\n\n\n\n\n\nAgora, realizamos os passos acima para B=100 conjuntos de dados e calculamos o viés, a variância, o MSE (Erro Quadrático Médio) e o MSE integrado.\n\nK.vec = 2*c(2:8)+1; \nB=100 #B=10000\nm = length(simulated_precip$resid)\nXfit = array(0, c(B, m, length(K.vec)))\nt.day <- simulated_precip$day\n\nfbasis=create.fourier.basis(rangeval = c(1,365), nbasis=max(K.vec), period=365)\nbvals = eval.basis(t.day,fbasis)\n\n# ----\n\nset.seed0=1234\nfor(b in 1:B){#b=1\n  set.seed(set.seed0+b)\n\n  simulated_precip <- plot_df %>% \n      mutate( \n        Eps.star = sample(resid, size = 365, replace = TRUE),\n        Y.star = fitted + Eps.star\n        )\n  \n  # fit using Fourier basis and K basis functions\n  index=0\n  for (k in K.vec){\n    index=index+1\n    Xbasis = bvals[, 1:k]\n    lm.fit = lm(Y.star~0+Xbasis, data = simulated_precip)\n    Xfit[b,,index] = as.vector(lm.fit$fitted.values)\n  }\n}\n\n\nMean.Est = apply(Xfit, c(2,3), mean)\nMean.Est2 = apply(Xfit, c(2,3), function(x) mean(x^2))\n\nXtrue <- simulated_precip$fitted\n\nBias = apply(Mean.Est, 2, function(x) Xtrue-x)\nVar = Mean.Est2 - (Mean.Est)^2\nMse= Bias^2+Var\n\nMean_Bias2_L2 = apply(Bias^2, 2, mean) \nVar_L2 = apply(Var, 2, mean) \nMSE_L2 = apply(Mse, 2, mean)\n\nbases_resid<- bind_cols(bases_number = K.vec,\n          mean_basis2 = Mean_Bias2_L2,\n          var_L2 = Var_L2,\n          MSE_L2 = MSE_L2)\n\nbases_resid %>% \n  pivot_longer(cols = 2:4) %>% \n  group_by(bases_number) %>% \n  ggplot(aes(bases_number, value,group = name, color = name))+\n  geom_line()+\n  geom_point()+\n  labs(x = \"número de funções de base\", y = \"erro quadrático total\")\n\n\n\n\n\n\n\n\nCross-validation (Validação Cruzada)\nUma maneira de escolher o número de funções de base é a validação cruzada.\n[trecho dos apontamentos de aula :]\n\nDeixe de fora uma observação \\((t_j, Y_j)\\).\nAjuste os dados restantes e obtenha \\(\\hat{X}_j(t_j)\\) para várias escolhas de \\(K\\).\nEscolha \\(K\\) que minimize o escore de validação cruzada, definido como:\n\n\\[CV(X) = \\sum_{j=1}^{m} (Y_j - \\hat{X}_j(t_j))^2\\]\nEssas são as mesmas medições\n\nvancouver_precip <- precipitation %>% \ndplyr::select(value, dplyr::contains(\"Vancouver\")) %>% \n  separate_wider_position(value,c(month = 3,day=2)) %>% \n  mutate(\n    day_year = 1:nrow(precipitation)\n  ) %>% \n  rename(\n    precipitation_mm = Vancouver.Precipitation.mm\n  ) %>% \n  janitor::clean_names() \n\n\n#\nK.vec = 2*c(2:8)+1; m = length(vancouver_precip$day)\nCVfit = matrix(0,  nrow=m, ncol=length(K.vec))\ny <- vancouver_precip$precipitation_mm\nt.day <- vancouver_precip$day_year\n\n\nfbasis=create.fourier.basis(rangeval = c(1,365), nbasis=max(K.vec), period=365)\nbvals = eval.basis(t.day,fbasis)\n\nfor(j in 1:m){\n  \n  Y.star = y[-j]\n  \n  # fit using Fourier basis and K basis functions\n  index=0\n  for (K in K.vec){\n    index=index+1\n    Xbasis=bvals[, 1:K];\n    Xbasis.j =  Xbasis[-j, ]; \n    lm.fit = lm(Y.star~0+Xbasis.j); Xbasis.coeff = lm.fit$coefficients\n    y.fit = Xbasis%*%Xbasis.coeff\n    CVfit[j,index] = (y[j] - y.fit[j])^2\n  }\n}\n\nCV_L2 = apply(CVfit, 2, sum)\n\n\nplot_resid <- bind_cols(bases_number = K.vec,\n                        CV_L2 = CV_L2\n                        )\n\nplot_resid %>% \n  ggplot(aes(bases_number, CV_L2))+\n  geom_line()+\n  geom_point()+\n  labs(x=\"Numero de funções base\",\n       y=\"erro total da validação cruzada\",\n       title=\"K=13 com o menor valor da validação cruzada\")\n\n\n\n\n\n\n\n\nAqui nos concentramos nas medições médias de precipitação realizadas em Vancouver. (Atividade em grupo) Tente este exercício com as medições realizadas em Quebec \\((l = 10)\\). Ainda escolheria \\(K = 13\\) como o número ótimo de funções de base? E em relação a outros locais?\nVerificando para Quebec!\n\nquebec_precip <- precipitation %>% \n dplyr::select(value, dplyr::contains(\"Quebec\")) %>% \n  separate_wider_position(value,c(month = 3,day=2)) %>% \n  mutate(\n    day_year = 1:nrow(precipitation)\n  ) %>% \n  rename(\n    precipitation_mm = Quebec.Precipitation.mm\n  ) %>% \n  janitor::clean_names() \n\n\n#\nK.vec = 2*c(2:8)+1; m = length(quebec_precip$day)\nCVfit = matrix(0,  nrow=m, ncol=length(K.vec))\ny <- quebec_precip$precipitation_mm\nt.day <- quebec_precip$day_year\n\n\nfbasis=create.fourier.basis(rangeval = c(1,365), nbasis=max(K.vec), period=365)\nbvals = eval.basis(t.day,fbasis)\n\nfor(j in 1:m){\n  \n  Y.star = y[-j]\n  \n  # fit using Fourier basis and K basis functions\n  index=0\n  for (K in K.vec){\n    index=index+1\n    Xbasis=bvals[, 1:K];\n    Xbasis.j =  Xbasis[-j, ]; \n    lm.fit = lm(Y.star~0+Xbasis.j); Xbasis.coeff = lm.fit$coefficients\n    y.fit = Xbasis%*%Xbasis.coeff\n    CVfit[j,index] = (y[j] - y.fit[j])^2\n  }\n}\n\nCV_L2 = apply(CVfit, 2, sum)\n\n\nplot_resid <- bind_cols(bases_number = K.vec,\n                        CV_L2 = CV_L2\n)\n\nplot_resid %>% \n  ggplot(aes(bases_number, CV_L2))+\n  geom_line()+\n  geom_point()+\n  labs(x=\"Numero de funções base\",\n       y=\"erro total da validação cruzada\",\n       title=\"K=5 com o menor valor da validação cruzada\")\n\n\n\n\n\n\n\n\nComo verificamos o número ideal de funções base para Quebec é igual a 5.\n\nquebec_precip %>% \n  ggplot(aes(day_year, precipitation_mm))+\n  geom_point()+\n  geom_smooth(method = lm, formula = y ~ splines::bs(x, 5), se = FALSE)+\n  labs(x = \"dia\", y = \"precipitação mm\")"
  },
  {
    "objectID": "dia02.html#como-levar-em-consideração-a-suavidade-na-curva-subjacente",
    "href": "dia02.html#como-levar-em-consideração-a-suavidade-na-curva-subjacente",
    "title": "Dia-02",
    "section": "5 Como levar em consideração a suavidade na curva subjacente?",
    "text": "5 Como levar em consideração a suavidade na curva subjacente?\nSuavização com Penalidade de Rugosidade\nConforme discutido na palestra, uma maneira melhor de controlar o viés e a variância é definir a suavidade de uma função e controlá-la por meio do parâmetro de penalidade λ.\nPrimeiro, vamos analisar a curva suavizada para um parâmetro de penalidade específico. Aqui, novamente, usamos as medições de precipitação de Vancouver e construímos as funções de base b-spline. Observe a quantidade de funções de base que construímos! Sem a penalidade de rugosidade, o ajuste dos mínimos quadrados ordinários conecta cada uma das medições!\n\n# define domain, #knots, and #order to construct b-spline basis\nmonths <- row.names(CanadianWeather$dailyAv) %>% \n  as_tibble() %>% \n  rownames_to_column()\n\nprecipitation <- CanadianWeather$dailyAv %>% \n  as_tibble() %>% \n dplyr::select(contains(c(\"Precipitation\"))) %>% \n  rownames_to_column() \n\nprecipitation <- left_join(months, precipitation, by = \"rowname\") %>% \n dplyr::select(-rowname) \n\nvancouver_precip <- precipitation %>% \ndplyr::select(value, dplyr::contains(\"Vancouver\")) %>% \n  separate_wider_position(value,c(month = 3,day=2)) %>% \n  mutate(\n    day_year = 1:nrow(precipitation)\n  ) %>% \n  rename(\n    precipitation_mm = Vancouver.Precipitation.mm\n  ) %>% \n  janitor::clean_names() \n\n#\nt.day <- vancouver_precip$day_year\ny <- vancouver_precip$precipitation_mm\n\ny.basis  <- create.bspline.basis(rangeval = c(1,365), nbasis = 365, norder=4)\n\nbvals = eval.basis(t.day, y.basis)\nXbasis =bvals; \n\n\nlm.fit = lm(y ~ 0 + Xbasis)   \n\nplot_df <- cbind(day_year = t.day, prec_mm = y ,fitted = lm.fit$fitted.values) %>% \n  as_tibble()\n\n#\nplot_df %>% \n  ggplot(aes(day_year, prec_mm))+\n  geom_line(color = \"red\")+\n  geom_point(color = \"blue\")+\n geom_line(aes(day_year, fitted), color = \"green\")+\n  labs(x=\"dia\",y=\"precipitação mm\", title=\"365 bases de Fourier\")\n\n\n\n\n\n\n\n\nAgora vamos usar a penalidade de rugosidade λ e ver o seu efeito na suavidade do ajuste resultante.\n\nlambda <- 10^4\n\nint2Lfd(m): use isso para definir o termo de penalidade de derivada de ordem m\nfdPar(): define os parâmetros funcionais; neste caso, o termo de penalidade de segunda ordem da derivada e o parâmetro de suavização.\nybasis  <- create.bspline.basis(rangeval = c(1,365), nbasis = 365, norder=4)\n\ntD2fdPar = fdPar(y.basis, Lfdobj=int2Lfd(2), lambda=lambda)\n\nsmooth.basis():suaviza os dados usando a penalidade de rugosidade e o parâmetro de suavização especificados em tD2fdPar\n\ntyfd = smooth.basis(t.day,y,tD2fdPar) \n\nnames(tyfd) [1] “fd” “df” “gcv” “beta” “SSE” “penmat” “y2cMap”\n“argvals” “y”\n\nfd um objeto de dados funcionais contendo uma suavização dos dados.\ndf uma medida de graus de liberdade da suavização.\ngcv o valor do critério de validação cruzada generalizado ou GCV.\nbeta os coeficientes de regressão associados às variáveis covariáveis.\nSSE as somas dos quadrados do erro.\npenmat: a matriz de penalidade.\ny2cMap a matriz que mapeia os dados nos coeficientes: \\(\\left(\\Phi^T \\Phi + R\\right)^{-1} \\Phi^T\\)\n\n\nplot_df <- cbind(plot_df, smooth = tyfd$fd$coefs[,1]) \n\nplot_df %>% \n  ggplot(aes(day_year, prec_mm))+\n  geom_point(color = \"red\")+\n  geom_line(aes(day_year, smooth), color = \"green\",linewidth = 2)+\n  labs(x=\"dia\", y=\"precipitação mm\", title = \"Vancouver (lambda=10000)\")\n\n\n\n\n\n\n\n\nO ajuste resultante com o parâmetro de suavização \\(λ=104\\) está bastante suave! (Atividade em grupo) Experimente valores diferentes de \\(λ\\) \\((λ=0,0001, λ=100, λ=108)\\) e discuta!\n\nlambda <- 0.0001\ntD2fdPar = fdPar(y.basis, Lfdobj=int2Lfd(2), lambda=lambda)\ntyfd = smooth.basis(t.day,y,tD2fdPar) \n\n\nplot_df <- cbind(plot_df, smooth2 = tyfd$fd$coefs[,1]) \n\nplot_df %>% \n  ggplot(aes(day_year, prec_mm))+\n  geom_point(color = \"red\")+\n  geom_line(aes(day_year, smooth2), color = \"green\",linewidth = 2)\n\n\n\n\n\n\n\n\n\nlambda <- 100\ntD2fdPar = fdPar(y.basis, Lfdobj=int2Lfd(2), lambda=lambda)\ntyfd = smooth.basis(t.day,y,tD2fdPar) \n\n\nplot_df <- cbind(plot_df, smooth3 = tyfd$fd$coefs[,1]) \n\nplot_df %>% \n  ggplot(aes(day_year, prec_mm))+\n  geom_point(color = \"red\")+\n  geom_line(aes(day_year, smooth3), color = \"green\",linewidth = 2)\n\n\n\n\n\n\n\n\n\nlambda <- 10^8\ntD2fdPar = fdPar(y.basis, Lfdobj=int2Lfd(2), lambda=lambda)\ntyfd = smooth.basis(t.day,y,tD2fdPar) \n\n\nplot_df <- cbind(plot_df, smooth4 = tyfd$fd$coefs[,1]) \n\nplot_df %>% \n  ggplot(aes(day_year, prec_mm))+\n  geom_point(color = \"red\")+\n  geom_line(aes(day_year, smooth4), color = \"green\",linewidth = 2)\n\n\n\n\n\n\n\n\nExistem várias maneiras de selecionar o \\(λ\\) ótimo; por exemplo, validação cruzada (atividade extraclasse), validação cruzada generalizada (GCV), estimação da máxima verossimilhança restrita (ReML) e assim por diante. Aqui ilustramos a seleção do parâmetro de suavização usando o escore GCV. O escore GCV é definido como\n\\[\nGCV(\\lambda) = \\frac{m}{m-df(\\lambda)} \\times \\frac{SSE}{m-df(\\lambda)}\n\\]\nonde:\nm é o número total de observações nos dados, \\(df(\\lambda)\\) é a medida de graus de liberdade da suavização para o parâmetro de \\(suavização \\lambda\\), e SSE é a soma dos quadrados do erro.\nGeralmente, o GCV resulta em um ajuste mais suave do que o CV porque a qualidade do ajuste (medida pela SSE) é penalizada tanto pelos graus de liberdade df(λ) quanto pelo parâmetro de suavização λ.\nO código a seguir usa o GCV para selecionar o λ ótimo, considerando 71 valores candidatos.\n\nlogl=seq(-5, 12, len=71)  \n#range(exp(logl))\n\ngcv = rep(0,71)\n\nfor(i in c(1:length(logl))){\n  lambda=exp(logl[i])\n  \n  tD2fdPar = fdPar(y.basis,Lfdobj=int2Lfd(2),lambda=lambda)\n  tyfd = smooth.basis(t.day,y,tD2fdPar)\n  \n  gcv[i] = tyfd$gcv\n}\n\n# PLOT GCV of FIT versus log lambda\n\ngcv %>% \n  as_tibble_col() %>% \n  mutate(\n    logl = logl\n  ) %>% \n  ggplot(aes(logl, value))+\n  geom_line()+\n  labs(x=\"log lambda\", y=\"GCV\", title = \"GCV(log.lambda)\")\n\n\n\n\n\n\n\n\nO parâmetro de suavização ótimo selecionado usando o GCV é 992.2747156 e o ajuste resultante é apresentado abaixo.\n\nindex.logl.opt = which(gcv==min(gcv))\nlambda.opt = exp(logl[index.logl.opt])\n\ntD2fdPar = fdPar(y.basis,Lfdobj=int2Lfd(2),lambda=lambda.opt)\ntyfd = smooth.basis(t.day,y,tD2fdPar)\n\n\nplot_df <- cbind(day_year = t.day, prec_mm = y ,fitted = lm.fit$fitted.values) %>% \n  as_tibble()\n\nplot_df <- cbind(plot_df, best_smooth = tyfd$fd$coefs[,1]) %>% \n  as_tibble()\n\nplot_df %>% \n  ggplot(aes(day_year, prec_mm))+\n  geom_point()+\n  geom_line(aes(day_year, best_smooth), color= \"green\", linewidth = 2)+\n  labs(x = \"dai\", y=\"precipitação mm\", title = \"parâmetro de suavização ótimo = 992\")\n\n\n\n\n\n\n\n\nOutras funções de suavização disponíveis no R\n\nA função gam no pacote mgcv possui diversas funções de base e métodos de seleção.\n\npara o parâmetro de suavização. É flexível e útil!\nVamos utilizar esta função nos capítulos seguintes!\nExemplo (atividade extraclasse):\n\n\n\nlibrary(mgcv)\n\nfit = gam(y ~ s(t.day, k = 30, bs = \"cr\"), method = 'REML')\n\nfit %>% \nbroom::augment() %>% \n  ggplot(aes(t.day,y))+\n  geom_point()+\n  geom_line(aes(t.day, .fitted), color=\"green\", linewidth = 1.5)+\n  ggtitle(\"usando a função gam (REML)\")\n\n\n\n\n\n\n\n\n\nFunção “ksmooth” (suavização por regressão do núcleo)\n\nExemplo (atividade extraclasse): experimente um núcleo gaussiano (“normal”) e diferentes larguras de banda (0.1, 10, 30).\n\n\n\nfit2 = ksmooth(t.day, y, kernel = c(\"box\"), bandwidth = 10)\n\nfit %>% \n broom::augment() %>% \n  mutate(\n    ksmooth_box = fit2$y\n  ) %>% \n  ggplot(aes(t.day,y))+\n  geom_point()+\n  geom_line(aes(t.day, ksmooth_box), color=\"red\", linewidth = 1.5)+\n  ggtitle(\"using ksmooth function (box)\")\n\n\n\n\n\n\n\n#\n#\n# kernel (“normal”) - bandwidth = 10\n\nfit3 = ksmooth(t.day, y, kernel = c(\"normal\"), bandwidth = 10)\n\nfit %>% \nbroom::augment() %>% \n  mutate(\n    ksmooth_normal = fit3$y\n  ) %>% \n  ggplot(aes(t.day,y))+\n  geom_point()+\n  geom_line(aes(t.day, ksmooth_normal), color=\"red\", linewidth = 1.5)+\n  ggtitle(\"using ksmooth function (normal|10)\")\n\n\n\n\n\n\n\n#\n#\n# kernel (“normal”) - bandwidth = 0.1\n\nfit4 = ksmooth(t.day, y, kernel = c(\"normal\"), bandwidth = 0.1)\n\nfit %>% \nbroom::augment() %>% \n  mutate(\n    ksmooth_normal = fit4$y\n  ) %>% \n  ggplot(aes(t.day,y))+\n  geom_point()+\n  geom_line(aes(t.day, ksmooth_normal), color=\"red\", linewidth = 1.5)+\n  ggtitle(\"using ksmooth function (normal|0.1)\")\n\n\n\n\n\n\n\n#\n# kernel (“normal”) - bandwidth = 0.1\n\nfit5 = ksmooth(t.day, y, kernel = c(\"normal\"), bandwidth = 30)\n\nfit %>% \nbroom::augment() %>% \n  mutate(\n    ksmooth_normal = fit5$y\n  ) %>% \n  ggplot(aes(t.day,y))+\n  geom_point()+\n  geom_line(aes(t.day, ksmooth_normal), color=\"red\", linewidth = 1.5)+\n  ggtitle(\"using ksmooth function (normal|30)\")"
  },
  {
    "objectID": "dia03.html",
    "href": "dia03.html",
    "title": "Dia-03",
    "section": "",
    "text": "FPCA para dados altamente frequentes (dados de imagens de tensor de difusão)\n\nEstimação das funções média e covariância (suavização de cada curva / média ponto a ponto)\nDecomposição espectral das funções de covariância (escalonamento!)\n\nFPCA para dados ruidosos e esparsos (dados de contagem de células CD4)\nVisualização usando refund.shiny (dados de clima canadense)\n\n\n\n\n\nModelo FPCA\n\n\\[\nY_i(t) = \\mu(t) + \\sum_{k=1}^{n_{pc}} \\xi_{ik} \\phi_k(t) + \\epsilon_i(t)\n\\]\nAqui vou usar o conjunto de dados de imagens de tensor de difusão (DTI) para ilustrar a análise de componentes principais funcionais para dados altamente frequentes com pequenos ruídos (sem dados faltantes).\nLembrando que, como discutido no Dia 1, iremos nos concentrar na Anisotropia Fracional (FA) ao longo do trato do corpo caloso (CCA) coletada de pacientes com esclerose múltipla (MS) sem nenhum valor faltante.\n\nDTI &lt;- DTI %&gt;% \n  drop_na() %&gt;% \n  filter(visit == 1 & case == 1)\n\n\ncca_data &lt;- DTI$cca %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  )\n\ngráfico da média dos dados\n\ncca_data %&gt;% \n  pivot_longer(cols = starts_with(\"x\")) %&gt;% \n  group_by(tract) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 2)+\n  theme(legend.position = \"none\")+\n  ylab(\"Anisotropia fracional (AF)\")+\n  ggtitle(\"Imagem de Tensor de Difusão:CCA\")\n\n\n\n\n\n\n\n\n\n\nSuavize cada curva e calcule a média ponto a ponto.\n\nlibrary(mgcv)\n\nsmooth.curves &lt;- array(0, dim(DTI$cca))\nn &lt;- nrow(DTI$cca)\ntract &lt;- 1:93\n\nfor(j in 1:n){\n  # j = 1\n  fit &lt;- gam(DTI$cca[j,] ~ s(tract, k = 10, bs = 'cr'), method = \"REML\")\n  # plot(tract, DTI.baseline$cca[j,])\n  # lines(tract, fit$fitted)\n  smooth.curves[j,] &lt;- fit$fitted\n}\n\n\nsmooth.curves %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nsmooth.curves %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 2)+\n  theme(legend.position = \"none\")+\n  ylab(\"Anisotropia Fracional (FA)\")+\n  ggtitle(\"Imagem de Tensor de Difusão:CCA\")\n\n\n\n\n\n\n\n\nObserve que ambas as abordagens fornecem exatamente a mesma função média!\n\n\n\nSuavize cada curva e calcule a covariância amostral.\n\nlibrary(RColorBrewer)\n\nsmooth.cov &lt;- cov(smooth.curves)\n\nsmooth.cov %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  ggplot(aes(tract, name, fill = value)) + \n  geom_tile()+\n  scale_fill_gradientn(colors = brewer.pal(11, \"RdBu\")) +\n  theme(axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())+\n  labs(y=tract,title = \"Covariância suavizada de Análise Fatorial (AF).\")\n\n\n\n\n\n\n\n\n\n\n\nA partir da decomposição espectral da função de covariância estimada, podemos obter as eigenfunções e os autovalores estimados.\n\nsvd.result0 &lt;- eigen(smooth.cov, symmetric = TRUE)\n# names(svd.result0)\n\nevectors &lt;- svd.result0$vectors[,svd.result0$values &gt; 0]\nevalues &lt;- svd.result0$values[svd.result0$values &gt; 0]\n\nhead(colSums(evectors^2)) # returns unitary vectors \n\n[1] 1 1 1 1 1 1\n\n#Como as eigenfunções retornam vetores unitários, precisamos escalá-los por sqrt(93) (raiz quadrada de 93).\n\nefns0 &lt;- evectors*sqrt(93)\nevals0 &lt;- evalues/93\npve &lt;- cumsum(evals0)/sum(evals0)\nnpc &lt;- sum(pve &lt; 0.95) + 1\n\n# Componentes eigen estimados truncados.\nefns &lt;- efns0[,1:npc]\nevals &lt;- evals0[1:npc]\n\nO gráfico de “scree” está apresentado abaixo:\n\npve[1:20] %&gt;% # existem 48 componentes / só estou usando as 20 primeiras\n  t() %&gt;% \n  as_tibble() %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  mutate(\n    n_pcs = 1:20\n  ) %&gt;%\n  ggplot(aes(n_pcs,value))+\n  geom_line()+\n  geom_point()+\n  geom_hline(yintercept = 0.95, color = \"red\", linetype = 2)+\n  labs(x= \"número de CPs\", y=\"% de variação explicada\", title = \"Scree plot\")\n\n\n\n\n\n\n\n\nCom base no gráfico sabemos que os primeiros 5 componentes principais explicam mais de 95% das variabilidades nos dados.\n\nefns[,1:5] %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;%\n  ggplot(aes(tract,value,group = name,color = name))+\n  geom_line()+\n  labs(y=\"autofunção\", title = \"5 primeiras autofunções\")\n\n\n\n\n\n\n\n\nPodemos também visualizar o efeito de cada componente principal traçando a função \\(\\mu(t) \\pm 2\\sqrt{\\lambda_k}\\phi_k(t)\\).\n\nk.pc &lt;- 1\neffect &lt;- efns[, k.pc]*2*sqrt(evals[k.pc])\nmean.hat &lt;- colMeans(smooth.curves)\nmat &lt;- cbind(mean.hat - effect,  mean.hat + effect)\n\n\nefns[, k.pc] %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;%\n  ggplot(aes(tract,value))+\n  geom_line()+ggtitle(\"fPC1\")+\n  ylim(-2,2)\n\n\n\n\n\n\n\nmat %&gt;% \n  as_tibble() %&gt;% \n  cbind(pc_1 = mean.hat, tract = 1:93) %&gt;% \n  pivot_longer(cols = 1:3) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line() +\n  ggtitle(glue::glue(\"fPC\",k.pc, \" (\",round(pve[k.pc]*100),\"%)\"))\n\n\n\n\n\n\n\n\n(Atividade em grupo) O que descobrimos a partir dos resultados da fPCA? Quanta variabilidade dos dados foi explicada pela primeira fPC? Que características das curvas a primeira fPC explica? Tente fazer os gráficos para a segunda e terceira fPCs e seus efeitos em relação à média geral.\n\nk.pc &lt;- 2\neffect &lt;- efns[, k.pc]*2*sqrt(evals[k.pc])\nmean.hat &lt;- colMeans(smooth.curves)\nmat &lt;- cbind(mean.hat - effect,  mean.hat + effect)\n\n\nefns[, k.pc] %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;%\n  ggplot(aes(tract,value))+\n  geom_line()+ggtitle(glue::glue(\"fPC\",k.pc,))\n\n\n\n\n\n\n\nmat %&gt;% \n  as_tibble() %&gt;% \n  cbind(pc_1 = mean.hat, tract = 1:93) %&gt;% \n  pivot_longer(cols = 1:3) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line() +\n  ggtitle(glue::glue(\"fPC\",k.pc, \" (\",round(pve[k.pc]*100),\"%)\"))\n\n\n\n\n\n\n\n#################################\n# Pc3\n\nk.pc &lt;- 3\neffect &lt;- efns[, k.pc]*2*sqrt(evals[k.pc])\nmean.hat &lt;- colMeans(smooth.curves)\nmat &lt;- cbind(mean.hat - effect,  mean.hat + effect)\n\n\nefns[, k.pc] %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;%\n  ggplot(aes(tract,value))+\n  geom_line()+ggtitle(glue::glue(\"fPC\",k.pc,))\n\n\n\n\n\n\n\nmat %&gt;% \n  as_tibble() %&gt;% \n  cbind(pc_1 = mean.hat, tract = 1:93) %&gt;% \n  pivot_longer(cols = 1:3) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line() +\n  ggtitle(glue::glue(\"fPC\",k.pc, \" (\",round(pve[k.pc]*100),\"%)\"))\n\n\n\n\n\n\n\n\n\n\n\nOs escores estimados podem ser obtidos calculando \\(\\hat{\\xi}_{ik} = \\int_{T} \\hat{\\phi}^k(t) \\{Y_i(t) - \\hat{\\mu}(t)\\} dt\\).\nE as curvas ajustadas são dadas por\n\\[\n\\hat{Y}_i(t) = \\hat{\\mu}(t) + \\sum_{k=1}^{npc} \\hat{\\xi}_{ik} \\hat{\\phi}_k(t).\n\\]\n\ndemeaned &lt;- DTI$cca - t(matrix(rep(mean.hat, n),\n                                        nrow=length(mean.hat)))\n\nscores &lt;- matrix(NA, nrow=n, ncol=npc)\nfitted &lt;- array(NA, dim(DTI$cca))\n\nfor(i in 1:n){\n  scores[i,] &lt;- colMeans(matrix(rep(demeaned[i,], npc), nrow=93) * efns)\n  fitted[i,] &lt;- mean.hat + scores[i,]%*%t(efns)\n}\n\n\nrbind( DTI$cca[1:3,], fitted[1:3,])%&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;%\n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = contains(\"2\"), values_to = \"c_value\") %&gt;% \n  pivot_longer(cols = starts_with(\"V\"), values_to = \"values_smoo\", names_to = \"name_smoo\") %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract,c_value, group = name, color=name))+\n  geom_point()+\n  geom_line(aes(tract,values_smoo,group = name_smoo, color = name_smoo), linewidth = 1.5)\n\n\n\n\n\n\n\n\n\n\n\nExistem várias funções em R que implementam o método fPCA para dados densamente observados (com/sem ruído):\n\nfpca.face, fpca.ssvd e fpca2s do pacote “refund”.\n\nDesenvolvidas especificamente para dados funcionais densos.\nNão são aplicáveis a dados funcionais esparsos.\nfpca.ssvd e fpca2s exigem a especificação do número de componentes principais (npc); não é possível selecionar o número de componentes principais com base na proporção da variância explicada (PVE, sigla em inglês).\n\n\n\ntract &lt;- 1:93\n\nres.face &lt;- fpca.face(Y = DTI$cca, argvals = tract, pve = 0.95)\n\nnames(res.face)\n\n[1] \"Yhat\"       \"Y\"          \"scores\"     \"mu\"         \"efunctions\"\n[6] \"evalues\"    \"npc\"       \n\nefn.face &lt;- res.face$efunctions*sqrt(93)\neval.face &lt;- res.face$evalues/93\n\n\nefn.face %&gt;% \n  as_tibble() %&gt;% \n  rownames_to_column(var = \"tract\") %&gt;% \n  mutate(\n    tract = as.numeric(tract)\n  ) %&gt;% \n  dplyr::select(1:6) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name,  color = name))+\n  geom_line()+\n  labs(x=\"tract\", y=\"\", title = \"5 primeiras autofunções\")\n\n\n\n\n\n\n\n\n\nk.pc &lt;- 1\nmu.hat &lt;- res.face$mu\neffect &lt;- efn.face[,k.pc] * 2* sqrt(eval.face[k.pc])\npve.face &lt;- (cumsum(eval.face)/sum(eval.face))\n\npc1_plot &lt;- cbind(tract, efn.face[,k.pc]) %&gt;% \n  as_tibble()\n\npc1_plot %&gt;% \n  ggplot(aes(tract, V2))+\n  geom_line()+\n  ylim(c(-2,2))\n\n\n\n\n\n\n\nmu.hat %&gt;% \n  as_tibble_col(column_name = \"mu.hat\") %&gt;% \n  mutate(\n    effect_Plus = mu.hat + effect,\n    effect_less = mu.hat - effect,\n    tract = tract\n  ) %&gt;% \n  pivot_longer(cols = 1:3) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line()+\n  ggtitle(glue::glue(\"fPC\", k.pc,\"(\", round(pve.face[k.pc]*100) ,\"%)\"))\n\n\n\n\n\n\n\nyhat_smooth &lt;- res.face$Yhat %&gt;% \n      t() %&gt;% \n      as_tibble() %&gt;% \n     dplyr::select(1:3) %&gt;% \n      pivot_longer(cols = contains(\"_1\")) %&gt;% \n      rownames_to_column(var = \"tract\") %&gt;% \n      mutate(\n        tract = as.numeric(tract)\n      ) %&gt;% \n dplyr::select(-name)\n\nDTI$cca %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \ndplyr::select(1:3) %&gt;% \n  pivot_longer(cols = contains(\"_1\")) %&gt;% \n  rownames_to_column(var = \"tract\") %&gt;% \n  mutate(\n    tract = as.numeric(tract)\n  ) %&gt;% \n  left_join(yhat_smooth, by = \"tract\") %&gt;% \n  ggplot(aes(tract, value.x, group = name, color = name))+\n  geom_point(alpha = 0.7)+\n  geom_line(aes(tract,value.y), linetype = 1, linewidth = 1.5)\n\n\n\n\n\n\n\n\nPara usar as funções fpca.ssvd e fpca2s, confira os seguintes códigos de exemplo:\n\nres.ssvd &lt;- fpca.ssvd(Y = DTI.baseline$cca, npc = 5)\nres.2s &lt;- fpca2s(Y = DTI.baseline$cca, npc = 5, argvals = tract)\n\nSempre verifique se os eigencomponentes e escores resultantes estão corretamente escalados!\n\n\n\n\nQuando queremos realizar a FPCA em dados funcionais esparsos e ruidosos, como o conjunto de dados de contagem de CD4…\n\ndata(cd4)\n\ncd4_tidy &lt;- cd4 %&gt;%\n  as_tibble() %&gt;% \n  rowid_to_column(var = \"affected\") %&gt;% \n  pivot_longer(cols = 2:62, names_to = \"months\", values_to = \"count-mm\" ) %&gt;% \n  mutate(\n    months = as.numeric(months)\n  )\n\ncd4_tidy %&gt;% \n  group_by(affected, months) %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(months, `count-mm`, group = affected))+\n  geom_line(color = \"grey\")\n\n\n\n\n\n\n\nset.seed(123)\nsampled &lt;- sample(cd4_tidy$affected, size = 5, replace = FALSE)\n\ncd4_tidy %&gt;% \n  filter(affected %in%  sampled) %&gt;% \n  mutate(\n    affected = as_factor(affected)\n  ) %&gt;% \n  group_by(affected, months) %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(months, `count-mm`, group = affected, color = affected))+\n  geom_line()+\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nEstimativa da média através da combinação dos dados\nDevido ao fato de haver apenas algumas medições repetidas da contagem de células CD4 de cada sujeito, as duas abordagens para estimar a função média (e covariância) para dados funcionais densos não são apropriadas. Portanto, a abordagem comum para dados funcionais esparsos é combinar todas as medições.\nPor exemplo, para estimar a função média,\n\nn &lt;- nrow(cd4)\nmonth &lt;- as.numeric(colnames(cd4)) # months -18 and 42 since seroconversion\nm &lt;- ncol(cd4)\n\n\nlibrary(mgcv)\n\n# use all measurements\ndat.vec &lt;- data.frame(na.omit(cbind(rep(1:n, each = length(month)), \n                                    rep(month, n), as.vector(t(cd4)))))\ncolnames(dat.vec) &lt;- c(\"id\", \"t\",\"y\")\nfit &lt;- gam(y ~ s(t, k = 10, bs = \"cr\"), \n           method=\"REML\", data = dat.vec)\nmean.hat &lt;- predict(fit, newdata = data.frame(\"t\" = month))\n\nmeand_df &lt;- cbind(months =  unique(cd4_tidy$months), mean.hat) %&gt;% \n  as_tibble()\n\n\nmeand_df %&gt;% \n  ggplot(aes(months, mean.hat))+\n  geom_line()+\n  ylim(c(0,3000))\n\n\n\n\n\n\n\n\nDa mesma forma, podemos estimar a função de covariância combinando todas as medições.\n\n\n\nExistem vários softwares que já foram desenvolvidos para implementar o FPCA para dados funcionais esparsos e ruidosos.\n\nA função fpca.sc no pacote refund (documentação)\nA função face.sparse no pacote face (documentação)\nA função fpca.mle no pacote fpca (documentação)\nO pacote PACE (escrito em MATLAB) (página da web)\n\nA seguir, é apresentada uma ilustração de FPCA na contagem de células CD4 usando a função fpca.sc.\n\nfpca.res &lt;- fpca.sc(cd4, argvals = month, pve = 0.95, var = TRUE)\n#colSums(fpca.res$efunctions^2)\nm &lt;- length(month)\n\nefns &lt;- fpca.res$efunctions*sqrt(m)\nevals &lt;- fpca.res$evalues/m\n\nefns %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    months = unique(cd4_tidy$months)\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  ggplot(aes(months, value, group = name, color = name))+\n  geom_line()\n\n\n\n\n\n\n\n#\nk.pc &lt;- 2\nmean.hat &lt;- fpca.res$mu\neffect &lt;- 2*sqrt(evals[k.pc])*efns[,k.pc]\n\ncbind(mean.hat = mean.hat, effect = effect) %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    upper_bound = mean.hat + effect,\n    lower_bound = mean.hat - effect,\n    months = unique(cd4_tidy$months)\n  ) %&gt;% \n  pivot_longer(cols = c(mean.hat,lower_bound,upper_bound)) %&gt;% \n  ggplot(aes(months, value, group = name, color = name))+\n  geom_line()\n\n\n\n\n\n\n\n\n(Atividade em grupo) Discuta e interprete os resultados da FPCA. Quantos CPs são necessários para explicar 95% das variabilidades nos dados? Que características das curvas subjacentes foram capturadas por esses CPs?\nUsando as eigenfunções e escores estimados, também podemos reconstruir as curvas verdadeiras específicas do sujeito. As curvas ajustadas podem ser obtidas a partir dos resultados do fpca.sc, mas para fins de ilustração…\n\n#isso vem das linhas 521 e 522, onde foram calculadas as componentes\npve &lt;- cumsum(evals)/sum(evals)\nnpc &lt;- sum(pve &lt; 0.95) + 1\n\n# truncated estimated eigen components\nefns &lt;- efns[,1:npc]\nevals &lt;- evals[1:npc]\n\n\n\npve[1:3] %&gt;% # \n  t() %&gt;% \n  as_tibble() %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  mutate(\n    n_pcs = 1:3\n  ) %&gt;%\n  ggplot(aes(n_pcs,value))+\n  geom_line()+\n  geom_point()+\n  geom_hline(yintercept = 0.95, color = \"red\", linetype = 2)\n\n\n\n\n\n\n\n#\nfpca.res$Yhat %&gt;%\n  t() %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    month = unique(cd4_tidy$months)\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(month) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  group_by(month) %&gt;% \n  ggplot(aes(month, value, group = name, color = name))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  geom_line(aes(month, avg), color = \"red\", linewidth = 1, linetype = 2)\n\n\n\n\n\n\n\n#\nfpca.res$Yhat %&gt;%\n  t() %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    month = unique(cd4_tidy$months)\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(month) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  group_by(month) %&gt;% \n dplyr::filter(name %in% c(\"V1\",\"V42\", \"V7\", \"V28\")) %&gt;% \n  ggplot(aes(month, value, group = name, color = name))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  geom_line(aes(month, avg), color = \"red\", linewidth = 1, linetype = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nA função principal no pacote refund.shiny é a função plot_shiny, que retorna gráficos interativos dos resultados de várias análises de dados funcionais:\n\nFPCA (dia 3)\nRegressão função-em-escalar (dia 4)\nFPCA variante no tempo (dia 5)\n\n\ndata(cd4)\n\nA função plot_shiny recebe a saída de qualquer uma das funções de FPCA no pacote refund, ou seja, fpca.sc, fpca.face, fpca.ssvd e fpca2s.\nfpca.res &lt;- fpca.sc(cd4, pve = 0.95, var = TRUE)\nplot_shiny(fpca.res)\nIsso retorna cinco abas com gráficos interativos:\n\nAba 1: média +/- CPs\nAba 2: gráfico de “scree”\nAba 3: combinações lineares\nAba 4: ajustes por sujeito\nAba 5: gráfico de dispersão de escores\n\n(Atividade em grupo) Explore os gráficos interativos.\n\n\n\n\nDiscutiu-se a estimativa de média e covariância para FD denso.\nInterpretou-se os resultados da fPCA.\nExploraram-se gráficos interativos do plot_shiny.\n\nAtividades em grupo / para casa:\nAnalisar os dados da Maratona (atividade em grupo)\n(duas respostas: tempo decorrido com transformação logarítmica e tempo por milha)\n\nPlotar / descrever os dados.\nPlotar / discutir as funções estimadas de média e covariância.\nUtilizar gráficos interativos para explorar os resultados da fPCA e resumir suas descobertas.\n\nAnalisar os dados de Poluentes (atividade para casa)\n(enfoque no nível de sulfato com transformação logarítmica)\n\nPlotar / descrever os dados.\nPlotar / discutir as funções estimadas de média e covariância.\nUtilizar gráficos interativos para explorar os resultados da fPCA e resumir suas descobertas."
  },
  {
    "objectID": "dia03.html#tópicos-de-hoje",
    "href": "dia03.html#tópicos-de-hoje",
    "title": "Dia-03",
    "section": "",
    "text": "FPCA para dados altamente frequentes (dados de imagens de tensor de difusão)\n\nEstimação das funções média e covariância (suavização de cada curva / média ponto a ponto)\nDecomposição espectral das funções de covariância (escalonamento!)\n\nFPCA para dados ruidosos e esparsos (dados de contagem de células CD4)\nVisualização usando refund.shiny (dados de clima canadense)"
  },
  {
    "objectID": "dia03.html#fpca-for-highly-frequent-data-with-noise",
    "href": "dia03.html#fpca-for-highly-frequent-data-with-noise",
    "title": "Dia-03",
    "section": "1 FPCA for highly frequent data with noise",
    "text": "1 FPCA for highly frequent data with noise\n\nModelo FPCA\n\n\\[\nY_i(t) = \\mu(t) + \\sum_{k=1}^{n_{pc}} \\xi_{ik} \\phi_k(t) + \\epsilon_i(t)\n\\]\nAqui vou usar o conjunto de dados de imagens de tensor de difusão (DTI) para ilustrar a análise de componentes principais funcionais para dados altamente frequentes com pequenos ruídos (sem dados faltantes).\nLembrando que, como discutido no Dia 1, iremos nos concentrar na Anisotropia Fracional (FA) ao longo do trato do corpo caloso (CCA) coletada de pacientes com esclerose múltipla (MS) sem nenhum valor faltante.\n\nDTI <- DTI %>% \n  drop_na() %>% \n  filter(visit == 1 & case == 1)\n\n\ncca_data <- DTI$cca %>% \n  t() %>% \n  as_tibble() %>% \n janitor::clean_names() %>% \n  mutate(\n    tract = 1:93\n  )\n\ngráfico da média dos dados\n\ncca_data %>% \n  pivot_longer(cols = starts_with(\"x\")) %>% \n  group_by(tract) %>% \n  mutate(\n    avg = mean(value)\n  ) %>% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 2)+\n  theme(legend.position = \"none\")+\n  ylab(\"Anisotropia fracional (AF)\")+\n  ggtitle(\"Imagem de Tensor de Difusão:CCA\")\n\n\n\n\n\n\n\n\n\n1.1 Estimação das funções média\nSuavize cada curva e calcule a média ponto a ponto.\n\nlibrary(mgcv)\n\nsmooth.curves <- array(0, dim(DTI$cca))\nn <- nrow(DTI$cca)\ntract <- 1:93\n\nfor(j in 1:n){\n  # j = 1\n  fit <- gam(DTI$cca[j,] ~ s(tract, k = 10, bs = 'cr'), method = \"REML\")\n  # plot(tract, DTI.baseline$cca[j,])\n  # lines(tract, fit$fitted)\n  smooth.curves[j,] <- fit$fitted\n}\n\n\nsmooth.curves %>% \n  t() %>% \n  as_tibble() %>% \n  janitor::clean_names() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  group_by(tract) %>% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nsmooth.curves %>% \n  t() %>% \n  as_tibble() %>% \n  janitor::clean_names() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  group_by(tract) %>% \n  mutate(\n    avg = mean(value)\n  ) %>% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 2)+\n  theme(legend.position = \"none\")+\n  ylab(\"Anisotropia Fracional (FA)\")+\n  ggtitle(\"Imagem de Tensor de Difusão:CCA\")\n\n\n\n\n\n\n\n\nObserve que ambas as abordagens fornecem exatamente a mesma função média!\n\n\n1.2 Estimação da função de covariância\nSuavize cada curva e calcule a covariância amostral.\n\nlibrary(RColorBrewer)\n\nsmooth.cov <- cov(smooth.curves)\n\nsmooth.cov %>% \n  as_tibble() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  ggplot(aes(tract, name, fill = value)) + \n  geom_tile()+\n  scale_fill_gradientn(colors = brewer.pal(11, \"RdBu\")) +\n  theme(axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())+\n  labs(y=tract,title = \"Covariância suavizada de Análise Fatorial (AF).\")"
  },
  {
    "objectID": "dia03.html#a-decomposição-espectral-da-matriz-de-covariância-estimada.",
    "href": "dia03.html#a-decomposição-espectral-da-matriz-de-covariância-estimada.",
    "title": "Dia-03",
    "section": "1.3 A decomposição espectral da matriz de covariância estimada.",
    "text": "1.3 A decomposição espectral da matriz de covariância estimada.\nA partir da decomposição espectral da função de covariância estimada, podemos obter as eigenfunções e os autovalores estimados.\n\nsvd.result0 <- eigen(smooth.cov, symmetric = TRUE)\n# names(svd.result0)\n\nevectors <- svd.result0$vectors[,svd.result0$values > 0]\nevalues <- svd.result0$values[svd.result0$values > 0]\n\nhead(colSums(evectors^2)) # returns unitary vectors \n\n[1] 1 1 1 1 1 1\n\n#Como as eigenfunções retornam vetores unitários, precisamos escalá-los por sqrt(93) (raiz quadrada de 93).\n\nefns0 <- evectors*sqrt(93)\nevals0 <- evalues/93\npve <- cumsum(evals0)/sum(evals0)\nnpc <- sum(pve < 0.95) + 1\n\n# Componentes eigen estimados truncados.\nefns <- efns0[,1:npc]\nevals <- evals0[1:npc]\n\nO gráfico de “scree” está apresentado abaixo:\n\npve[1:20] %>% # existem 48 componentes / só estou usando as 20 primeiras\n  t() %>% \n  as_tibble() %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  mutate(\n    n_pcs = 1:20\n  ) %>%\n  ggplot(aes(n_pcs,value))+\n  geom_line()+\n  geom_point()+\n  geom_hline(yintercept = 0.95, color = \"red\", linetype = 2)+\n  labs(x= \"número de CPs\", y=\"% de variação explicada\", title = \"Scree plot\")\n\n\n\n\n\n\n\n\nCom base no gráfico sabemos que os primeiros 5 componentes principais explicam mais de 95% das variabilidades nos dados.\n\nefns[,1:5] %>% \n  as_tibble() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  group_by(tract) %>%\n  ggplot(aes(tract,value,group = name,color = name))+\n  geom_line()+\n  labs(y=\"autofunção\", title = \"5 primeiras autofunções\")\n\n\n\n\n\n\n\n\nPodemos também visualizar o efeito de cada componente principal traçando a função \\(\\mu(t) \\pm 2\\sqrt{\\lambda_k}\\phi_k(t)\\).\n\nk.pc <- 1\neffect <- efns[, k.pc]*2*sqrt(evals[k.pc])\nmean.hat <- colMeans(smooth.curves)\nmat <- cbind(mean.hat - effect,  mean.hat + effect)\n\n\nefns[, k.pc] %>% \n  as_tibble() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  group_by(tract) %>%\n  ggplot(aes(tract,value))+\n  geom_line()+ggtitle(\"fPC1\")+\n  ylim(-2,2)\n\n\n\n\n\n\n\nmat %>% \n  as_tibble() %>% \n  cbind(pc_1 = mean.hat, tract = 1:93) %>% \n  pivot_longer(cols = 1:3) %>% \n  group_by(tract) %>% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line() +\n  ggtitle(glue::glue(\"fPC\",k.pc, \" (\",round(pve[k.pc]*100),\"%)\"))\n\n\n\n\n\n\n\n\n(Atividade em grupo) O que descobrimos a partir dos resultados da fPCA? Quanta variabilidade dos dados foi explicada pela primeira fPC? Que características das curvas a primeira fPC explica? Tente fazer os gráficos para a segunda e terceira fPCs e seus efeitos em relação à média geral.\n\nk.pc <- 2\neffect <- efns[, k.pc]*2*sqrt(evals[k.pc])\nmean.hat <- colMeans(smooth.curves)\nmat <- cbind(mean.hat - effect,  mean.hat + effect)\n\n\nefns[, k.pc] %>% \n  as_tibble() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  group_by(tract) %>%\n  ggplot(aes(tract,value))+\n  geom_line()+ggtitle(glue::glue(\"fPC\",k.pc,))\n\n\n\n\n\n\n\nmat %>% \n  as_tibble() %>% \n  cbind(pc_1 = mean.hat, tract = 1:93) %>% \n  pivot_longer(cols = 1:3) %>% \n  group_by(tract) %>% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line() +\n  ggtitle(glue::glue(\"fPC\",k.pc, \" (\",round(pve[k.pc]*100),\"%)\"))\n\n\n\n\n\n\n\n#################################\n# Pc3\n\nk.pc <- 3\neffect <- efns[, k.pc]*2*sqrt(evals[k.pc])\nmean.hat <- colMeans(smooth.curves)\nmat <- cbind(mean.hat - effect,  mean.hat + effect)\n\n\nefns[, k.pc] %>% \n  as_tibble() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  group_by(tract) %>%\n  ggplot(aes(tract,value))+\n  geom_line()+ggtitle(glue::glue(\"fPC\",k.pc,))\n\n\n\n\n\n\n\nmat %>% \n  as_tibble() %>% \n  cbind(pc_1 = mean.hat, tract = 1:93) %>% \n  pivot_longer(cols = 1:3) %>% \n  group_by(tract) %>% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line() +\n  ggtitle(glue::glue(\"fPC\",k.pc, \" (\",round(pve[k.pc]*100),\"%)\"))"
  },
  {
    "objectID": "dia03.html#section",
    "href": "dia03.html#section",
    "title": "Dia-03",
    "section": "3",
    "text": "3"
  },
  {
    "objectID": "dia03.html#estimativa-de-escores-e-curvas-ajustadas.",
    "href": "dia03.html#estimativa-de-escores-e-curvas-ajustadas.",
    "title": "Dia-03",
    "section": "1.4 Estimativa de escores e curvas ajustadas.",
    "text": "1.4 Estimativa de escores e curvas ajustadas.\nOs escores estimados podem ser obtidos calculando \\(\\hat{\\xi}_{ik} = \\int_{T} \\hat{\\phi}^k(t) \\{Y_i(t) - \\hat{\\mu}(t)\\} dt\\).\nE as curvas ajustadas são dadas por\n\\[\n\\hat{Y}_i(t) = \\hat{\\mu}(t) + \\sum_{k=1}^{npc} \\hat{\\xi}_{ik} \\hat{\\phi}_k(t).\n\\]\n\ndemeaned <- DTI$cca - t(matrix(rep(mean.hat, n),\n                                        nrow=length(mean.hat)))\n\nscores <- matrix(NA, nrow=n, ncol=npc)\nfitted <- array(NA, dim(DTI$cca))\n\nfor(i in 1:n){\n  scores[i,] <- colMeans(matrix(rep(demeaned[i,], npc), nrow=93) * efns)\n  fitted[i,] <- mean.hat + scores[i,]%*%t(efns)\n}\n\n\nrbind( DTI$cca[1:3,], fitted[1:3,])%>% \n  t() %>% \n  as_tibble() %>%\n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = contains(\"2\"), values_to = \"c_value\") %>% \n  pivot_longer(cols = starts_with(\"V\"), values_to = \"values_smoo\", names_to = \"name_smoo\") %>% \n  group_by(tract) %>% \n  ggplot(aes(tract,c_value, group = name, color=name))+\n  geom_point()+\n  geom_line(aes(tract,values_smoo,group = name_smoo, color = name_smoo), linewidth = 1.5)"
  },
  {
    "objectID": "dia03.html#fpca-para-dados-altamente-frequentes-com-ruído.",
    "href": "dia03.html#fpca-para-dados-altamente-frequentes-com-ruído.",
    "title": "Dia-03",
    "section": "1 FPCA para dados altamente frequentes com ruído.",
    "text": "1 FPCA para dados altamente frequentes com ruído.\n\nModelo FPCA\n\n\\[\nY_i(t) = \\mu(t) + \\sum_{k=1}^{n_{pc}} \\xi_{ik} \\phi_k(t) + \\epsilon_i(t)\n\\]\nAqui vou usar o conjunto de dados de imagens de tensor de difusão (DTI) para ilustrar a análise de componentes principais funcionais para dados altamente frequentes com pequenos ruídos (sem dados faltantes).\nLembrando que, como discutido no Dia 1, iremos nos concentrar na Anisotropia Fracional (FA) ao longo do trato do corpo caloso (CCA) coletada de pacientes com esclerose múltipla (MS) sem nenhum valor faltante.\n\nDTI <- DTI %>% \n  drop_na() %>% \n  filter(visit == 1 & case == 1)\n\n\ncca_data <- DTI$cca %>% \n  t() %>% \n  as_tibble() %>% \n janitor::clean_names() %>% \n  mutate(\n    tract = 1:93\n  )\n\ngráfico da média dos dados\n\ncca_data %>% \n  pivot_longer(cols = starts_with(\"x\")) %>% \n  group_by(tract) %>% \n  mutate(\n    avg = mean(value)\n  ) %>% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 2)+\n  theme(legend.position = \"none\")+\n  ylab(\"Anisotropia fracional (AF)\")+\n  ggtitle(\"Imagem de Tensor de Difusão:CCA\")\n\n\n\n\n\n\n\n\n\n1.1 Estimação das funções média\nSuavize cada curva e calcule a média ponto a ponto.\n\nlibrary(mgcv)\n\nsmooth.curves <- array(0, dim(DTI$cca))\nn <- nrow(DTI$cca)\ntract <- 1:93\n\nfor(j in 1:n){\n  # j = 1\n  fit <- gam(DTI$cca[j,] ~ s(tract, k = 10, bs = 'cr'), method = \"REML\")\n  # plot(tract, DTI.baseline$cca[j,])\n  # lines(tract, fit$fitted)\n  smooth.curves[j,] <- fit$fitted\n}\n\n\nsmooth.curves %>% \n  t() %>% \n  as_tibble() %>% \n  janitor::clean_names() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  group_by(tract) %>% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nsmooth.curves %>% \n  t() %>% \n  as_tibble() %>% \n  janitor::clean_names() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  group_by(tract) %>% \n  mutate(\n    avg = mean(value)\n  ) %>% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 2)+\n  theme(legend.position = \"none\")+\n  ylab(\"Anisotropia Fracional (FA)\")+\n  ggtitle(\"Imagem de Tensor de Difusão:CCA\")\n\n\n\n\n\n\n\n\nObserve que ambas as abordagens fornecem exatamente a mesma função média!\n\n\n1.2 Estimação da função de covariância\nSuavize cada curva e calcule a covariância amostral.\n\nlibrary(RColorBrewer)\n\nsmooth.cov <- cov(smooth.curves)\n\nsmooth.cov %>% \n  as_tibble() %>% \n  mutate(\n    tract = 1:93\n  ) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  ggplot(aes(tract, name, fill = value)) + \n  geom_tile()+\n  scale_fill_gradientn(colors = brewer.pal(11, \"RdBu\")) +\n  theme(axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())+\n  labs(y=tract,title = \"Covariância suavizada de Análise Fatorial (AF).\")"
  },
  {
    "objectID": "dia03.html#fpca-para-dados-de-alta-frequentes-com-ruído.",
    "href": "dia03.html#fpca-para-dados-de-alta-frequentes-com-ruído.",
    "title": "Dia-03",
    "section": "",
    "text": "Modelo FPCA\n\n\\[\nY_i(t) = \\mu(t) + \\sum_{k=1}^{n_{pc}} \\xi_{ik} \\phi_k(t) + \\epsilon_i(t)\n\\]\nAqui vou usar o conjunto de dados de imagens de tensor de difusão (DTI) para ilustrar a análise de componentes principais funcionais para dados altamente frequentes com pequenos ruídos (sem dados faltantes).\nLembrando que, como discutido no Dia 1, iremos nos concentrar na Anisotropia Fracional (FA) ao longo do trato do corpo caloso (CCA) coletada de pacientes com esclerose múltipla (MS) sem nenhum valor faltante.\n\nDTI &lt;- DTI %&gt;% \n  drop_na() %&gt;% \n  filter(visit == 1 & case == 1)\n\n\ncca_data &lt;- DTI$cca %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  )\n\ngráfico da média dos dados\n\ncca_data %&gt;% \n  pivot_longer(cols = starts_with(\"x\")) %&gt;% \n  group_by(tract) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 2)+\n  theme(legend.position = \"none\")+\n  ylab(\"Anisotropia fracional (AF)\")+\n  ggtitle(\"Imagem de Tensor de Difusão:CCA\")\n\n\n\n\n\n\n\n\n\n\nSuavize cada curva e calcule a média ponto a ponto.\n\nlibrary(mgcv)\n\nsmooth.curves &lt;- array(0, dim(DTI$cca))\nn &lt;- nrow(DTI$cca)\ntract &lt;- 1:93\n\nfor(j in 1:n){\n  # j = 1\n  fit &lt;- gam(DTI$cca[j,] ~ s(tract, k = 10, bs = 'cr'), method = \"REML\")\n  # plot(tract, DTI.baseline$cca[j,])\n  # lines(tract, fit$fitted)\n  smooth.curves[j,] &lt;- fit$fitted\n}\n\n\nsmooth.curves %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nsmooth.curves %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 2)+\n  theme(legend.position = \"none\")+\n  ylab(\"Anisotropia Fracional (FA)\")+\n  ggtitle(\"Imagem de Tensor de Difusão:CCA\")\n\n\n\n\n\n\n\n\nObserve que ambas as abordagens fornecem exatamente a mesma função média!\n\n\n\nSuavize cada curva e calcule a covariância amostral.\n\nlibrary(RColorBrewer)\n\nsmooth.cov &lt;- cov(smooth.curves)\n\nsmooth.cov %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  ggplot(aes(tract, name, fill = value)) + \n  geom_tile()+\n  scale_fill_gradientn(colors = brewer.pal(11, \"RdBu\")) +\n  theme(axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())+\n  labs(y=tract,title = \"Covariância suavizada de Análise Fatorial (AF).\")\n\n\n\n\n\n\n\n\n\n\n\nA partir da decomposição espectral da função de covariância estimada, podemos obter as eigenfunções e os autovalores estimados.\n\nsvd.result0 &lt;- eigen(smooth.cov, symmetric = TRUE)\n# names(svd.result0)\n\nevectors &lt;- svd.result0$vectors[,svd.result0$values &gt; 0]\nevalues &lt;- svd.result0$values[svd.result0$values &gt; 0]\n\nhead(colSums(evectors^2)) # returns unitary vectors \n\n[1] 1 1 1 1 1 1\n\n#Como as eigenfunções retornam vetores unitários, precisamos escalá-los por sqrt(93) (raiz quadrada de 93).\n\nefns0 &lt;- evectors*sqrt(93)\nevals0 &lt;- evalues/93\npve &lt;- cumsum(evals0)/sum(evals0)\nnpc &lt;- sum(pve &lt; 0.95) + 1\n\n# Componentes eigen estimados truncados.\nefns &lt;- efns0[,1:npc]\nevals &lt;- evals0[1:npc]\n\nO gráfico de “scree” está apresentado abaixo:\n\npve[1:20] %&gt;% # existem 48 componentes / só estou usando as 20 primeiras\n  t() %&gt;% \n  as_tibble() %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  mutate(\n    n_pcs = 1:20\n  ) %&gt;%\n  ggplot(aes(n_pcs,value))+\n  geom_line()+\n  geom_point()+\n  geom_hline(yintercept = 0.95, color = \"red\", linetype = 2)+\n  labs(x= \"número de CPs\", y=\"% de variação explicada\", title = \"Scree plot\")\n\n\n\n\n\n\n\n\nCom base no gráfico sabemos que os primeiros 5 componentes principais explicam mais de 95% das variabilidades nos dados.\n\nefns[,1:5] %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;%\n  ggplot(aes(tract,value,group = name,color = name))+\n  geom_line()+\n  labs(y=\"autofunção\", title = \"5 primeiras autofunções\")\n\n\n\n\n\n\n\n\nPodemos também visualizar o efeito de cada componente principal traçando a função \\(\\mu(t) \\pm 2\\sqrt{\\lambda_k}\\phi_k(t)\\).\n\nk.pc &lt;- 1\neffect &lt;- efns[, k.pc]*2*sqrt(evals[k.pc])\nmean.hat &lt;- colMeans(smooth.curves)\nmat &lt;- cbind(mean.hat - effect,  mean.hat + effect)\n\n\nefns[, k.pc] %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;%\n  ggplot(aes(tract,value))+\n  geom_line()+ggtitle(\"fPC1\")+\n  ylim(-2,2)\n\n\n\n\n\n\n\nmat %&gt;% \n  as_tibble() %&gt;% \n  cbind(pc_1 = mean.hat, tract = 1:93) %&gt;% \n  pivot_longer(cols = 1:3) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line() +\n  ggtitle(glue::glue(\"fPC\",k.pc, \" (\",round(pve[k.pc]*100),\"%)\"))\n\n\n\n\n\n\n\n\n(Atividade em grupo) O que descobrimos a partir dos resultados da fPCA? Quanta variabilidade dos dados foi explicada pela primeira fPC? Que características das curvas a primeira fPC explica? Tente fazer os gráficos para a segunda e terceira fPCs e seus efeitos em relação à média geral.\n\nk.pc &lt;- 2\neffect &lt;- efns[, k.pc]*2*sqrt(evals[k.pc])\nmean.hat &lt;- colMeans(smooth.curves)\nmat &lt;- cbind(mean.hat - effect,  mean.hat + effect)\n\n\nefns[, k.pc] %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;%\n  ggplot(aes(tract,value))+\n  geom_line()+ggtitle(glue::glue(\"fPC\",k.pc,))\n\n\n\n\n\n\n\nmat %&gt;% \n  as_tibble() %&gt;% \n  cbind(pc_1 = mean.hat, tract = 1:93) %&gt;% \n  pivot_longer(cols = 1:3) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line() +\n  ggtitle(glue::glue(\"fPC\",k.pc, \" (\",round(pve[k.pc]*100),\"%)\"))\n\n\n\n\n\n\n\n#################################\n# Pc3\n\nk.pc &lt;- 3\neffect &lt;- efns[, k.pc]*2*sqrt(evals[k.pc])\nmean.hat &lt;- colMeans(smooth.curves)\nmat &lt;- cbind(mean.hat - effect,  mean.hat + effect)\n\n\nefns[, k.pc] %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;%\n  ggplot(aes(tract,value))+\n  geom_line()+ggtitle(glue::glue(\"fPC\",k.pc,))\n\n\n\n\n\n\n\nmat %&gt;% \n  as_tibble() %&gt;% \n  cbind(pc_1 = mean.hat, tract = 1:93) %&gt;% \n  pivot_longer(cols = 1:3) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line() +\n  ggtitle(glue::glue(\"fPC\",k.pc, \" (\",round(pve[k.pc]*100),\"%)\"))\n\n\n\n\n\n\n\n\n\n\n\nOs escores estimados podem ser obtidos calculando \\(\\hat{\\xi}_{ik} = \\int_{T} \\hat{\\phi}^k(t) \\{Y_i(t) - \\hat{\\mu}(t)\\} dt\\).\nE as curvas ajustadas são dadas por\n\\[\n\\hat{Y}_i(t) = \\hat{\\mu}(t) + \\sum_{k=1}^{npc} \\hat{\\xi}_{ik} \\hat{\\phi}_k(t).\n\\]\n\ndemeaned &lt;- DTI$cca - t(matrix(rep(mean.hat, n),\n                                        nrow=length(mean.hat)))\n\nscores &lt;- matrix(NA, nrow=n, ncol=npc)\nfitted &lt;- array(NA, dim(DTI$cca))\n\nfor(i in 1:n){\n  scores[i,] &lt;- colMeans(matrix(rep(demeaned[i,], npc), nrow=93) * efns)\n  fitted[i,] &lt;- mean.hat + scores[i,]%*%t(efns)\n}\n\n\nrbind( DTI$cca[1:3,], fitted[1:3,])%&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;%\n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = contains(\"2\"), values_to = \"c_value\") %&gt;% \n  pivot_longer(cols = starts_with(\"V\"), values_to = \"values_smoo\", names_to = \"name_smoo\") %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract,c_value, group = name, color=name))+\n  geom_point()+\n  geom_line(aes(tract,values_smoo,group = name_smoo, color = name_smoo), linewidth = 1.5)\n\n\n\n\n\n\n\n\n\n\n\nExistem várias funções em R que implementam o método fPCA para dados densamente observados (com/sem ruído):\n\nfpca.face, fpca.ssvd e fpca2s do pacote “refund”.\n\nDesenvolvidas especificamente para dados funcionais densos.\nNão são aplicáveis a dados funcionais esparsos.\nfpca.ssvd e fpca2s exigem a especificação do número de componentes principais (npc); não é possível selecionar o número de componentes principais com base na proporção da variância explicada (PVE, sigla em inglês).\n\n\n\ntract &lt;- 1:93\n\nres.face &lt;- fpca.face(Y = DTI$cca, argvals = tract, pve = 0.95)\n\nnames(res.face)\n\n[1] \"Yhat\"       \"Y\"          \"scores\"     \"mu\"         \"efunctions\"\n[6] \"evalues\"    \"npc\"       \n\nefn.face &lt;- res.face$efunctions*sqrt(93)\neval.face &lt;- res.face$evalues/93\n\n\nefn.face %&gt;% \n  as_tibble() %&gt;% \n  rownames_to_column(var = \"tract\") %&gt;% \n  mutate(\n    tract = as.numeric(tract)\n  ) %&gt;% \n  dplyr::select(1:6) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name,  color = name))+\n  geom_line()+\n  labs(x=\"tract\", y=\"\", title = \"5 primeiras autofunções\")\n\n\n\n\n\n\n\n\n\nk.pc &lt;- 1\nmu.hat &lt;- res.face$mu\neffect &lt;- efn.face[,k.pc] * 2* sqrt(eval.face[k.pc])\npve.face &lt;- (cumsum(eval.face)/sum(eval.face))\n\npc1_plot &lt;- cbind(tract, efn.face[,k.pc]) %&gt;% \n  as_tibble()\n\npc1_plot %&gt;% \n  ggplot(aes(tract, V2))+\n  geom_line()+\n  ylim(c(-2,2))\n\n\n\n\n\n\n\nmu.hat %&gt;% \n  as_tibble_col(column_name = \"mu.hat\") %&gt;% \n  mutate(\n    effect_Plus = mu.hat + effect,\n    effect_less = mu.hat - effect,\n    tract = tract\n  ) %&gt;% \n  pivot_longer(cols = 1:3) %&gt;% \n  group_by(tract) %&gt;% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line()+\n  ggtitle(glue::glue(\"fPC\", k.pc,\"(\", round(pve.face[k.pc]*100) ,\"%)\"))\n\n\n\n\n\n\n\nyhat_smooth &lt;- res.face$Yhat %&gt;% \n      t() %&gt;% \n      as_tibble() %&gt;% \n     dplyr::select(1:3) %&gt;% \n      pivot_longer(cols = contains(\"_1\")) %&gt;% \n      rownames_to_column(var = \"tract\") %&gt;% \n      mutate(\n        tract = as.numeric(tract)\n      ) %&gt;% \n dplyr::select(-name)\n\nDTI$cca %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \ndplyr::select(1:3) %&gt;% \n  pivot_longer(cols = contains(\"_1\")) %&gt;% \n  rownames_to_column(var = \"tract\") %&gt;% \n  mutate(\n    tract = as.numeric(tract)\n  ) %&gt;% \n  left_join(yhat_smooth, by = \"tract\") %&gt;% \n  ggplot(aes(tract, value.x, group = name, color = name))+\n  geom_point(alpha = 0.7)+\n  geom_line(aes(tract,value.y), linetype = 1, linewidth = 1.5)\n\n\n\n\n\n\n\n\nPara usar as funções fpca.ssvd e fpca2s, confira os seguintes códigos de exemplo:\n\nres.ssvd &lt;- fpca.ssvd(Y = DTI.baseline$cca, npc = 5)\nres.2s &lt;- fpca2s(Y = DTI.baseline$cca, npc = 5, argvals = tract)\n\nSempre verifique se os eigencomponentes e escores resultantes estão corretamente escalados!"
  },
  {
    "objectID": "dia03.html#funções-já-incorporadas-disponíveis-no-r.",
    "href": "dia03.html#funções-já-incorporadas-disponíveis-no-r.",
    "title": "Dia-03",
    "section": "1.5 Funções já incorporadas disponíveis no R.",
    "text": "1.5 Funções já incorporadas disponíveis no R.\nExistem várias funções em R que implementam o método fPCA para dados densamente observados (com/sem ruído):\n\nfpca.face, fpca.ssvd e fpca2s do pacote “refund”.\n\nDesenvolvidas especificamente para dados funcionais densos.\nNão são aplicáveis a dados funcionais esparsos.\nfpca.ssvd e fpca2s exigem a especificação do número de componentes principais (npc); não é possível selecionar o número de componentes principais com base na proporção da variância explicada (PVE, sigla em inglês).\n\n\n\ntract <- 1:93\n\nres.face <- fpca.face(Y = DTI$cca, argvals = tract, pve = 0.95)\n\nnames(res.face)\n\n[1] \"Yhat\"       \"Y\"          \"scores\"     \"mu\"         \"efunctions\"\n[6] \"evalues\"    \"npc\"       \n\nefn.face <- res.face$efunctions*sqrt(93)\neval.face <- res.face$evalues/93\n\n\nefn.face %>% \n  as_tibble() %>% \n  rownames_to_column(var = \"tract\") %>% \n  mutate(\n    tract = as.numeric(tract)\n  ) %>% \n  dplyr::select(1:6) %>% \n  pivot_longer(cols = starts_with(\"V\")) %>% \n  group_by(tract) %>% \n  ggplot(aes(tract, value, group = name,  color = name))+\n  geom_line()+\n  labs(x=\"tract\", y=\"\", title = \"5 primeiras autofunções\")\n\n\n\n\n\n\n\n\n\nk.pc <- 1\nmu.hat <- res.face$mu\neffect <- efn.face[,k.pc] * 2* sqrt(eval.face[k.pc])\npve.face <- (cumsum(eval.face)/sum(eval.face))\n\npc1_plot <- cbind(tract, efn.face[,k.pc]) %>% \n  as_tibble()\n\npc1_plot %>% \n  ggplot(aes(tract, V2))+\n  geom_line()+\n  ylim(c(-2,2))\n\n\n\n\n\n\n\nmu.hat %>% \n  as_tibble_col(column_name = \"mu.hat\") %>% \n  mutate(\n    effect_Plus = mu.hat + effect,\n    effect_less = mu.hat - effect,\n    tract = tract\n  ) %>% \n  pivot_longer(cols = 1:3) %>% \n  group_by(tract) %>% \n  ggplot(aes(tract, value, group = name, color = name))+\n  geom_line()+\n  ggtitle(glue::glue(\"fPC\", k.pc,\"(\", round(pve.face[k.pc]*100) ,\"%)\"))\n\n\n\n\n\n\n\nyhat_smooth <- res.face$Yhat %>% \n      t() %>% \n      as_tibble() %>% \n     dplyr::select(1:3) %>% \n      pivot_longer(cols = contains(\"_1\")) %>% \n      rownames_to_column(var = \"tract\") %>% \n      mutate(\n        tract = as.numeric(tract)\n      ) %>% \n dplyr::select(-name)\n\nDTI$cca %>% \n  t() %>% \n  as_tibble() %>% \ndplyr::select(1:3) %>% \n  pivot_longer(cols = contains(\"_1\")) %>% \n  rownames_to_column(var = \"tract\") %>% \n  mutate(\n    tract = as.numeric(tract)\n  ) %>% \n  left_join(yhat_smooth, by = \"tract\") %>% \n  ggplot(aes(tract, value.x, group = name, color = name))+\n  geom_point(alpha = 0.7)+\n  geom_line(aes(tract,value.y), linetype = 1, linewidth = 1.5)\n\n\n\n\n\n\n\n\nPara usar as funções fpca.ssvd e fpca2s, confira os seguintes códigos de exemplo:\n\nres.ssvd <- fpca.ssvd(Y = DTI.baseline$cca, npc = 5)\nres.2s <- fpca2s(Y = DTI.baseline$cca, npc = 5, argvals = tract)\n\nSempre verifique se os eigencomponentes e escores resultantes estão corretamente escalados!"
  },
  {
    "objectID": "dia03.html#análise-de-fpca-nos-dados-de-contagem-de-células-cd4.",
    "href": "dia03.html#análise-de-fpca-nos-dados-de-contagem-de-células-cd4.",
    "title": "Dia-03",
    "section": "2.1 Análise de FPCA nos dados de contagem de células CD4.",
    "text": "2.1 Análise de FPCA nos dados de contagem de células CD4.\nEstimativa da média através da combinação dos dados\nDevido ao fato de haver apenas algumas medições repetidas da contagem de células CD4 de cada sujeito, as duas abordagens para estimar a função média (e covariância) para dados funcionais densos não são apropriadas. Portanto, a abordagem comum para dados funcionais esparsos é combinar todas as medições.\nPor exemplo, para estimar a função média,\n\nn <- nrow(cd4)\nmonth <- as.numeric(colnames(cd4)) # months -18 and 42 since seroconversion\nm <- ncol(cd4)\n\n\nlibrary(mgcv)\n\n# use all measurements\ndat.vec <- data.frame(na.omit(cbind(rep(1:n, each = length(month)), \n                                    rep(month, n), as.vector(t(cd4)))))\ncolnames(dat.vec) <- c(\"id\", \"t\",\"y\")\nfit <- gam(y ~ s(t, k = 10, bs = \"cr\"), \n           method=\"REML\", data = dat.vec)\nmean.hat <- predict(fit, newdata = data.frame(\"t\" = month))\n\nmeand_df <- cbind(months =  unique(cd4_tidy$months), mean.hat) %>% \n  as_tibble()\n\n\nmeand_df %>% \n  ggplot(aes(months, mean.hat))+\n  geom_line()+\n  ylim(c(0,3000))"
  },
  {
    "objectID": "dia03.html#fpca-para-dados-ruidosos-e-esparsos.",
    "href": "dia03.html#fpca-para-dados-ruidosos-e-esparsos.",
    "title": "Dia-03",
    "section": "",
    "text": "Quando queremos realizar a FPCA em dados funcionais esparsos e ruidosos, como o conjunto de dados de contagem de CD4…\n\ndata(cd4)\n\ncd4_tidy &lt;- cd4 %&gt;%\n  as_tibble() %&gt;% \n  rowid_to_column(var = \"affected\") %&gt;% \n  pivot_longer(cols = 2:62, names_to = \"months\", values_to = \"count-mm\" ) %&gt;% \n  mutate(\n    months = as.numeric(months)\n  )\n\ncd4_tidy %&gt;% \n  group_by(affected, months) %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(months, `count-mm`, group = affected))+\n  geom_line(color = \"grey\")\n\n\n\n\n\n\n\nset.seed(123)\nsampled &lt;- sample(cd4_tidy$affected, size = 5, replace = FALSE)\n\ncd4_tidy %&gt;% \n  filter(affected %in%  sampled) %&gt;% \n  mutate(\n    affected = as_factor(affected)\n  ) %&gt;% \n  group_by(affected, months) %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(months, `count-mm`, group = affected, color = affected))+\n  geom_line()+\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nEstimativa da média através da combinação dos dados\nDevido ao fato de haver apenas algumas medições repetidas da contagem de células CD4 de cada sujeito, as duas abordagens para estimar a função média (e covariância) para dados funcionais densos não são apropriadas. Portanto, a abordagem comum para dados funcionais esparsos é combinar todas as medições.\nPor exemplo, para estimar a função média,\n\nn &lt;- nrow(cd4)\nmonth &lt;- as.numeric(colnames(cd4)) # months -18 and 42 since seroconversion\nm &lt;- ncol(cd4)\n\n\nlibrary(mgcv)\n\n# use all measurements\ndat.vec &lt;- data.frame(na.omit(cbind(rep(1:n, each = length(month)), \n                                    rep(month, n), as.vector(t(cd4)))))\ncolnames(dat.vec) &lt;- c(\"id\", \"t\",\"y\")\nfit &lt;- gam(y ~ s(t, k = 10, bs = \"cr\"), \n           method=\"REML\", data = dat.vec)\nmean.hat &lt;- predict(fit, newdata = data.frame(\"t\" = month))\n\nmeand_df &lt;- cbind(months =  unique(cd4_tidy$months), mean.hat) %&gt;% \n  as_tibble()\n\n\nmeand_df %&gt;% \n  ggplot(aes(months, mean.hat))+\n  geom_line()+\n  ylim(c(0,3000))\n\n\n\n\n\n\n\n\nDa mesma forma, podemos estimar a função de covariância combinando todas as medições.\n\n\n\nExistem vários softwares que já foram desenvolvidos para implementar o FPCA para dados funcionais esparsos e ruidosos.\n\nA função fpca.sc no pacote refund (documentação)\nA função face.sparse no pacote face (documentação)\nA função fpca.mle no pacote fpca (documentação)\nO pacote PACE (escrito em MATLAB) (página da web)\n\nA seguir, é apresentada uma ilustração de FPCA na contagem de células CD4 usando a função fpca.sc.\n\nfpca.res &lt;- fpca.sc(cd4, argvals = month, pve = 0.95, var = TRUE)\n#colSums(fpca.res$efunctions^2)\nm &lt;- length(month)\n\nefns &lt;- fpca.res$efunctions*sqrt(m)\nevals &lt;- fpca.res$evalues/m\n\nefns %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    months = unique(cd4_tidy$months)\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  ggplot(aes(months, value, group = name, color = name))+\n  geom_line()\n\n\n\n\n\n\n\n#\nk.pc &lt;- 2\nmean.hat &lt;- fpca.res$mu\neffect &lt;- 2*sqrt(evals[k.pc])*efns[,k.pc]\n\ncbind(mean.hat = mean.hat, effect = effect) %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    upper_bound = mean.hat + effect,\n    lower_bound = mean.hat - effect,\n    months = unique(cd4_tidy$months)\n  ) %&gt;% \n  pivot_longer(cols = c(mean.hat,lower_bound,upper_bound)) %&gt;% \n  ggplot(aes(months, value, group = name, color = name))+\n  geom_line()\n\n\n\n\n\n\n\n\n(Atividade em grupo) Discuta e interprete os resultados da FPCA. Quantos CPs são necessários para explicar 95% das variabilidades nos dados? Que características das curvas subjacentes foram capturadas por esses CPs?\nUsando as eigenfunções e escores estimados, também podemos reconstruir as curvas verdadeiras específicas do sujeito. As curvas ajustadas podem ser obtidas a partir dos resultados do fpca.sc, mas para fins de ilustração…\n\n#isso vem das linhas 521 e 522, onde foram calculadas as componentes\npve &lt;- cumsum(evals)/sum(evals)\nnpc &lt;- sum(pve &lt; 0.95) + 1\n\n# truncated estimated eigen components\nefns &lt;- efns[,1:npc]\nevals &lt;- evals[1:npc]\n\n\n\npve[1:3] %&gt;% # \n  t() %&gt;% \n  as_tibble() %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  mutate(\n    n_pcs = 1:3\n  ) %&gt;%\n  ggplot(aes(n_pcs,value))+\n  geom_line()+\n  geom_point()+\n  geom_hline(yintercept = 0.95, color = \"red\", linetype = 2)\n\n\n\n\n\n\n\n#\nfpca.res$Yhat %&gt;%\n  t() %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    month = unique(cd4_tidy$months)\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(month) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  group_by(month) %&gt;% \n  ggplot(aes(month, value, group = name, color = name))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  geom_line(aes(month, avg), color = \"red\", linewidth = 1, linetype = 2)\n\n\n\n\n\n\n\n#\nfpca.res$Yhat %&gt;%\n  t() %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    month = unique(cd4_tidy$months)\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"V\")) %&gt;% \n  group_by(month) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  group_by(month) %&gt;% \n dplyr::filter(name %in% c(\"V1\",\"V42\", \"V7\", \"V28\")) %&gt;% \n  ggplot(aes(month, value, group = name, color = name))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  geom_line(aes(month, avg), color = \"red\", linewidth = 1, linetype = 2)"
  },
  {
    "objectID": "dia03.html#visualização-usando-o-pacote-refund.shiny.",
    "href": "dia03.html#visualização-usando-o-pacote-refund.shiny.",
    "title": "Dia-03",
    "section": "",
    "text": "A função principal no pacote refund.shiny é a função plot_shiny, que retorna gráficos interativos dos resultados de várias análises de dados funcionais:\n\nFPCA (dia 3)\nRegressão função-em-escalar (dia 4)\nFPCA variante no tempo (dia 5)\n\n\ndata(cd4)\n\nA função plot_shiny recebe a saída de qualquer uma das funções de FPCA no pacote refund, ou seja, fpca.sc, fpca.face, fpca.ssvd e fpca2s.\nfpca.res &lt;- fpca.sc(cd4, pve = 0.95, var = TRUE)\nplot_shiny(fpca.res)\nIsso retorna cinco abas com gráficos interativos:\n\nAba 1: média +/- CPs\nAba 2: gráfico de “scree”\nAba 3: combinações lineares\nAba 4: ajustes por sujeito\nAba 5: gráfico de dispersão de escores\n\n(Atividade em grupo) Explore os gráficos interativos."
  },
  {
    "objectID": "dia03.html#resumo",
    "href": "dia03.html#resumo",
    "title": "Dia-03",
    "section": "",
    "text": "Discutiu-se a estimativa de média e covariância para FD denso.\nInterpretou-se os resultados da fPCA.\nExploraram-se gráficos interativos do plot_shiny.\n\nAtividades em grupo / para casa:\nAnalisar os dados da Maratona (atividade em grupo)\n(duas respostas: tempo decorrido com transformação logarítmica e tempo por milha)\n\nPlotar / descrever os dados.\nPlotar / discutir as funções estimadas de média e covariância.\nUtilizar gráficos interativos para explorar os resultados da fPCA e resumir suas descobertas.\n\nAnalisar os dados de Poluentes (atividade para casa)\n(enfoque no nível de sulfato com transformação logarítmica)\n\nPlotar / descrever os dados.\nPlotar / discutir as funções estimadas de média e covariância.\nUtilizar gráficos interativos para explorar os resultados da fPCA e resumir suas descobertas."
  },
  {
    "objectID": "dia05.html#passo-1-função-média",
    "href": "dia05.html#passo-1-função-média",
    "title": "Dia-05",
    "section": "Passo 1: função média",
    "text": "Passo 1: função média\n\ndim(Y)\n\n[1] 340  93\n\nlength(visit.time)\n\n[1] 340\n\nY.vec <- as.vector(t(Y))\ntract <- 1:93\ns <- rep(tract, nrow(Y))\nt <- rep(visit.time, each = length(tract))\n\n\nfit <- gam(Y.vec ~ te(s,t, k = c(10, 7)))\nt1 <- rep(seq(0,1, length.out=81), each = 93)\ns1 <- rep(tract, 81)\n\n\nmu.hat <- matrix(predict(fit, newdata = data.frame(s = s1, t = t1)), 93)\n\npersp(x = tract, y = seq(0,1,length.out=81), z = mu.hat, \n      xlab=\"tract\", ylab=\"visit.time\", zlab = \"mu_hat\")\n\n\n\n\n\n\n\n\nGráfico interativo\nrgl::persp3d(x = tract, y = seq(0,1,length.out=81), z = mu.hat,                xlab=\"tract\", ylab=\"visit.time\", zlab = \"mu_hat\")"
  },
  {
    "objectID": "dia05.html#passo-3-covariância-dos-coeficientes-variáveis-no-tempo",
    "href": "dia05.html#passo-3-covariância-dos-coeficientes-variáveis-no-tempo",
    "title": "Dia-05",
    "section": "Passo 3: Covariância dos coeficientes variáveis no tempo",
    "text": "Passo 3: Covariância dos coeficientes variáveis no tempo\nConstrua um novo conjunto de dados com os coeficientes estimados da base \\((\\xi_{ik}(t_{ij}))\\) e os tempos de visita \\((t_{ij})\\).\n\nscore <- m.fit$scores/sqrt(93)\nid <- MS$ID\n\nsecond.fPCA <- list()\nfor(k in 1:m.fit$npc){\n  ydata <- data.frame(.id = MS$ID, .index = visit.time, \n                      .value = score[,k])\n  second.fPCA[[k]] <- fpca.sc(ydata = ydata, pve = 0.80)\n}\n\nA entrada ydata é outra forma de fornecer dados funcionais; isso é especialmente útil quando você possui dados longitudinais.\n(Atividade em grupo) Plote o segundo resultado do fPCA para diferentes k e discuta."
  },
  {
    "objectID": "dia05.html#passo-2-função-de-covariância-marginal",
    "href": "dia05.html#passo-2-função-de-covariância-marginal",
    "title": "Dia-05",
    "section": "Passo 2: função de covariância marginal",
    "text": "Passo 2: função de covariância marginal\n\ndemeaned <- Y - t(matrix(fit$fitted.values, 93))\n\nSigma <- cov(demeaned)\n\nimage.plot(tract, tract, Sigma, xlab=\"tract\", ylab=\"tract\")\n\n\n\n\n\n\n\n\n\nm.fit <- fpca.face(demeaned, pve = 0.80)\nefn <- m.fit$efunctions*sqrt(93)\neval <- m.fit$evalues/93\n\nmatplot(tract, efn, type='l', lty=1,\n        xlab=\"tract\", ylab=\"eigenfunctions\")\n\n\n\n\n\n\n\n\n\nk.pc <- 1\nm <- colMeans(Y)\neffect <- efn[,k.pc] * sqrt(eval[k.pc])\nmatplot(tract, cbind(m-effect, m+effect), \n        col=c(2,4), pch = c(\"-\", \"+\"), \n        ylab=\"\", xlab=\"tract\", main=paste(\"fPC\", k.pc))\nlines(tract, m, lwd=2)"
  },
  {
    "objectID": "dia05.html#passo-4-reconstrução-de-trajetórias",
    "href": "dia05.html#passo-4-reconstrução-de-trajetórias",
    "title": "Dia-05",
    "section": "Passo 4: Reconstrução de trajetórias",
    "text": "Passo 4: Reconstrução de trajetórias\n\ni <- 1\nfixed_visit.time <- 0.5\n\nmu.hat <- predict(fit, newdata = data.frame(s = tract, \n                                            t = fixed_visit.time))\nscore <- c()\nfor(k in 1:m.fit$npc){\n  temp <- data.frame(Y = second.fPCA[[k]]$Yhat[i,], \n                     t = sort(unique(visit.time)))\n  temp.fit <- gam(Y ~ s(t), data = temp)\n  score <- c(score, predict(temp.fit, \n                            newdata = data.frame(t = fixed_visit.time)))\n}\n\nYhat <- as.vector(mu.hat) + \n  as.vector(m.fit$mu) + as.vector(t(score)%*%t(efn))\nplot(tract, Yhat, type='l', lwd=2,\n     xlab=\"tract\", ylab=\"predicted\")"
  },
  {
    "objectID": "dia05.html#passo-1.1-função-média",
    "href": "dia05.html#passo-1.1-função-média",
    "title": "Dia-05",
    "section": "Passo 1.1: função média",
    "text": "Passo 1.1: função média\n\ndim(Y)\n\n[1] 340  93\n\nlength(visit.time)\n\n[1] 340\n\nY.vec <- as.vector(t(Y))\ntract <- 1:93\ns <- rep(tract, nrow(Y))\nt <- rep(visit.time, each = length(tract))\n\n\nfit <- gam(Y.vec ~ te(s,t, k = c(10, 7)))\nt1 <- rep(seq(0,1, length.out=81), each = 93)\ns1 <- rep(tract, 81)\n\n\nmu.hat <- matrix(predict(fit, newdata = data.frame(s = s1, t = t1)), 93)\n\npersp(x = tract, y = seq(0,1,length.out=81), z = mu.hat, \n      xlab=\"tract\", ylab=\"visit.time\", zlab = \"mu_hat\")\n\n\n\n\n\n\n\n\nGráfico interativo\n\n# rgl::persp3d(x = tract, y = seq(0,1,length.out=81), z = mu.hat, \n#               xlab=\"tract\", ylab=\"visit.time\", zlab = \"mu_hat\")"
  },
  {
    "objectID": "dia05.html#passo-1.2-função-de-covariância-marginal",
    "href": "dia05.html#passo-1.2-função-de-covariância-marginal",
    "title": "Dia-05",
    "section": "Passo 1.2: função de covariância marginal",
    "text": "Passo 1.2: função de covariância marginal\n\ndemeaned <- Y - t(matrix(fit$fitted.values, 93))\n\nSigma <- cov(demeaned)\n\nimage.plot(tract, tract, Sigma, xlab=\"tract\", ylab=\"tract\")\n\n\n\n\n\n\n\n\n\nm.fit <- fpca.face(demeaned, pve = 0.80)\nefn <- m.fit$efunctions*sqrt(93)\neval <- m.fit$evalues/93\n\nmatplot(tract, efn, type='l', lty=1,\n        xlab=\"tract\", ylab=\"eigenfunctions\")\n\n\n\n\n\n\n\n\n\nk.pc <- 1\nm <- colMeans(Y)\neffect <- efn[,k.pc] * sqrt(eval[k.pc])\nmatplot(tract, cbind(m-effect, m+effect), \n        col=c(2,4), pch = c(\"-\", \"+\"), \n        ylab=\"\", xlab=\"tract\", main=paste(\"fPC\", k.pc))\nlines(tract, m, lwd=2)"
  },
  {
    "objectID": "dia05.html#passo-1.3-covariância-dos-coeficientes-variáveis-no-tempo",
    "href": "dia05.html#passo-1.3-covariância-dos-coeficientes-variáveis-no-tempo",
    "title": "Dia-05",
    "section": "Passo 1.3: Covariância dos coeficientes variáveis no tempo",
    "text": "Passo 1.3: Covariância dos coeficientes variáveis no tempo\nConstrua um novo conjunto de dados com os coeficientes estimados da base \\((\\xi_{ik}(t_{ij}))\\) e os tempos de visita \\((t_{ij})\\).\n\nscore <- m.fit$scores/sqrt(93)\nid <- MS$ID\n\nsecond.fPCA <- list()\nfor(k in 1:m.fit$npc){\n  ydata <- data.frame(.id = MS$ID, .index = visit.time, \n                      .value = score[,k])\n  second.fPCA[[k]] <- fpca.sc(ydata = ydata, pve = 0.80)\n}\n\nA entrada ydata é outra forma de fornecer dados funcionais; isso é especialmente útil quando você possui dados longitudinais.\n(Atividade em grupo) Plote o segundo resultado do fPCA para diferentes k e discuta."
  },
  {
    "objectID": "dia05.html#passo-1.4-reconstrução-de-trajetórias",
    "href": "dia05.html#passo-1.4-reconstrução-de-trajetórias",
    "title": "Dia-05",
    "section": "Passo 1.4: Reconstrução de trajetórias",
    "text": "Passo 1.4: Reconstrução de trajetórias\n\ni <- 1\nfixed_visit.time <- 0.5\n\nmu.hat <- predict(fit, newdata = data.frame(s = tract, \n                                            t = fixed_visit.time))\nscore <- c()\nfor(k in 1:m.fit$npc){\n  temp <- data.frame(Y = second.fPCA[[k]]$Yhat[i,], \n                     t = sort(unique(visit.time)))\n  temp.fit <- gam(Y ~ s(t), data = temp)\n  score <- c(score, predict(temp.fit, \n                            newdata = data.frame(t = fixed_visit.time)))\n}\n\nYhat <- as.vector(mu.hat) + \n  as.vector(m.fit$mu) + as.vector(t(score)%*%t(efn))\nplot(tract, Yhat, type='l', lwd=2,\n     xlab=\"tract\", ylab=\"predicted\")\n\n\n\n\n\n\n\n\n(Atividade em grupo) Plote a trajetória prevista da FA para um paciente com EM diferente / um tempo de visita diferente."
  },
  {
    "objectID": "dia04.html",
    "href": "dia04.html",
    "title": "Dia-04",
    "section": "",
    "text": "Os modelos lineares funcionais são uma classe de modelos estatísticos que lidam com dados funcionais, ou seja, observações que são representadas como funções contínuas ao invés de valores numéricos isolados. Esses modelos são amplamente utilizados para análise de dados funcionais em várias áreas, como biologia, economia, engenharia e ciências da saúde.\nA ideia básica por trás dos modelos lineares funcionais é estender a estrutura dos modelos lineares tradicionais para acomodar dados funcionais. Em um modelo linear funcional, a variável de resposta é modelada como uma combinação linear de funções base, onde os coeficientes de regressão indicam o efeito das funções base na resposta. Essas funções base podem ser pré-determinadas, como funções polinomiais ou splines, ou podem ser obtidas de forma adaptativa a partir dos próprios dados, por exemplo, usando a Análise de Componentes Principais Funcionais (fPCA).\nAjustar um modelo linear funcional envolve estimar os coeficientes de regressão e possivelmente outros parâmetros do modelo, como a variância do erro. Diversas técnicas de estimação estão disponíveis para modelos lineares funcionais, incluindo mínimos quadrados ponderados, estimação por máxima verossimilhança e métodos baseados em penalização.\nUma vantagem dos modelos lineares funcionais é que eles permitem modelar a estrutura temporal dos dados e capturar a variabilidade e dependência funcional ao longo do tempo. Isso torna esses modelos particularmente adequados para dados longitudinais ou séries temporais, onde a variação e a relação entre as observações podem mudar ao longo do tempo.\nEm resumo, os modelos lineares funcionais são uma poderosa ferramenta estatística para análise de dados funcionais, permitindo modelar e interpretar relações entre variáveis em um contexto funcional e temporal."
  },
  {
    "objectID": "dia04.html#modelo-de-regressão-escalar-em-função",
    "href": "dia04.html#modelo-de-regressão-escalar-em-função",
    "title": "Dia-04",
    "section": "Modelo de regressão escalar em função",
    "text": "Modelo de regressão escalar em função"
  },
  {
    "objectID": "dia04.html#scalar-on-function-regression-model",
    "href": "dia04.html#scalar-on-function-regression-model",
    "title": "Dia-04",
    "section": "1 Scalar-on-function regression model",
    "text": "1 Scalar-on-function regression model\n“Scalar-on-function regression model” pode ser traduzido como “modelo de regressão escalar em função”. Nesse contexto, “scalar” se refere a um valor único, enquanto “function” se refere a uma função matemática que pode variar de acordo com uma variável independente. Portanto, um “scalar-on-function regression model” é um modelo de regressão que visa prever um valor único (escalar) com base em uma função variável.\n\nmarathon_results_2017 &lt;-readr::read_csv(\"marathon_results_2017.csv\")\n\n\nmarathon_df &lt;- marathon_results_2017 %&gt;% \ndplyr::select(-c(`...1`,\n            Bib,City,\n            State,\n            Country,\n            Citizen,\n            `...10`,\n            Division,\n            Gender,\n            Overall,\n            `Proj Time`)) %&gt;% \n  janitor::clean_names()\n\nmutate_seconds &lt;- function(x){\n  lubridate::hour(x)* 3600 + lubridate::minute(x) * 60 + lubridate::second(x) *1\n}\n  \n\n marathon_df &lt;- \n   marathon_df %&gt;% \n        drop_na() %&gt;% \n        mutate(\n          across(starts_with(c(\"x\",\"half\")), lubridate::hms),\n          across(starts_with(c(\"x\",\"half\",\"pace\", \"official_time\")), mutate_seconds),\n          `5` = (5000/ `x5k`)*3.6,\n          `10` = (10000/ `x10k`)*3.6,\n          `15` = (15000/ `x15k`)*3.6,\n          `20` = (20000/ `x20k`)*3.6,\n          `21` = (21000/ half)*3.6,\n          `25` = (25000/ `x25k`)*3.6,\n          `30` = (30000/ `x30k`)*3.6,\n          `35` = (35000/ `x35k`)*3.6,\n          `40` = (40000/ `x40k`)*3.6,\n          `42` = (42000/ official_time)*3.6\n            ) %&gt;% \n   rename(\n     names = name\n   )\n \n \nmarathon_df %&gt;% \n  dplyr::slice(1:105) %&gt;% \n  pivot_longer(cols = 15:24) %&gt;% \n  mutate(\n    name = as.numeric(name)\n  ) %&gt;% \n  group_by(name) %&gt;% \n  ggplot(aes(name,value, group = names, color = m_f))+\n  geom_line(alpha = 0.7, color = \"grey\")+\n  #geom_point(color = \"black\", alpha = 0.7)+\n  ylab(\"Velocidade Km/h\")+\n  xlab(\"Km de maratona\")+\n  ggtitle(\"Maratona - Velocidade\")\n\n\n\n\n\n\n\n\nPergunta de interesse: Existe alguma relação entre o tempo total decorrido e o ritmo da corrida (perfis de milhas por minuto)?\nAgora podemos responder a essas perguntas usando o modelo de regressão linear funcional com resposta escalar e covariável funcional! (SoF)\nLembrando:\n\\[\nY_i = \\mu_Y + \\int X_i(t) \\beta(t) dt + \\epsilon_i,\n\\]\nonde Xi é modelado usando fPCA, \\(X_i = \\mu(t) + \\sum_{i=1}^{n} \\sum_{k=1}^{\\infty} \\xi_{ik}\\phi_{k}(t)t)\\)\nLembrando que fPCA se refere à Análise de Componentes Principais Funcionais (Functional Principal Component Analysis) e \\(\\mu(t)\\) é a média da função, ξᵢₖ são os coeficientes de PCA e \\(\\phi_k(t)\\) são as funções de base de PCA."
  },
  {
    "objectID": "dia04.html#regressão-de-componentes-principais-funcionais-fpc",
    "href": "dia04.html#regressão-de-componentes-principais-funcionais-fpc",
    "title": "Dia-04",
    "section": "1.1 Regressão de componentes principais funcionais (FPC)",
    "text": "1.1 Regressão de componentes principais funcionais (FPC)\nAqui ilustramos o ajuste de regressão linear funcional assumindo \\(\\beta(t) = \\sum_{i=1}^{n} \\sum_{k=1}^{\\infty} \\beta_{k}\\phi_{k}(t)\\). Primeiro carregamos o conjunto de dados e definimos a resposta e a covariável.\n\nPrimeiro passo\nO primeiro passo da estimativa é executar o fPCA na covariável funcional usando um dos softwares que implementam o fPCA (módulo 3); por exemplo, fpca.ssvd, fpca.face e fpca.sc no pacote refund / pca.fd no pacote fda / fpca.mle no pacote fpca / PACE no Matlab.\n\nspeed &lt;- marathon_df %&gt;% \ndplyr::select(15:24) %&gt;% \ndplyr::slice(1:150) %&gt;% \n  as.matrix()\n\nkms &lt;- c(5,10,15,20,21,25,30,35,40,42)\n\nfinal_time &lt;- marathon_df$official_time %&gt;% \n  as_tibble() %&gt;% \n  dplyr::slice(1:150) %&gt;% \n  as.matrix()\n\nfpca_res &lt;- fpca.face(Y= speed ,pve = 0.97, argvals = kms, knots = 6)\n\n#fpca_res &lt;- fpca.sc(X, argvals = miles, pve = 0.97)\n\nm &lt;- length(kms)\nefn &lt;- fpca_res$efunctions*sqrt(m)\neval &lt;- fpca_res$evalues/m\nscr &lt;- fpca_res$scores/sqrt(m)\nnpc &lt;- fpca_res$npc\n\n\ncbind(kms,efn) %&gt;% \n  as_tibble() %&gt;% \n  rename(\n    pc1 = V2,\n    pc2 = V3\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"pc\")) %&gt;% \n  group_by(kms) %&gt;% \n  ggplot(aes(kms,value,group = name, color = name))+\n  geom_line()+\n  labs(x=\"km\",y=\"\", title = \"Autofunções estimadas\" )\n\n\n\n\n\n\n\n\n\nk.pc &lt;- 1\neffect &lt;- sqrt(eval[k.pc])*efn[,k.pc]\nmu_hat &lt;- fpca_res$mu\n\n\ncbind(kms,efn[,k.pc]) %&gt;% \n  as_tibble() %&gt;% \n  ggplot(aes(kms, V2))+\n  geom_line()+\n  ylim(c(-2,2))+\n  labs(x=\"km\",y=\"\", title = \"fCP-1\" )\n\n\n\n\n\n\n\nmu_hat %&gt;% \n  as_tibble_col(column_name = \"mu.hat\") %&gt;% \n  mutate(\n    effect_Plus = mu.hat + effect,\n    effect_less = mu.hat - effect,\n    kms = kms\n  ) %&gt;% \n  pivot_longer(cols = 1:3) %&gt;% \n  group_by(kms) %&gt;% \n  ggplot(aes(kms, value, group = name, color = name))+\n  geom_line()+\n  labs(x=\"km\",y=\"\", title = \"fCP-1\")\n\n\n\n\n\n\n\n\n(Atividade para fazer em casa) Experimente o gráfico interativo dos resultados do fPCA (plot_shiny(fpca_res)) e interprete. (Módulo 3-3)\n\n\nPasso dois\nAgora, usando a matriz de escores estimados, faça uma regressão linear múltipla no vetor de respostas escalares \\(Y\\) (tempo de conclusão). Obtenha os coeficientes estimados, \\(\\beta_j\\)’s.\n\nout = lm(final_time ~ scr) ## Multiple linear regression\n# summary(out)\nbeta_hat = out$coefficients\nbeta_hat\n\n(Intercept)        scr1        scr2 \n  9013.1234    424.5687   -894.1181 \n\nsummary(out)\n\n\nCall:\nlm(formula = final_time ~ scr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-176.316  -34.756    2.237   29.209  177.537 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9013.123      4.523 1992.94   &lt;2e-16 ***\nscr1         424.569      4.796   88.53   &lt;2e-16 ***\nscr2        -894.118     22.536  -39.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.39 on 147 degrees of freedom\nMultiple R-squared:  0.9846,    Adjusted R-squared:  0.9844 \nF-statistic:  4706 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nUma vez que o fPCA selecionou os três primeiros componentes principais com base na porcentagem especificada da variância explicada, temos aqui três coeficientes de base correspondentes.\nAgora, para reconstruir a função do coeficiente de regressão,\n\nbeta_fn_hat = efn %*% as.matrix(beta_hat[-1], col = 1) \n\ncbind(kms,beta_fn_hat) %&gt;% \n  as_tibble() %&gt;% \n  ggplot(aes(kms,V2))+\n  geom_line()+\n  ylim(c(-2000,800))+\n  ylab(\"\")\n\n\n\n\n\n\n\n\nComo podemos dar sentido ao coeficiente? Vamos nos concentrar em analisar três perfis de velocidade aleatórios:\n\nset.seed(12)\nn.crv &lt;- 3\nn &lt;- nrow(speed)\nsel.crv &lt;- sample(1:n, size=n.crv, replace = FALSE)\n\nrand_speed &lt;- t(fpca_res$Yhat[sel.crv,])\n\ncbind(kms,rand_speed) %&gt;% \n  as_tibble() %&gt;% \n  pivot_longer(cols = contains(\"V\")) %&gt;% \n  group_by(kms) %&gt;% \n  ggplot(aes(kms,value,group = name,color=name))+\n  geom_line()+\n  ylab(\"\")\n\n\n\n\n\n\n\npar(mfrow=c(3,3))\nfor(i in 1:3){\n  ind &lt;- sel.crv[i]\n  demeaned &lt;- fpca_res$Yhat[ind,]-as.vector(fpca_res$mu)\n  \n  matplot(kms, t(fpca_res$Yhat[sel.crv,]-t(matrix(rep(fpca_res$mu,3), nrow=10))), \n          type='l', lwd=2, lty=1, col = 'light grey',\n          xlab=\"miles\", ylab=\"speed (demeaned)\", main=\"\")\n  lines(kms, demeaned, type='l', lwd=2, col='red')\n  \n  \n  plot(kms, beta_fn_hat, type='l', lwd=2,\n       xlab=\"miles\", ylab = \"estimated coefficient fn\", main=\"\")\n  plot(kms, demeaned*beta_fn_hat,type='l', lwd=2, col='blue',\n       xlab=\"miles\", ylab = \"\", ylim=c(-55, 70),\n       main=round(mean(demeaned*beta_fn_hat), 2))\n}\n\n\n\n\n\n\n\n\nPor último, vamos analisar a bondade de ajuste estimada.\n\npar(mfrow=c(1,1))\nplot(final_time, out$fitted, cex=0.5, ylab=\"Fitted\", xlab=\"Observed\")\nabline(a = 0, b = 1)\n\n\n\n\n\n\n\nRsq = 1-sum((out$residuals)^2)/sum((final_time- mean(final_time))^2)\nRsq\n\n[1] 0.984621\n\n\nExistem várias funções integradas que podem ajustar um modelo linear funcional usando fPCA: a função PACE-REG no pacote Matlab PACE; a função pfr no pacote refund; a função fRegress no pacote fda.\nVantagens: computacionalmente simples e aplicável a qualquer design de amostragem. Desvantagens: forte pressuposto de que \\(\\beta(\\cdot)\\) e \\(X(\\cdot)\\) estão no mesmo espaço e têm uma suavidade similar."
  },
  {
    "objectID": "dia04.html#modelo-linear-funcional-com-base-mista",
    "href": "dia04.html#modelo-linear-funcional-com-base-mista",
    "title": "Dia-04",
    "section": "2 Modelo linear funcional com base mista",
    "text": "2 Modelo linear funcional com base mista\nPara superar algumas limitações do método anterior, Goldsmith et al. (2011) propuseram modelar a função de coeficiente \\(\\beta(\\cdot)\\)) usando uma função de base truncada; no entanto, outras funções de base também são aplicáveis.\nLembre-se de que, ao modelar \\(X(\\cdot)\\) usando eigenfunções e \\(\\beta(\\cdot)\\) usando funções de base pré-definidas, temos -\n\\[\nY_i = \\alpha + \\xi^T_i J \\beta,\n\\]\nonde \\(\\boldsymbol{\\xi}_i = [ \\xi_{i1}, \\xi_{i2}, \\ldots, \\xi_{iK} ]^T, J\\) é uma matriz \\(K \\times L\\) com o elemento \\((k,ℓ)-th\\) dado por \\(J_{k\\ell} = \\int \\phi_k(t)\\theta_\\ell(t) dt\\), e \\(\\boldsymbol{\\beta} = [ \\beta_1, \\beta_2, \\ldots, \\beta_K ]^T\\).\nEste modelo pode ser ajustado usando a função pfr no pacote refund. (Novamente, usamos “milhas por minuto” e “tempo de conclusão” como covariável e resposta)\n\nX &lt;- as.matrix(speed) # functional covariate\nY &lt;- final_time    # scalar response\n\n\nmyDat &lt;- data.frame(X, Y)\n\nfit &lt;- pfr(Y ~ lf(X, k = 10, bs = \"cr\"), method = \"REML\", data = myDat)\ncoef &lt;- coef(fit)\n\n\ncbind(coef$X.argvals, coef$value) %&gt;% \n  as_tibble() %&gt;% \n  ggplot(aes(V1,V2))+\n  geom_line()+\n  labs(x = \"km\", y=expression(paste(beta(t))), title = \"função de coeficiente estimado\")\n\n\n\n\n\n\n\n\n\npar(mfrow=c(3,3))\n\nbeta_fn_hat0 &lt;- beta_fn_hat  # saving beta(t) from fPCA approach\n\nfit &lt;- pfr(Y ~ lf(X, k = 10, bs = \"cr\"), method = \"REML\", data = myDat)\ncoef &lt;- coef(fit)\nbeta_fn_hat &lt;- coef$value\n\nfor(i in 1:3){\n  ind &lt;- sel.crv[i]\n  demeaned &lt;- fpca_res$Yhat[ind,]-as.vector(fpca_res$mu)\n  \n  matplot(kms, t(fpca_res$Yhat[sel.crv,]-t(matrix(rep(fpca_res$mu,3), nrow=10))), \n          type='l', lwd=2, lty=1, col = 'light grey',\n          xlab=\"miles\", ylab=\"speed (demeaned)\", main=\"\")\n  lines(kms, demeaned, type='l', lwd=2, col='red')\n  \n  \n  plot(kms, beta_fn_hat, type='l', lwd=2,\n       xlab=\"miles\", ylab = \"estimated coefficient fn\", main=\"\")\n  plot(kms, demeaned*beta_fn_hat,type='l', lwd=2, col='blue',\n       xlab=\"miles\", ylab = \"\", main=round(mean(demeaned*beta_fn_hat), 2)) \n}\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\nplot( rowSums( sapply(1:2, function(a) (t(efn) %*% beta_fn_hat/10)[a] * efn[,a]) ) , type='l', lwd=2, col = \"red\", ylab=\"\", xlab=\"miles\")\nlines(kms, beta_fn_hat0, type='l', lwd=2)\n\n\n\n\n\n\n\n\nO comando plot(fit) plota a função de coeficientes estimada com intervalo de confiança ponto a ponto, o que NÃO é útil para inferência! Uma possível maneira de construir um intervalo de confiança conjunto é por meio do método de bootstrap.\n\nfpca_res &lt;- fpca.face(X ,pve = 0.97, argvals = kms, knots = 6)\nXhat &lt;- fpca_res$Yhat\nYhat &lt;- predict(fit, newdata = list(X = Xhat))\n\n# goodness-of-fit\npar(mfrow=c(1,1))\nplot(Y, Yhat, cex=0.5, ylab=\"Fitted\", xlab=\"Observed\")\nabline(a = 0, b = 1)\n\n\n\n\n\n\n\nRsq = 1-sum((Y- as.vector(Yhat))^2)/sum((Y - mean(Y))^2)\nRsq\n\n[1] 0.9841658\n\n\nA função pfr é flexível para adicionar mais de uma covariável funcional e/ou escalar; por exemplo, pfr(Y ~ X0 + lf(X1) + lf(X2)) para ajustar.\n\\[\nY_i = \\mu_Y + \\beta_0 X_0 + \\int X_{1i}(t) \\beta_1(t) dt + \\int X_{2i}(t) \\beta_2(t) dt + \\epsilon_i.\n\\]\nAlém disso, também pode ser usado para ajustar regressão funcional não linear,\n\\[\nY_i = \\mu_Y + \\int F\\{X_i(t), t\\} dt + \\epsilon_i.\n\\]\nonde \\(F(\\cdot,\\cdot)\\) é uma função suave bivariada desconhecida;\n\n#fit &lt;- pfr(Y ~ af(X, k = c(10, 8), bs = \"cr\"))\n\nfit &lt;- pfr(Y ~ af(X, k = c(10, 8), bs = \"cr\"))\ncoef &lt;- coef(fit)\n\nplot(fit$fitted.values,Y)\nabline(a = 0, b = 1)"
  },
  {
    "objectID": "dia04.html#regressão-de-função-em-escalar-fos",
    "href": "dia04.html#regressão-de-função-em-escalar-fos",
    "title": "Dia-04",
    "section": "3 Regressão de Função em Escalar (FoS)",
    "text": "3 Regressão de Função em Escalar (FoS)\nA regressão de função em escalar (FoS) é uma abordagem estatística que lida com a relação entre uma função e uma variável escalar. Nesse tipo de análise, a variável de interesse é uma função contínua ao longo de uma dimensão, como o tempo, enquanto a variável preditora é uma única medida numérica.\nA FoS tem várias aplicações em diferentes campos, como medicina, economia, ecologia e engenharia. Por exemplo, na medicina, pode ser usado para estudar a relação entre o perfil de expressão gênica (função) e uma variável clínica (escalar), como a gravidade de uma doença.\nUma das principais vantagens da FoS é que ela permite modelar a relação entre a função e a variável escalar de forma flexível, capturando padrões complexos e não lineares. Isso é especialmente útil quando a relação entre as duas variáveis é esperada para variar ao longo da dimensão da função.\nExistem várias abordagens e métodos para realizar a regressão de função em escalar, incluindo o uso de bases funcionais, como splines, wavelets e Fourier, além de técnicas específicas, como regressão de spline penalizada e modelos de mistura.\nEm resumo, a regressão de função em escalar é uma ferramenta poderosa para explorar e modelar a relação entre funções e variáveis escalares, permitindo uma análise mais detalhada e flexível dos dados em várias áreas de estudo.\nNeste estudo, serão utilizados dados meteorológicos do Canadá para fins de análise.\n\ndata(\"CanadianWeather\")\n\n# Temperature data\ndaily_avg_temp &lt;- \n    CanadianWeather$dailyAv %&gt;% \n    as_tibble() %&gt;% \n    dplyr::select(contains(\"Temperature\")) %&gt;% \n    janitor::clean_names()\n\n\n# temperature plot\ndaily_avg_temp %&gt;% \n  mutate(\n    day = 1:365\n  ) %&gt;% \n  pivot_longer(cols = contains(\"Temperature\")) %&gt;% \n  group_by(day) %&gt;% \n  ggplot(aes(day,value, group = name, color = name)) + \n  geom_line()+\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n# precipitation data\ndaily_avg_prec &lt;-\n      CanadianWeather$dailyAv %&gt;% \n      as_tibble() %&gt;% \n      dplyr::select(contains(\"Precipitation\")) %&gt;% \n      janitor::clean_names()\n\n# precipitation plot\ndaily_avg_prec %&gt;% \n  mutate(\n    day = 1:365\n  ) %&gt;% \n  pivot_longer(cols = contains(\"Precipitation\")) %&gt;% \n  group_by(day) %&gt;% \n  ggplot(aes(day,value, group = name, color = name)) + \n  geom_line()+\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n# Total avg data \ntotal_avg_prec &lt;- \n  daily_avg_prec %&gt;% \n  pivot_longer(\n    cols = contains(\"Precipitation\")\n    ) %&gt;% \n  group_by(fct_inorder(name)) %&gt;% \n  summarise(\n    avg_mm = sum(value)\n  ) %&gt;% \n  rename(\n    name = `fct_inorder(name)`\n  ) %&gt;% \n  mutate(\n     name = str_remove(name,\"_precipitation_mm\")\n   ) \n\n\n# Total avg plot \ntotal_avg_prec %&gt;% \n  ggplot(aes(fct_inorder(name),avg_mm))+\n  geom_point()+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  ylim(0,max(total_avg_prec$avg_mm))\n\n\n\n\n\n\n\n\nQuestão de interesse: Qual é a associação entre a precipitação anual total e a curva de temperatura diária?\nPodemos usar um modelo de regressão de função em escalar para responder a essa pergunta:\n\\[\nTemp_i(t) = \\beta_0(t) + \\beta_1(t) \\cdot TotalPreci + \\epsilon_i(t)\n\\]\nPrimeiro, definimos nossa variável de resposta e nossa variável preditora.\n\nday &lt;- 1:365\n\nY &lt;- \n  daily_avg_temp %&gt;% \n  as.matrix() %&gt;% \n  t()\n\nX &lt;-\n  total_avg_prec %&gt;% \n  dplyr::select(-name) %&gt;% \n  as_vector()\n\nmyDat &lt;- data.frame(X = X, Y= Y)\n\ndim(Y);length(X)\n\n[1]  35 365\n\n\n[1] 35\n\n\nE ajustamos o modelo de regressão de função em escalar usando a função pffr. (A função pffr no pacote refund pode ajustar qualquer modelo linear funcional com resposta funcional.)\n\nfit &lt;- pffr(Y ~ X, data = myDat)\nyhat &lt;- predict(fit, newdata = myDat) \n  \n\nRsq_t &lt;- 1-colSums((Y - yhat)^2) / colSums((Y - colMeans(Y))^2)\nmean(Rsq_t)\n\n[1] 0.7297034\n\n\n\ny_pivot &lt;- Y %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n  mutate(\n    day = 1:365\n  ) %&gt;% \n  pivot_longer(cols = contains(\"_c\"), values_to = \"true_value\", names_to = \"station\")\n\n\nyhat_pivot &lt;- yhat %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n  pivot_longer(cols = contains(\"V\"), values_to = \"fitted\", names_to = \"station_fitted\")\n\n\ntibble(y_pivot, yhat_pivot) %&gt;% \n  group_by(day) %&gt;% \n  ggplot(aes(day, true_value, group = station))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(day, fitted, group = station_fitted, color = station_fitted))+\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nA função de coeficientes estimados é:\n\ncoef &lt;- coef(fit)\n\nusing seWithMean for  s(yindex.vec) .\n\nbeta0.hat &lt;- coef$smterms$`Intercept(yindex)`$coef\nbeta1.hat &lt;- coef$smterms$`X(yindex)`$coef\n\ntibble(index = beta0.hat$yindex.vec, value = beta0.hat$value) %&gt;% \n  ggplot(aes(index,value))+\n  geom_line()+\n  labs(x = \"day\", y = expression(paste(beta[0](t))), title = \"\")\n\n\n\n\n\n\n\ntibble(index = beta1.hat$yindex.vec, value = beta1.hat$value)%&gt;% \n  ggplot(aes(index,value))+\n  geom_line()+\n  labs(x = \"day\", y = expression(paste(beta[1](t))), title = \"\")\n\n\n\n\n\n\n\n\nOutras funções que podem ajustar regressão de função em escalar são bayes_fosr e fosr no pacote refund e fRegress no pacote fda. Note que a seleção dos parâmetros de suavização não está implementada na função fRegress. Enquanto isso, fosr pode receber tanto uma matriz como um objeto fd do pacote fda, além de poder selecionar parâmetros de suavização ótimos usando diversos métodos, como GCV, REML, ML, entre outros (?fosr).\nbayes_fosr utiliza estimação Bayesiana e plot_shiny recebe a saída de bayes_fosr para plotagens interativas.\n\n#fit &lt;- bayes_fosr(Y ~ X)\n#plot_shiny(fit)\n\nIsso retorna cinco abas com gráficos interativos:\nAba 1: Dados Observados (resposta observada colorida com base na covariável selecionada pelo usuário.)\n\nAba 2: Valores Ajustados (curvas previstas com diferentes valores da covariável)\nAba 3: Funções de Coeficientes (funções de coeficientes estimadas)\nAba 4: Resíduos (curvas de resíduos coloridas com base em sua profundidade)\n\n(Atividade em grupo) Explore os gráficos interativos."
  },
  {
    "objectID": "dia04.html#regressão-de-função-em-função-fof---modelo-concorrente",
    "href": "dia04.html#regressão-de-função-em-função-fof---modelo-concorrente",
    "title": "Dia-04",
    "section": "4 Regressão de função em função (FoF) - Modelo Concorrente",
    "text": "4 Regressão de função em função (FoF) - Modelo Concorrente\nA regressão de função em função (FoF) é uma abordagem estatística que permite modelar a relação entre duas funções contínuas ao longo de uma dimensão comum. No contexto do modelo concorrente, a FoF é usada para investigar a relação entre uma função resposta e uma função preditora, ambas observadas na mesma dimensão temporal.\nNo modelo concorrente, a função resposta é modelada como uma combinação linear das funções preditoras, ponderadas por coeficientes de regressão desconhecidos. Esses coeficientes indicam como a função resposta é influenciada pelas diferentes características da função preditora.\nPara ajustar o modelo FoF concorrente, são utilizadas técnicas estatísticas como mínimos quadrados parciais ou máxima verossimilhança. O objetivo é estimar os coeficientes de regressão para descrever a relação entre as funções resposta e preditora.\nO modelo FoF concorrente pode ser aplicado em várias áreas, como ciências ambientais, medicina, economia e engenharia, onde existem dados funcionais coletados ao longo do tempo. Ele fornece uma abordagem flexível para modelar a complexa relação funcional entre duas variáveis observadas em uma dimensão comum.\nO modelo de regressão funcional concorrente\n\\[\nTemp_i(t) = \\beta_0(t) + \\beta_1(t) \\cdot Preci(t) + \\epsilon_i(t)\n\\]\nrelaciona a temperatura média diária no ponto de tempo atual t com a precipitação média diária no mesmo ponto de tempo t.\n\ndata(\"CanadianWeather\")\n\nday &lt;- 1:365\n# selecionando os dados de temperatura de todas as estações - 365 dias\nY &lt;- t(as.matrix(CanadianWeather$dailyAv[,,1]))\n# criando Fpca com os dados de precipitação\nfit &lt;- fpca.sc(t(as.matrix(CanadianWeather$dailyAv[,,2])), pve=0.99)\n#selecionando os dados ajustados criados pela Fpca\nX &lt;- fit$Yhat \n\n\nmyDat &lt;- list()\nmyDat$X &lt;- X\nmyDat$Y &lt;- Y\n\n# Criando um modelo de regressão usando pffr function\nfit &lt;- pffr(Y ~ X, data = myDat)\nyhat &lt;- predict(fit, newdata = myDat)\nRsq_t &lt;- 1-colSums((Y - yhat)^2) / colSums((Y - colMeans(Y))^2)\nmean(Rsq_t) # erro médio quadratico\n\n[1] 0.7018062\n\n\nEste modelo explica cerca de 77% da variabilidade total. As funções de coeficiente estimadas são:\n\ncoef &lt;- coef(fit)\n\nusing seWithMean for  s(yindex.vec) .\n\nbeta0.hat &lt;- coef$smterms$`Intercept(yindex)`$coef\nbeta1.hat &lt;- coef$smterms$`X(yindex)`$coef\n\n\n\ntibble(index = beta0.hat$yindex.vec, value = beta0.hat$value) %&gt;% \n  ggplot(aes(index,value))+\n  geom_line()+\n  labs(x = \"day\", y = expression(paste(beta[0](t))), title = \"\")\n\n\n\n\n\n\n\ntibble(index = beta1.hat$yindex.vec, value = beta1.hat$value)%&gt;% \n  ggplot(aes(index,value))+\n  geom_line()+\n  labs(x = \"day\", y = expression(paste(beta[1](t))), title = \"\")\n\n\n\n\n\n\n\n\nA função fRegress no pacote fda também ajusta a regressão funcional concorrente, mas a seleção dos parâmetros de suavização não está implementada na função.\nResumo: - Regressão escalar em função - Dados de maratona - Gráficos interativos (plot_shiny) - Regressão de função em escalar e regressão de função em função - Dados meteorológicos do Canadá - Precipitação média como covariável escalar - Precipitação e temperatura como função de localização\nAtividades em grupo e individuais: - Analise os dados de DTI usando os modelos de regressão que aprendemos hoje. - Qual é a associação entre os perfis de FA e as pontuações de PASAT de pacientes com esclerose múltipla em sua primeira visita? (regressão escalar em função) - O conjunto de dados também inclui rcst - perfis de FA coletados do trato corticospinal direito. Como essas medidas se relacionam com as medidas de FA ao longo de CCA? - Ajuste um modelo linear funcional com FA ao longo de CCA como resposta e pontuações de PASAT como covariável; experimente diferentes funções R. Discuta os gráficos interativos dos resultados da regressão de função em escalar.\nVamos usar o conjunto de dados DTI para ilustrar dados funcionais observados longitudinalmente amanhã. Tente plotar vários perfis observados de um paciente com EM selecionado aleatoriamente. (Atividade individual)"
  },
  {
    "objectID": "dia01.html",
    "href": "dia01.html",
    "title": "Dia-01",
    "section": "",
    "text": "Breve descrição dos pacotes utilizados\nIntrodução aos dados utilizados\n\n\n\nDados Funcionais: Dados funcionais são uma forma de representar informações observadas ao longo de uma dimensão contínua, geralmente o tempo. Em vez de ter observações pontuais em momentos específicos, os dados funcionais registram a evolução de uma variável ao longo de uma curva contínua, muitas vezes representada como uma função. Essa função pode ser descrita por uma série de pontos discretos ou através de uma representação contínua, como uma curva suave. Dados funcionais são frequentemente utilizados em áreas como análise de séries temporais, análise de imagens médicas, processamento de sinais, entre outras.\nDados Longitudinais: Dados longitudinais referem-se a dados coletados repetidamente de um mesmo indivíduo, objeto ou unidade de estudo ao longo do tempo. Nesse tipo de dado, o foco está na observação das mudanças que ocorrem em uma variável ou conjunto de variáveis ao longo de diferentes momentos. Os dados longitudinais permitem analisar tendências, padrões de crescimento, estabilidade ou mudanças em uma população ao longo do tempo. São comumente utilizados em estudos longitudinais, pesquisas de acompanhamento de indivíduos, estudos de coorte e estudos de desenvolvimento, entre outros.\nEm resumo, dados funcionais representam a evolução de uma variável em uma dimensão contínua, enquanto dados longitudinais referem-se a observações repetidas de uma mesma unidade de estudo ao longo do tempo. Ambos os tipos de dados são valiosos para análises estatísticas e científicas em diferentes áreas de estudo.\n\n\n\n\nif(!require(pacman)) install.packages(c(\"pacman\", # instalando os pacotes necessários\n                                        \"tidyverse\",\n                                        \"lme4\",\n                                        \"mgcv\",\n                                        \"refund\",\n                                        \"face\",\n                                        \"fda\", \n                                        \"rgl\",\n                                        \"fields\",\n                                        \"refund.shiny\",\n                                        \"janitor\"))\n\nCarregando os pacotes simultaneamente utilizando o pacote pacman\n\npacman::p_load(tidyverse, lme4, mgcv, refund, face, fda, rgl, fields, refund.shiny, janitor)\n\nPacotes para estimação dos modelos\n\nlme4 - modelos lineares, lineares generalizados, e modelos mistos não lineares\nmgcv - modelos aditivos generalizados (mistos); suavização semi-paramétrica\nrefund - modelos de regressão usando dados funcionais\nface - para estimativa rápida de covariância para dados funcionais esparsos\nfda - para análise de dados funcionais\n\nPacotes para manipulação e visualização dos dados\n\ntidyverse - auxilia na importação, organização, manipulação e visualização de dados\nrgl - gráficos 3d\nrefund.shiny - gráficos interativos para análise de dados funcionais\n\n\n\n\nPara este curso, exploraremos e analisaremos os seguintes conjuntos de dados (onde encontrar):\n\nDados de imagem de tensor de difusão (“DTI” pacote refund)\nDados meteorológicos canadenses (“CanadianWeather” pacote fda)\nDados de crescimento de Berkeley (“growth” pacote fda)\nDados CD4 (“cd4” pacote refund)\n\n\n\nOs dados de imagem de tensor de difusão (DTI - Diffusion Tensor Imaging) são amplamente utilizados em estudos sobre esclerose múltipla (EM) e em outras áreas de pesquisa neurocientífica. A DTI é uma técnica de ressonância magnética que permite avaliar a microestrutura e a conectividade das fibras nervosas no cérebro.\nNa esclerose múltipla, a DTI pode ser usada para detectar e quantificar as alterações nas propriedades de difusão da água nos tecidos cerebrais. Essas alterações podem ser indicativas de danos ou desmielinização das fibras nervosas, que são características da EM.\nOs principais parâmetros medidos na DTI incluem a difusão isotrópica (representada pela medida do coeficiente de difusão isotrópica - ADC) e a difusão anisotrópica (representada pelos valores de fração anisotrópica - FA). A FA é particularmente importante na DTI, pois fornece informações sobre a direcionalidade das fibras nervosas e sua integridade estrutural.\nAo analisar os dados de DTI na EM, os pesquisadores podem investigar as alterações na integridade das fibras nervosas, a presença de lesões ou placas desmielinizantes e o impacto dessas alterações na conectividade e no funcionamento cerebral dos pacientes com EM. Além disso, a DTI também pode ser usada para avaliar a progressão da doença ao longo do tempo e monitorar a eficácia de intervenções terapêuticas.\n\n# Os dados de DTI vem do pacote refund\ndata(\"DTI\")\nnames(DTI)\n\n[1] \"ID\"         \"visit\"      \"visit.time\" \"Nscans\"     \"case\"      \n[6] \"sex\"        \"pasat\"      \"cca\"        \"rcst\"      \n\n\nAqui vamos considerar apenas os casos completos, de pacientes com EM em suas prinmeiras visitas.\n\n# filtrando apenas os casos completos ----\nDTI2 &lt;- DTI %&gt;% \n  drop_na() %&gt;% \n  filter(visit == 1 & case == 1)\n\n# verificando as dimensoes dos dados de CCA\n# F1 para mais infomações sobre o formato dos dados\ndim(DTI2$cca)\n\n[1] 66 93\n\n# arrumando os dados para o gráfico ----\nDTI2$cca %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"x\")) %&gt;% \n  \n  ggplot(aes(tract, value, group = name, colour = name))+ #gráfico de todos os 66 casos completos\n  geom_line()+\n  theme(legend.position = \"none\")+\n  labs(x=\"tract\",\n       y =\"Anisotropia Fracionária (AF)\",\n       title = \"imagem de tensor de difusão:CCA\")\n\n\n\n\n\n\n\n# gráfico da média dos dados ----\n\nDTI2$cca %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"x\")) %&gt;% \n  group_by(tract) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 1)+\n  theme(legend.position = \"none\")+\n  labs(x=\"tract\",\n       y =\"Anisotropia Fracionária (AF)\",\n       title = \"imagem de tensor de difusão:CCA\")\n\n\n\n\n\n\n\n\nAlém das medidas de AF, o conjunto de dados DTI também inclui as pontuações do Teste de Adição Auditiva em Série (PASAT) dos pacientes, que medem suas funções cognitivas. Os gráficos a seguir ajudam a visualizar a relação entre as medições de AF e as pontuações do PASAT, codificadas por cores.\n\n#[code by R. Todd Ogden and Jeff Goldsmith]\n\ntract &lt;- 1:93\ncolfct &lt;- as.numeric(cut(DTI2$pasat, 40))\n\npar(mar=c(1,1,0,0), cex.axis=1, cex.lab=1)\nclrs &lt;- rev(colorRampPalette(c(\"blue\", \"green\", \"yellow\", \"red\"))(40))    \n\nproj = persp(x = tract, y = seq(min(DTI2$pasat), max(DTI2$pasat), l=length(DTI2$pasat)),  z=t(DTI2$cca),\n  xlab=\"tract\", ylab=\"PASAT\", zlab=\"AF\", col=NA, border=NA,\n  ticktype = \"detailed\", axes=TRUE, theta=30, phi=30)\n\no &lt;- rev(order(DTI2$pasat))\nfor(i in o){\n  lines(trans3d(x = tract, y=rep(DTI2$pasat[i], ncol(DTI2$cca)),  z=DTI2$cca[i,], pmat=proj), col=clrs[colfct[i]])\n}\n\n\n\n\n\n\n\n\nQuestões de interesse:\n\nComo as medições de AF ao longo do CCA variam entre os indivíduos com EM?\nQual é a associação entre medições de AF e funções cognitivas (pontuações PASAT)?\nO perfil típico de AF varia entre pacientes com EM e indivíduos saudáveis?\n\n\n\n\nO banco de dados Canadian Weather contém informações sobre as temperaturas e precipitações diárias registradas em 35 estações meteorológicas localizadas no Canadá. Os dados abrangem um período de 365 dias, fornecendo uma série temporal de temperaturas para cada estação (Ramsay and Silverman, 2002).\n\n# Arrumando os dados diários de temperatura para todas as estações\ndaily_data_temp &lt;- CanadianWeather$dailyAv %&gt;% \n  as_tibble() %&gt;% \n  dplyr::select(contains(\"Temperature\")) %&gt;% \n  rownames_to_column(var = \"day\") %&gt;% \n  mutate(\n    day = as.numeric(day)\n  )  \n \n\n# Criando o gráfico dos dados de teperatura diária\ndaily_data_temp %&gt;% \n  pivot_longer(cols = c(2:ncol(daily_data_temp))) %&gt;% \n  group_by(day) %&gt;% \n  ggplot(aes(day,value, color = name ))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Temperatura c°\", title = \"Temperaturas diárias\")\n\n\n\n\n\n\n\n# Criando o gráfico com o comportamento médio dos dados\ndaily_data_temp %&gt;% \n  pivot_longer(cols = c(2:ncol(daily_data_temp))) %&gt;% \n  group_by(day) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(day,value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(day, avg), color = \"red\", linewidth = 2, linetype = 1)+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Temperatura c°\", title = \"Temperaturas diárias - c/média\")\n\n\n\n\n\n\n\n\n\n# Arrumando os dados diários de preciptação para todas as estações\ndaily_data_prec &lt;- CanadianWeather$dailyAv %&gt;% \n  as_tibble() %&gt;% \n  dplyr::select(contains(\"Precipitation\")) %&gt;% \n  rownames_to_column(var = \"day\") %&gt;% \n  mutate(\n    day = as.numeric(day)\n  )  \n\n\n# Criando o gráfico dos dados de precipitação diária\ndaily_data_prec %&gt;% \n  pivot_longer(cols = c(2:ncol(daily_data_prec))) %&gt;% \n  group_by(day) %&gt;% \n  ggplot(aes(day,value, color = name ))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Precipitação mm\", title = \"Precipitação diária\")\n\n\n\n\n\n\n\n# Criando o gráfico com o comportamento médio dos dados\ndaily_data_prec %&gt;% \n  pivot_longer(cols = c(2:ncol(daily_data_prec))) %&gt;% \n  group_by(day) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(day,value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(day, avg), color = \"red\", linewidth = 2, linetype = 1)+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Precipitação mm\", title = \"Precipitação diária c/média\")\n\n\n\n\n\n\n\n\nQuestões de interesse:\n\nDescrever características que caracterizam os perfis diários de temperatura.\nQual é a associação entre a precipitação média anual e a temperatura diária no Canadá?\nE quanto à associação entre as precipitações diárias e os perfis de temperatura.\n\n\n\n\nAs células imunes (células CD4) são tipicamente atacadas pelo HIV (vírus da imunodeficiência humana); A contagem de células CD4 por mm de sangue é um substituto útil da progressão do HIV. O estudo Multicenter Aids Cohort (disponível no pacote de reembolso) coletou as contagens de células CD4 de 366 indivíduos afetados entre -18 e 42 meses desde a soroconversão (diagnóstico de HIV).\n\ndata(cd4)\nview(cd4)\n\ncd4_tidy &lt;- cd4 %&gt;%\n  as_tibble() %&gt;% \n rowid_to_column(var = \"affected\") %&gt;% \n  pivot_longer(cols = 2:62, names_to = \"months\", values_to = \"count-mm\" ) %&gt;% \n  mutate(\n    months = as.numeric(months)\n  )\n\ncd4_tidy %&gt;% \n  group_by(months) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(months, `count-mm`, group = affected, color = affected))+\n  geom_line()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nQuestões de interesse:\nComo as contagens de CD4 variam ao longo do tempo na população, bem como no nível individual?\nDescreva as principais direções nas quais as contagens de CD4 variam.\n\n\n\nInfelizmente eu não encontrei os dados utilizados no curso original, para substituir eu vou utilizar os dados sobre a maratona de Boston. Que como veremos não são o ideal para o proposito do curso.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescreva os gráficos - como a estratégia de corrida difere entre os corredores?\nPerguntas de interesse:\n\nComo a velocidade varia ao longo de uma maratona?\nQual é a relação entre o comportamento da velocidade durante a maratona e o tempo de chegada?\nQual é a melhor estratégia para terminar a maratona entre os cinco primeiros colocados?\n\n\n\n\n\n\n\nO conjunto de dados Berkeley Growth Data (também conhecido como Berkeley Growth Study) é um conjunto de dados histórico amplamente utilizado em pesquisas sobre crescimento e desenvolvimento infantil, contendo em alturas de 39 meninos e 54 meninas de 1 a 18 anos e as idades em que foram coletados. Esse conjunto de dados foi coletado como parte do Berkeley Growth Study, que ocorreu na Universidade da Califórnia, Berkeley, durante o século XX.\nO Berkeley Growth Data contém informações longitudinais de medidas antropométricas (como altura e peso) de um grande número de crianças desde o nascimento até a idade adulta. As medidas foram coletadas em intervalos regulares ao longo de vários anos, permitindo uma análise detalhada do crescimento e desenvolvimento ao longo do tempo.\n\ndata(\"growth\")\nview(growth)\n\n# Separando os dados referentes aos meninos\ngrowth_boy &lt;- growth$hgtm %&gt;%\n  as_tibble() %&gt;% \n   mutate(\n     age = growth$age\n     )\n# Separando os dados referentes as meninas\ngrowth_girl &lt;- growth$hgtf %&gt;%\n  as_tibble() %&gt;% \n  mutate(\n    age = growth$age\n  )\n\n# unificando os dois conuntos e criando uma variável gênero\ngrowth_df &lt;- left_join(\n  growth_boy,\n  growth_girl,\n  by = \"age\") %&gt;% \n  pivot_longer(\n    cols = contains(c(\"boy\",\"girl\")),\n    names_to = \"kids\",\n    values_to = \"height\") %&gt;% \n   mutate(\n    gender = if_else(stringr::str_starts(kids, \"boy\"), \"M\", \"F\")\n  )\n\n\n# curva de crescimento \ngrowth_df %&gt;%  \n   group_by(age) %&gt;% \n  ggplot(aes(age, height, group = kids, color = gender))+\n  geom_line()\n\n\n\n\n\n\n\n# taxa de crescimento \ngrowth_df %&gt;%  \n  group_by(kids, gender) %&gt;% \n  mutate(\n    g_rate = c(0,diff(height))\n  ) %&gt;%\n  ggplot(aes(age, g_rate, group = kids,color = gender))+\n  geom_line()\n\n\n\n\n\n\n\n\nQuestões de interesse:\nComo a altura varia em meninos e meninas?\nO género tem impacto no processo de crescimento de uma criança?\n\n\n\n\nIntrodução dos quatro conjuntos de dados com características funcionais (ou longitudinais).\nDiscutiu possíveis questões científicas para cada conjunto de dados."
  },
  {
    "objectID": "dia01.html#instalando-os-pacotes-que-serão-utilizados",
    "href": "dia01.html#instalando-os-pacotes-que-serão-utilizados",
    "title": "Dia-01",
    "section": "",
    "text": "if(!require(pacman)) install.packages(c(\"pacman\", # instalando os pacotes necessários\n                                        \"tidyverse\",\n                                        \"lme4\",\n                                        \"mgcv\",\n                                        \"refund\",\n                                        \"face\",\n                                        \"fda\", \n                                        \"rgl\",\n                                        \"fields\",\n                                        \"refund.shiny\",\n                                        \"janitor\"))\n\nCarregando os pacotes simultaneamente utilizando o pacote pacman\n\npacman::p_load(tidyverse, lme4, mgcv, refund, face, fda, rgl, fields, refund.shiny, janitor)\n\nPacotes para estimação dos modelos\n\nlme4 - modelos lineares, lineares generalizados, e modelos mistos não lineares\nmgcv - modelos aditivos generalizados (mistos); suavização semi-paramétrica\nrefund - modelos de regressão usando dados funcionais\nface - para estimativa rápida de covariância para dados funcionais esparsos\nfda - para análise de dados funcionais\n\nPacotes para manipulação e visualização dos dados\n\ntidyverse - auxilia na importação, organização, manipulação e visualização de dados\nrgl - gráficos 3d\nrefund.shiny - gráficos interativos para análise de dados funcionais"
  },
  {
    "objectID": "dia01.html#introdução-aos-dados-funcionais-e-longitudinais",
    "href": "dia01.html#introdução-aos-dados-funcionais-e-longitudinais",
    "title": "Dia-01",
    "section": "2 Introdução aos Dados Funcionais e longitudinais",
    "text": "2 Introdução aos Dados Funcionais e longitudinais\nDados Funcionais: Dados funcionais são uma forma de representar informações observadas ao longo de uma dimensão contínua, geralmente o tempo. Em vez de ter observações pontuais em momentos específicos, os dados funcionais registram a evolução de uma variável ao longo de uma curva contínua, muitas vezes representada como uma função. Essa função pode ser descrita por uma série de pontos discretos ou através de uma representação contínua, como uma curva suave. Dados funcionais são frequentemente utilizados em áreas como análise de séries temporais, análise de imagens médicas, processamento de sinais, entre outras.\nDados Longitudinais: Dados longitudinais referem-se a dados coletados repetidamente de um mesmo indivíduo, objeto ou unidade de estudo ao longo do tempo. Nesse tipo de dado, o foco está na observação das mudanças que ocorrem em uma variável ou conjunto de variáveis ao longo de diferentes momentos. Os dados longitudinais permitem analisar tendências, padrões de crescimento, estabilidade ou mudanças em uma população ao longo do tempo. São comumente utilizados em estudos longitudinais, pesquisas de acompanhamento de indivíduos, estudos de coorte e estudos de desenvolvimento, entre outros.\nEm resumo, dados funcionais representam a evolução de uma variável em uma dimensão contínua, enquanto dados longitudinais referem-se a observações repetidas de uma mesma unidade de estudo ao longo do tempo. Ambos os tipos de dados são valiosos para análises estatísticas e científicas em diferentes áreas de estudo."
  },
  {
    "objectID": "dia01.html#dados-utilizados-no-curso",
    "href": "dia01.html#dados-utilizados-no-curso",
    "title": "Dia-01",
    "section": "",
    "text": "Para este curso, exploraremos e analisaremos os seguintes conjuntos de dados (onde encontrar):\n\nDados de imagem de tensor de difusão (“DTI” pacote refund)\nDados meteorológicos canadenses (“CanadianWeather” pacote fda)\nDados de crescimento de Berkeley (“growth” pacote fda)\nDados CD4 (“cd4” pacote refund)\n\n\n\nOs dados de imagem de tensor de difusão (DTI - Diffusion Tensor Imaging) são amplamente utilizados em estudos sobre esclerose múltipla (EM) e em outras áreas de pesquisa neurocientífica. A DTI é uma técnica de ressonância magnética que permite avaliar a microestrutura e a conectividade das fibras nervosas no cérebro.\nNa esclerose múltipla, a DTI pode ser usada para detectar e quantificar as alterações nas propriedades de difusão da água nos tecidos cerebrais. Essas alterações podem ser indicativas de danos ou desmielinização das fibras nervosas, que são características da EM.\nOs principais parâmetros medidos na DTI incluem a difusão isotrópica (representada pela medida do coeficiente de difusão isotrópica - ADC) e a difusão anisotrópica (representada pelos valores de fração anisotrópica - FA). A FA é particularmente importante na DTI, pois fornece informações sobre a direcionalidade das fibras nervosas e sua integridade estrutural.\nAo analisar os dados de DTI na EM, os pesquisadores podem investigar as alterações na integridade das fibras nervosas, a presença de lesões ou placas desmielinizantes e o impacto dessas alterações na conectividade e no funcionamento cerebral dos pacientes com EM. Além disso, a DTI também pode ser usada para avaliar a progressão da doença ao longo do tempo e monitorar a eficácia de intervenções terapêuticas.\n\n# Os dados de DTI vem do pacote refund\ndata(\"DTI\")\nnames(DTI)\n\n[1] \"ID\"         \"visit\"      \"visit.time\" \"Nscans\"     \"case\"      \n[6] \"sex\"        \"pasat\"      \"cca\"        \"rcst\"      \n\n\nAqui vamos considerar apenas os casos completos, de pacientes com EM em suas prinmeiras visitas.\n\n# filtrando apenas os casos completos ----\nDTI2 &lt;- DTI %&gt;% \n  drop_na() %&gt;% \n  filter(visit == 1 & case == 1)\n\n# verificando as dimensoes dos dados de CCA\n# F1 para mais infomações sobre o formato dos dados\ndim(DTI2$cca)\n\n[1] 66 93\n\n# arrumando os dados para o gráfico ----\nDTI2$cca %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"x\")) %&gt;% \n  \n  ggplot(aes(tract, value, group = name, colour = name))+ #gráfico de todos os 66 casos completos\n  geom_line()+\n  theme(legend.position = \"none\")+\n  labs(x=\"tract\",\n       y =\"Anisotropia Fracionária (AF)\",\n       title = \"imagem de tensor de difusão:CCA\")\n\n\n\n\n\n\n\n# gráfico da média dos dados ----\n\nDTI2$cca %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n janitor::clean_names() %&gt;% \n  mutate(\n    tract = 1:93\n  ) %&gt;% \n  pivot_longer(cols = starts_with(\"x\")) %&gt;% \n  group_by(tract) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(tract, value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(tract,avg), color = \"red\",linewidth = 2, linetype = 1)+\n  theme(legend.position = \"none\")+\n  labs(x=\"tract\",\n       y =\"Anisotropia Fracionária (AF)\",\n       title = \"imagem de tensor de difusão:CCA\")\n\n\n\n\n\n\n\n\nAlém das medidas de AF, o conjunto de dados DTI também inclui as pontuações do Teste de Adição Auditiva em Série (PASAT) dos pacientes, que medem suas funções cognitivas. Os gráficos a seguir ajudam a visualizar a relação entre as medições de AF e as pontuações do PASAT, codificadas por cores.\n\n#[code by R. Todd Ogden and Jeff Goldsmith]\n\ntract &lt;- 1:93\ncolfct &lt;- as.numeric(cut(DTI2$pasat, 40))\n\npar(mar=c(1,1,0,0), cex.axis=1, cex.lab=1)\nclrs &lt;- rev(colorRampPalette(c(\"blue\", \"green\", \"yellow\", \"red\"))(40))    \n\nproj = persp(x = tract, y = seq(min(DTI2$pasat), max(DTI2$pasat), l=length(DTI2$pasat)),  z=t(DTI2$cca),\n  xlab=\"tract\", ylab=\"PASAT\", zlab=\"AF\", col=NA, border=NA,\n  ticktype = \"detailed\", axes=TRUE, theta=30, phi=30)\n\no &lt;- rev(order(DTI2$pasat))\nfor(i in o){\n  lines(trans3d(x = tract, y=rep(DTI2$pasat[i], ncol(DTI2$cca)),  z=DTI2$cca[i,], pmat=proj), col=clrs[colfct[i]])\n}\n\n\n\n\n\n\n\n\nQuestões de interesse:\n\nComo as medições de AF ao longo do CCA variam entre os indivíduos com EM?\nQual é a associação entre medições de AF e funções cognitivas (pontuações PASAT)?\nO perfil típico de AF varia entre pacientes com EM e indivíduos saudáveis?\n\n\n\n\nO banco de dados Canadian Weather contém informações sobre as temperaturas e precipitações diárias registradas em 35 estações meteorológicas localizadas no Canadá. Os dados abrangem um período de 365 dias, fornecendo uma série temporal de temperaturas para cada estação (Ramsay and Silverman, 2002).\n\n# Arrumando os dados diários de temperatura para todas as estações\ndaily_data_temp &lt;- CanadianWeather$dailyAv %&gt;% \n  as_tibble() %&gt;% \n  dplyr::select(contains(\"Temperature\")) %&gt;% \n  rownames_to_column(var = \"day\") %&gt;% \n  mutate(\n    day = as.numeric(day)\n  )  \n \n\n# Criando o gráfico dos dados de teperatura diária\ndaily_data_temp %&gt;% \n  pivot_longer(cols = c(2:ncol(daily_data_temp))) %&gt;% \n  group_by(day) %&gt;% \n  ggplot(aes(day,value, color = name ))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Temperatura c°\", title = \"Temperaturas diárias\")\n\n\n\n\n\n\n\n# Criando o gráfico com o comportamento médio dos dados\ndaily_data_temp %&gt;% \n  pivot_longer(cols = c(2:ncol(daily_data_temp))) %&gt;% \n  group_by(day) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(day,value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(day, avg), color = \"red\", linewidth = 2, linetype = 1)+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Temperatura c°\", title = \"Temperaturas diárias - c/média\")\n\n\n\n\n\n\n\n\n\n# Arrumando os dados diários de preciptação para todas as estações\ndaily_data_prec &lt;- CanadianWeather$dailyAv %&gt;% \n  as_tibble() %&gt;% \n  dplyr::select(contains(\"Precipitation\")) %&gt;% \n  rownames_to_column(var = \"day\") %&gt;% \n  mutate(\n    day = as.numeric(day)\n  )  \n\n\n# Criando o gráfico dos dados de precipitação diária\ndaily_data_prec %&gt;% \n  pivot_longer(cols = c(2:ncol(daily_data_prec))) %&gt;% \n  group_by(day) %&gt;% \n  ggplot(aes(day,value, color = name ))+\n  geom_line()+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Precipitação mm\", title = \"Precipitação diária\")\n\n\n\n\n\n\n\n# Criando o gráfico com o comportamento médio dos dados\ndaily_data_prec %&gt;% \n  pivot_longer(cols = c(2:ncol(daily_data_prec))) %&gt;% \n  group_by(day) %&gt;% \n  mutate(\n    avg = mean(value)\n  ) %&gt;% \n  ggplot(aes(day,value, group = name))+\n  geom_line(color = \"gray\")+\n  geom_line(aes(day, avg), color = \"red\", linewidth = 2, linetype = 1)+\n  theme(legend.position = \"none\")+\n  labs(x = \"day\", y = \"Precipitação mm\", title = \"Precipitação diária c/média\")\n\n\n\n\n\n\n\n\nQuestões de interesse:\n\nDescrever características que caracterizam os perfis diários de temperatura.\nQual é a associação entre a precipitação média anual e a temperatura diária no Canadá?\nE quanto à associação entre as precipitações diárias e os perfis de temperatura.\n\n\n\n\nAs células imunes (células CD4) são tipicamente atacadas pelo HIV (vírus da imunodeficiência humana); A contagem de células CD4 por mm de sangue é um substituto útil da progressão do HIV. O estudo Multicenter Aids Cohort (disponível no pacote de reembolso) coletou as contagens de células CD4 de 366 indivíduos afetados entre -18 e 42 meses desde a soroconversão (diagnóstico de HIV).\n\ndata(cd4)\nview(cd4)\n\ncd4_tidy &lt;- cd4 %&gt;%\n  as_tibble() %&gt;% \n rowid_to_column(var = \"affected\") %&gt;% \n  pivot_longer(cols = 2:62, names_to = \"months\", values_to = \"count-mm\" ) %&gt;% \n  mutate(\n    months = as.numeric(months)\n  )\n\ncd4_tidy %&gt;% \n  group_by(months) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(months, `count-mm`, group = affected, color = affected))+\n  geom_line()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nQuestões de interesse:\nComo as contagens de CD4 variam ao longo do tempo na população, bem como no nível individual?\nDescreva as principais direções nas quais as contagens de CD4 variam.\n\n\n\nInfelizmente eu não encontrei os dados utilizados no curso original, para substituir eu vou utilizar os dados sobre a maratona de Boston. Que como veremos não são o ideal para o proposito do curso.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescreva os gráficos - como a estratégia de corrida difere entre os corredores?\nPerguntas de interesse:\n\nComo a velocidade varia ao longo de uma maratona?\nQual é a relação entre o comportamento da velocidade durante a maratona e o tempo de chegada?\nQual é a melhor estratégia para terminar a maratona entre os cinco primeiros colocados?\n\n\n\n\n\n\n\nO conjunto de dados Berkeley Growth Data (também conhecido como Berkeley Growth Study) é um conjunto de dados histórico amplamente utilizado em pesquisas sobre crescimento e desenvolvimento infantil, contendo em alturas de 39 meninos e 54 meninas de 1 a 18 anos e as idades em que foram coletados. Esse conjunto de dados foi coletado como parte do Berkeley Growth Study, que ocorreu na Universidade da Califórnia, Berkeley, durante o século XX.\nO Berkeley Growth Data contém informações longitudinais de medidas antropométricas (como altura e peso) de um grande número de crianças desde o nascimento até a idade adulta. As medidas foram coletadas em intervalos regulares ao longo de vários anos, permitindo uma análise detalhada do crescimento e desenvolvimento ao longo do tempo.\n\ndata(\"growth\")\nview(growth)\n\n# Separando os dados referentes aos meninos\ngrowth_boy &lt;- growth$hgtm %&gt;%\n  as_tibble() %&gt;% \n   mutate(\n     age = growth$age\n     )\n# Separando os dados referentes as meninas\ngrowth_girl &lt;- growth$hgtf %&gt;%\n  as_tibble() %&gt;% \n  mutate(\n    age = growth$age\n  )\n\n# unificando os dois conuntos e criando uma variável gênero\ngrowth_df &lt;- left_join(\n  growth_boy,\n  growth_girl,\n  by = \"age\") %&gt;% \n  pivot_longer(\n    cols = contains(c(\"boy\",\"girl\")),\n    names_to = \"kids\",\n    values_to = \"height\") %&gt;% \n   mutate(\n    gender = if_else(stringr::str_starts(kids, \"boy\"), \"M\", \"F\")\n  )\n\n\n# curva de crescimento \ngrowth_df %&gt;%  \n   group_by(age) %&gt;% \n  ggplot(aes(age, height, group = kids, color = gender))+\n  geom_line()\n\n\n\n\n\n\n\n# taxa de crescimento \ngrowth_df %&gt;%  \n  group_by(kids, gender) %&gt;% \n  mutate(\n    g_rate = c(0,diff(height))\n  ) %&gt;%\n  ggplot(aes(age, g_rate, group = kids,color = gender))+\n  geom_line()\n\n\n\n\n\n\n\n\nQuestões de interesse:\nComo a altura varia em meninos e meninas?\nO género tem impacto no processo de crescimento de uma criança?"
  },
  {
    "objectID": "dia01.html#resumo",
    "href": "dia01.html#resumo",
    "title": "Dia-01",
    "section": "",
    "text": "Introdução dos quatro conjuntos de dados com características funcionais (ou longitudinais).\nDiscutiu possíveis questões científicas para cada conjunto de dados."
  },
  {
    "objectID": "index.html#módulo-1-introdução-aos-dados-funcionais-aula-1",
    "href": "index.html#módulo-1-introdução-aos-dados-funcionais-aula-1",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "Módulo 1: Introdução aos Dados Funcionais [Aula 1]",
    "text": "Módulo 1: Introdução aos Dados Funcionais [Aula 1]\n\nMódulo 1-1: R e RStudio\nMódulo 1-2: Introdução a conjuntos de dados\nMódulo 1-3: Introdução a conjuntos de dados (continuação)\nMódulo 1-4: Resumo e Discussão"
  },
  {
    "objectID": "index.html#módulo-2-modelagem-de-dados-funcionais-com-expansões-de-base-pré-definidas-aula-2",
    "href": "index.html#módulo-2-modelagem-de-dados-funcionais-com-expansões-de-base-pré-definidas-aula-2",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "Módulo 2: Modelagem de Dados Funcionais com Expansões de Base Pré-definidas [Aula 2]",
    "text": "Módulo 2: Modelagem de Dados Funcionais com Expansões de Base Pré-definidas [Aula 2]\n\nMódulo 2-1: Funções de base\nMódulo 2-2: Regressão linear em funções de base\nMódulo 2-3: Seleção do número de funções de base (validação cruzada)\nMódulo 2-4: Suavização com penalidade de rugosidade\nMódulo 2-5: Resumo e Discussão"
  },
  {
    "objectID": "index.html#módulo-3-modelagem-de-dados-funcionais-usando-componentes-principais-funcionais-aula-3",
    "href": "index.html#módulo-3-modelagem-de-dados-funcionais-usando-componentes-principais-funcionais-aula-3",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "Módulo 3: Modelagem de Dados Funcionais usando Componentes Principais Funcionais [Aula 3]",
    "text": "Módulo 3: Modelagem de Dados Funcionais usando Componentes Principais Funcionais [Aula 3]\n\nMódulo 3-1: Análise de Componentes Principais Funcionais para dados altamente - frequentes\nMódulo 3-2: Análise de Componentes Principais Funcionais para dados funcionais esparsos\nMódulo 3-3: Gráficos interativos para Análise de Componentes Principais Funcionais\nMódulo 3-4: Resumo e Discussão"
  },
  {
    "objectID": "index.html#módulo-4-modelos-lineares-funcionais-aula-4",
    "href": "index.html#módulo-4-modelos-lineares-funcionais-aula-4",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "Módulo 4: Modelos Lineares Funcionais [Aula 4]",
    "text": "Módulo 4: Modelos Lineares Funcionais [Aula 4]\n\nMódulo 4-1: Regressão escalar em função\nMódulo 4-2: Regressão de função em escalar (visualização)\nMódulo 4-3: Regressão de função em função\nMódulo 4-4: Resumo, Discussão e Atividades em Grupo"
  },
  {
    "objectID": "index.html#módulo-5-dados-funcionais-correlacionados-aula-5",
    "href": "index.html#módulo-5-dados-funcionais-correlacionados-aula-5",
    "title": "Introdução à Análise de Dados Funcionais",
    "section": "Módulo 5: Dados Funcionais Correlacionados [Aula 5]",
    "text": "Módulo 5: Dados Funcionais Correlacionados [Aula 5]\n\nMódulo 5-1: Análise de Dados Funcionais Longitudinais e Gráficos interativos\nMódulo 5-2: Discussão"
  },
  {
    "objectID": "dia01.html#secao-1",
    "href": "dia01.html#secao-1",
    "title": "Dia-01",
    "section": "",
    "text": "Dados Funcionais: Dados funcionais são uma forma de representar informações observadas ao longo de uma dimensão contínua, geralmente o tempo. Em vez de ter observações pontuais em momentos específicos, os dados funcionais registram a evolução de uma variável ao longo de uma curva contínua, muitas vezes representada como uma função. Essa função pode ser descrita por uma série de pontos discretos ou através de uma representação contínua, como uma curva suave. Dados funcionais são frequentemente utilizados em áreas como análise de séries temporais, análise de imagens médicas, processamento de sinais, entre outras.\nDados Longitudinais: Dados longitudinais referem-se a dados coletados repetidamente de um mesmo indivíduo, objeto ou unidade de estudo ao longo do tempo. Nesse tipo de dado, o foco está na observação das mudanças que ocorrem em uma variável ou conjunto de variáveis ao longo de diferentes momentos. Os dados longitudinais permitem analisar tendências, padrões de crescimento, estabilidade ou mudanças em uma população ao longo do tempo. São comumente utilizados em estudos longitudinais, pesquisas de acompanhamento de indivíduos, estudos de coorte e estudos de desenvolvimento, entre outros.\nEm resumo, dados funcionais representam a evolução de uma variável em uma dimensão contínua, enquanto dados longitudinais referem-se a observações repetidas de uma mesma unidade de estudo ao longo do tempo. Ambos os tipos de dados são valiosos para análises estatísticas e científicas em diferentes áreas de estudo."
  },
  {
    "objectID": "respostas.html",
    "href": "respostas.html",
    "title": "Resposta dos exercícios propostos",
    "section": "",
    "text": "Como variam as medidas de AF ao longo da CCA em indivíduos com esclerose múltipla?\nR: Apesar do grande “range” de valores de AF, a grande maioria das curvas pode ser bem representada pelo formato obtido com a média de todas as curvas (figura abaixo). Contudo principalmente na parte central da imagem podemos notar que o comportamento é muito mais errático. Essas variações nos valores de AF para o diferentes indivíduos podem estár associadas a diferentes localizações, extensão e gravidade da desmielinização do corpo caloso (CCA).\n\n\n\n\n\n\n\n\n\n\n\n\nQual é a associação entre as medidas de anisotropia fracionada (fractional anisotropy, FA) e as funções cognitivas (pontuações do teste PASAT)?\nÉssa é uma pergunta que pode ser respondida de modo mais efetivo ao utilizarmos as técnicas que serão apresentadas mais adiante no curso. Contudo, vamos fazer uma primeira abordagem visual.\n\n\n\n\n\n\n\n\n\nAtravés da figura acima podemos notar que notas mais baixas de PASAT estão relacionadas a valores mais baixos de AF, principalemnte na parce central do corpo caloso.\n\n\n\nO perfil típico de anisotropia fracionada (fractional anisotropy, FA) varia entre pacientes com esclerose múltipla e indivíduos saudáveis?\nPara responder essa questão vamos dar uma olhada nos perfis dos pacientes com EM e nos pacientes saudáveis\n\n\n\n\n\n\n\n\n\nAtravés da comparação dos gráficos é fácil notar que apesar de apresentarem m comportamento médio pareceido (formato da curva), os valores de AF dos pacientes com EM são mais baixos (gráfico da esquerda) do que os pacientes saudáveis (gráfico da direita).\nConforme indicado na questão anterior, existe uma aparente correlação entre as notas de PASAT e os valores de AF. O que nos daria um bom modo de testar, mesmo que de modo indireto, se a diferença entre os valores de de AF para pacientes com e sem EM é significativa. Entretanto, infelizmente o conjunto de dados não apresenta as notas de PASAT para pacientes saudáveis. Assim sendo vamos utilizar os valores médios de AF para cada paciente.\n\nDTI4 &lt;- DTI %&gt;% \n  filter(visit == 1)\n\n#vamos dar uma olhada se as classes estão balanceadas\n\nfreq_relativa &lt;- DTI4 %&gt;%\n  count(sex, case) %&gt;%\n  mutate(freq_rel = n / sum(n))\nprint(freq_relativa)\n\n     sex case  n   freq_rel\n1   male    0 30 0.21126761\n2   male    1 66 0.46478873\n3 female    0 12 0.08450704\n4 female    1 34 0.23943662\n\n\n\nfreq_relativa %&gt;% \n  ggplot(aes(x = sex, y = freq_rel, fill = as.factor(case))) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Sex\", y = \"Proportion\", fill = \"Case\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEssas classes parecem um pouco desbalanceadas, vamos tentar melhorar isso.\n\n# Contagem das classes\ncontagem_classes &lt;- DTI4 %&gt;%\n  count(case,sex)\n\n# Identificar a classe majoritária e a classe minoritária\nclasse_majoritaria &lt;- contagem_classes %&gt;%\n  filter(n == max(n)) %&gt;%\n  pull(case)\n\nclasse_minoritaria &lt;- contagem_classes %&gt;%\n  filter(n == min(n)) %&gt;%\n  pull(case)\n\n# Subamostragem da classe majoritária para igualar o número de observações\ndados_balanceados &lt;- DTI4 %&gt;%\n  group_by(sex) %&gt;%\n  sample_n(size = max(contagem_classes$n), replace = TRUE) %&gt;%\n  ungroup()\n\n# Verificar a contagem das classes nos dados balanceados\ncontagem_classes_balanceadas &lt;- dados_balanceados %&gt;%\n  count(sex, case)\n\nprint(contagem_classes_balanceadas)\n\n# A tibble: 4 × 3\n  sex     case     n\n  &lt;fct&gt;  &lt;dbl&gt; &lt;int&gt;\n1 male       0    18\n2 male       1    48\n3 female     0    23\n4 female     1    43\n\n\n\nfreq_relativa2 &lt;- contagem_classes_balanceadas %&gt;%\n  count(sex, case) %&gt;%\n  mutate(freq_rel = n / sum(n))\n\n\nfreq_relativa2 %&gt;% \nggplot(aes(x = sex, y = freq_rel, fill = as.factor(case))) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Sex\", y = \"Proportion\", fill = \"Case\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNeste caso os grupos que queremos testar são independentes, ou você tem ou você não EM… Assim, o teste que me vem a mente para testar esa diferença entre os grupos é o teste t.\n\npacman::p_load(RVAideMemoire, car)\n\n\n#Obitendo os valores médios para cada ID\n\ndf_teste&lt;- dados_balanceados %&gt;% \n  mutate(\n    mean_cca = rowMeans(dados_balanceados$cca, na.rm = TRUE),\n    case = as_factor(case)\n  ) %&gt;% \ndplyr::select(ID,case,sex,mean_cca) \n\n#glimpse(df_teste)\n\n# Verificando a normalidade dos dados\nbyf.shapiro(mean_cca ~ case, df_teste)\n\n\n    Shapiro-Wilk normality tests\n\ndata:  mean_cca by case \n\n       W p-value\n0 0.9771  0.5683\n1 0.9896  0.6958\n\nbyf.shapiro(mean_cca ~ sex, df_teste)\n\n\n    Shapiro-Wilk normality tests\n\ndata:  mean_cca by sex \n\n            W p-value\nmale   0.9896  0.8585\nfemale 0.9760  0.2277\n\n\nNão encontramos evidencias para rejeitar as hipoteses nulas \\(H_0\\): distribuição dos dados = normal \\(p &gt; 0.05\\).\nVamos verificar a homogeneidade de variâncias\n\nleveneTest(mean_cca ~ case, df_teste)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value   Pr(&gt;F)   \ngroup   1  8.3346 0.004557 **\n      130                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nleveneTest(mean_cca ~ sex, df_teste)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  0.0359   0.85\n      130               \n\n\n\\(H_0\\): as variâncias dos grupos são homogêneas. Vamos seguir com o teste t\n\nt.test(mean_cca ~ case, df_teste, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  mean_cca by case\nt = 6.7243, df = 130, p-value = 5.051e-10\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.03829808 0.07022817\nsample estimates:\nmean in group 0 mean in group 1 \n      0.5504240       0.4961609 \n\nt.test(mean_cca ~ sex, df_teste, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  mean_cca by sex\nt = 0.46751, df = 130, p-value = 0.6409\nalternative hypothesis: true difference in means between group male and group female is not equal to 0\n95 percent confidence interval:\n -0.01308899  0.02118927\nsample estimates:\n  mean in group male mean in group female \n           0.5150404            0.5109903 \n\n\nAssim, temos indícios para acreditar que existe diferença entre as pessos que tem e que não tem EM (rejeitamos a hipótese nula) e não existe diferença entre os gêneros (não rejeitamos a hipótese nula).\n\np1&lt;- df_teste %&gt;% \n  ggplot(aes(x = factor(case), y = mean_cca)) +\n  geom_boxplot() +\n  xlab(\"Case\") +\n  ylab(\"Mean CCA\")\n\n\n\np2&lt;- df_teste %&gt;% \n  ggplot(aes(x = factor(sex), y = mean_cca)) +\n  geom_boxplot() +\n  xlab(\"sex\") +\n  ylab(\"Mean CCA\")\n\n\nplot_combinado2 &lt;- p1 + p2 + plot_layout(ncol = 1)\n\nprint(plot_combinado2)\n\n\n\n\n\n\n\n\nOs perfís variam sim entre individuos com e sem EM, e não variam entre pacientes de gênero distintos.\n\n\n\nDescreva as principais características dos perfis diários de temperatura.\n\n\n\n\n\n\n\n\n\nTodas as curvas tem um comportamento muito parecido, com um aparente comportamento ciclico (só temos um ano no gráfico) inverno-verão. Contudo apresentam um range de temperaturas dependendo da localidade da estação. Localidades mais quentes apresentam em média, durante o inverno valores um pouco abaixo de zero e um pouco maiores que 20° no verão. Equanto localidades mais frias apresentam valores inferiores a -30° no inversno e ~5°/10° graus no verão.\n\n\n\nQual é a associação entre a precipitação média anual e a temperatura diária em todo o Canadá?\nUm bom modo de responder essa questão sería através da utilização da técnica de regressão de escalar em função (SoF/prf) que ainda não vimos. Em breve voltaremos a essa questão.\n\n\n\nE quanto à associação entre as precipitações diárias e os perfis de temperatura?\nAssim como na questão anterior essa questão demanda uma técnica que ainda não foi apresentada, nesse caso Regressão de componentes principais funcionais (FPC) e Função em função - FoF (pffr).\n\n\n\nComo as contagens de CD4 variam ao longo do tempo na população, bem como no nível individual?\n\n\n\n\n\n\n\n\n\nEssa pergunta busca compreender como as contagens de células CD4 mudam ao longo do tempo em indivíduos com HIV em tratamento. As contagens de CD4 são um indicador-chave da saúde imunológica das pessoas infectadas pelo HIV, e monitorar essas contagens ao longo do tempo pode fornecer informações valiosas sobre a progressão da doença e a eficácia do tratamento.\nEm geral os dados apresentam um comportamento muito menos errático após o mes 0. Contudo, em nível individual o comportamento possui um range de valores bem grande.\n\n\n\nDescreva as principais direções nas quais as contagens de CD4 variam.\nNão entendi bem essa pergunta…\nOs dados apresentam comportamento muito diferentes entre os indivíduos.\n\n\n\nDescreva os gráficos - como a estratégia de corrida difere entre os corredores?\ncriar os gráficos e colocar um link para eles\n\n\n\nComo a velocidade varia ao longo de um percurso de uma maratona?\n\n\n\nQual é a relação entre o comportamento da velocidade durante a maratona e o tempo de chegada?\n\n\n\nQual é a melhor estratégia para terminar a maratona entre os cinco primeiros colocados?\n\n\n\nDescreva a variação do poluente em todo os Estados Unidos. (ainda não tem um dados para substituir)\n\n\n\nDescreva os principais modos de variação. (ainda não tem um dados para substituir)\n\n\n\nComo a altura varia em meninos e meninas?\n\n\n\nO gênero tem impacto no processo de crescimento de uma criança?\n\n\n\nPlote a taxa de crescimento de meninos e meninas e descreva-a.\n\n\n\nExplore os dados das curvas de crescimento de Berkeley\n\nPlote/descreva a função média.\nPlote/descreva a função de covariância.\nAs funções média e de covariância diferem entre mulheres e homens?\n\n\n\n\nReproduza os gráficos para os conjuntos de dados discutidos na terceira página. (Maratona,poluição,crescimento)\n\n\n\n\n\n\nTente executar o código acima várias vezes. Experimente também executar o código com diferentes números de funções de base, por exemplo, nb &lt;- 5, nb &lt;- 15, nb &lt;- 30. (colocar o link para a pagina onde tem o código mncionado)\n\n\n\nAgora tente executar o código com nbasis = 13, nbasis = 27 e nbasis = 365. Qual é o efeito de usar diferentes números de bases? (colocar o link onde tem o código mencionado)\n\n\n\nTente este exercício com as medidas feitas em Quebec (l = 10). Você ainda escolhe K = 13 como o número ótimo de funções de base? E em relação a outras localidades? (colocar o link para a pagina onde está o código refetrente a pergunta)\n\n\n\nExperimente diferentes valores de λ (λ = 0.0001, λ = 100 e λ = 108) e discuta os resultados! (colocar o link do código)\n\n\n\nExistem várias maneiras de selecionar o λ ótimo; por exemplo, validação cruzada\n(colocar o link para a pagina onde tem a compração entre cross-validation e generalized cross-validation)\n\n\n\nExplorar as funcções do do pacote mgcv\nFunção ksmooth (alisador de regressão de kernel) Exemplo (atividade para fazer em casa): experimente um kernel gaussiano (“normal”) e diferentes larguras de banda (0.1, 10, 30).\n\n#fit = gam(y ~ s(t.day, k = 30, bs = \"cr\"), method = 'REML')\n# fit = ksmooth(t.day, y, kernel = c(\"box\"), bandwidth = 10)\n\n\n\n\nPágina 2: Experimente diferentes números de funções de base e ajuste OLS (atividade em grupo).\n\n\n\nPágina 3: Exercício de validação cruzada com medidas de precipitação transformadas em logaritmo feitas em Quebec.\n\n\n\nPágina 4: Experimente as funções gam e ksmooth; especialmente a função gam, pois a usaremos nos próximos capítulos!\n\n\n\n\n\n\nO que encontramos a partir dos resultados desta fPCA (análise de componentes principais funcionais)?\n\n\n\nQuanta variabilidade dos dados foi explicada pelo primeiro fPC (componente principal funcional)?\n\n\n\nQue característica das curvas o primeiro fPC (componente principal funcional) explica?\n\n\n\nTente fazer os gráficos para o segundo e terceiro fPCs (componentes principais funcionais) e seus efeitos em relação à média geral.\n\n\n\nDiscuta e interprete os resultados da fPCA (análise de componentes principais funcionais).\n\n\n\nQuantos CPs (componentes principais) são necessários para explicar 95% das variabilidades nos dados?\n\n\n\nWhat features of the underlying curves were captured by these PCs?\n\n\n\nExplore os gráficos interativos.\n\n\n\nAnalisar os dados da Maratona (dois aspectos: tempo decorrido transformado em logaritmo e tempo por milha)\n\nPlote / descreva os dados.\nPlote / discuta as estimativas das funções média e de covariância.\nUse gráficos interativos para explorar os resultados da fPCA e resuma suas descobertas.\n\n\n\n\nAnalisar os dados do Poluente (foco no nível de sulfato transformado em logaritmo)\nPlote / descreva os dados.\nPlote / discuta as estimativas das funções média e de covariância.\nUse gráficos interativos para explorar os resultados da fPCA e resuma suas descobertas.\n\n\n\n\n\n\nExperimente o gráfico interativo dos resultados da fPCA (plot_shiny(fpca_res)) e interprete. (Módulo 3-3)\n\n\n\nVoltando a questão do início…\nQual é a associação entre a precipitação total anual e a curva diária de temperatura?\n\n\n\nExplore os gr´sficos interativos (colocar o link para a perguinta)\n\n\n\nQual é a associação entre os perfis de FA e as pontuações do teste PASAT em pacientes com esclerose múltipla em sua primeira visita? (regressão escalar em função)\n\n\n\nO conjunto de dados também inclui rcst - perfis de FA coletados do trato corticoespinhal direito. Como essas medidas se relacionam com as medidas de FA ao longo da CCA?\n\n\n\nAjuste um modelo linear funcional com FA ao longo da CCA como resposta e pontuações do teste PASAT como covariável; tente diferentes funções R. Discuta gráficos interativos dos resultados da regressão função-em-escalar.\n\n\n\nUsaremos o conjunto de dados de DTI para ilustrar dados funcionais observados longitudinalmente amanhã. Tente plotar vários perfis observados de um paciente com esclerose múltipla selecionado aleatoriamente (algo semelhante ao exemplo abaixo). (ainda tem que criar os gráficos)\n\n\n\n\n\n\nPlote a trajetória predita de FA para um paciente com esclerose múltipla diferente / um tempo de visita diferente.\n\n\n\nExplore os gráficos interativos."
  },
  {
    "objectID": "respostas.html#dia-01",
    "href": "respostas.html#dia-01",
    "title": "Resposta dos exercícios propostos",
    "section": "",
    "text": "Como variam as medidas de AF ao longo da CCA em indivíduos com esclerose múltipla?\nR: Apesar do grande “range” de valores de AF, a grande maioria das curvas pode ser bem representada pelo formato obtido com a média de todas as curvas (figura abaixo). Contudo principalmente na parte central da imagem podemos notar que o comportamento é muito mais errático. Essas variações nos valores de AF para o diferentes indivíduos podem estár associadas a diferentes localizações, extensão e gravidade da desmielinização do corpo caloso (CCA).\n\n\n\n\n\n\n\n\n\n\n\n\nQual é a associação entre as medidas de anisotropia fracionada (fractional anisotropy, FA) e as funções cognitivas (pontuações do teste PASAT)?\nÉssa é uma pergunta que pode ser respondida de modo mais efetivo ao utilizarmos as técnicas que serão apresentadas mais adiante no curso. Contudo, vamos fazer uma primeira abordagem visual.\n\n\n\n\n\n\n\n\n\nAtravés da figura acima podemos notar que notas mais baixas de PASAT estão relacionadas a valores mais baixos de AF, principalemnte na parce central do corpo caloso.\n\n\n\nO perfil típico de anisotropia fracionada (fractional anisotropy, FA) varia entre pacientes com esclerose múltipla e indivíduos saudáveis?\nPara responder essa questão vamos dar uma olhada nos perfis dos pacientes com EM e nos pacientes saudáveis\n\n\n\n\n\n\n\n\n\nAtravés da comparação dos gráficos é fácil notar que apesar de apresentarem m comportamento médio pareceido (formato da curva), os valores de AF dos pacientes com EM são mais baixos (gráfico da esquerda) do que os pacientes saudáveis (gráfico da direita).\nConforme indicado na questão anterior, existe uma aparente correlação entre as notas de PASAT e os valores de AF. O que nos daria um bom modo de testar, mesmo que de modo indireto, se a diferença entre os valores de de AF para pacientes com e sem EM é significativa. Entretanto, infelizmente o conjunto de dados não apresenta as notas de PASAT para pacientes saudáveis. Assim sendo vamos utilizar os valores médios de AF para cada paciente.\n\nDTI4 &lt;- DTI %&gt;% \n  filter(visit == 1)\n\n#vamos dar uma olhada se as classes estão balanceadas\n\nfreq_relativa &lt;- DTI4 %&gt;%\n  count(sex, case) %&gt;%\n  mutate(freq_rel = n / sum(n))\nprint(freq_relativa)\n\n     sex case  n   freq_rel\n1   male    0 30 0.21126761\n2   male    1 66 0.46478873\n3 female    0 12 0.08450704\n4 female    1 34 0.23943662\n\n\n\nfreq_relativa %&gt;% \n  ggplot(aes(x = sex, y = freq_rel, fill = as.factor(case))) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Sex\", y = \"Proportion\", fill = \"Case\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEssas classes parecem um pouco desbalanceadas, vamos tentar melhorar isso.\n\n# Contagem das classes\ncontagem_classes &lt;- DTI4 %&gt;%\n  count(case,sex)\n\n# Identificar a classe majoritária e a classe minoritária\nclasse_majoritaria &lt;- contagem_classes %&gt;%\n  filter(n == max(n)) %&gt;%\n  pull(case)\n\nclasse_minoritaria &lt;- contagem_classes %&gt;%\n  filter(n == min(n)) %&gt;%\n  pull(case)\n\n# Subamostragem da classe majoritária para igualar o número de observações\ndados_balanceados &lt;- DTI4 %&gt;%\n  group_by(sex) %&gt;%\n  sample_n(size = max(contagem_classes$n), replace = TRUE) %&gt;%\n  ungroup()\n\n# Verificar a contagem das classes nos dados balanceados\ncontagem_classes_balanceadas &lt;- dados_balanceados %&gt;%\n  count(sex, case)\n\nprint(contagem_classes_balanceadas)\n\n# A tibble: 4 × 3\n  sex     case     n\n  &lt;fct&gt;  &lt;dbl&gt; &lt;int&gt;\n1 male       0    18\n2 male       1    48\n3 female     0    23\n4 female     1    43\n\n\n\nfreq_relativa2 &lt;- contagem_classes_balanceadas %&gt;%\n  count(sex, case) %&gt;%\n  mutate(freq_rel = n / sum(n))\n\n\nfreq_relativa2 %&gt;% \nggplot(aes(x = sex, y = freq_rel, fill = as.factor(case))) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Sex\", y = \"Proportion\", fill = \"Case\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNeste caso os grupos que queremos testar são independentes, ou você tem ou você não EM… Assim, o teste que me vem a mente para testar esa diferença entre os grupos é o teste t.\n\npacman::p_load(RVAideMemoire, car)\n\n\n#Obitendo os valores médios para cada ID\n\ndf_teste&lt;- dados_balanceados %&gt;% \n  mutate(\n    mean_cca = rowMeans(dados_balanceados$cca, na.rm = TRUE),\n    case = as_factor(case)\n  ) %&gt;% \ndplyr::select(ID,case,sex,mean_cca) \n\n#glimpse(df_teste)\n\n# Verificando a normalidade dos dados\nbyf.shapiro(mean_cca ~ case, df_teste)\n\n\n    Shapiro-Wilk normality tests\n\ndata:  mean_cca by case \n\n       W p-value\n0 0.9771  0.5683\n1 0.9896  0.6958\n\nbyf.shapiro(mean_cca ~ sex, df_teste)\n\n\n    Shapiro-Wilk normality tests\n\ndata:  mean_cca by sex \n\n            W p-value\nmale   0.9896  0.8585\nfemale 0.9760  0.2277\n\n\nNão encontramos evidencias para rejeitar as hipoteses nulas \\(H_0\\): distribuição dos dados = normal \\(p &gt; 0.05\\).\nVamos verificar a homogeneidade de variâncias\n\nleveneTest(mean_cca ~ case, df_teste)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value   Pr(&gt;F)   \ngroup   1  8.3346 0.004557 **\n      130                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nleveneTest(mean_cca ~ sex, df_teste)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  0.0359   0.85\n      130               \n\n\n\\(H_0\\): as variâncias dos grupos são homogêneas. Vamos seguir com o teste t\n\nt.test(mean_cca ~ case, df_teste, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  mean_cca by case\nt = 6.7243, df = 130, p-value = 5.051e-10\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.03829808 0.07022817\nsample estimates:\nmean in group 0 mean in group 1 \n      0.5504240       0.4961609 \n\nt.test(mean_cca ~ sex, df_teste, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  mean_cca by sex\nt = 0.46751, df = 130, p-value = 0.6409\nalternative hypothesis: true difference in means between group male and group female is not equal to 0\n95 percent confidence interval:\n -0.01308899  0.02118927\nsample estimates:\n  mean in group male mean in group female \n           0.5150404            0.5109903 \n\n\nAssim, temos indícios para acreditar que existe diferença entre as pessos que tem e que não tem EM (rejeitamos a hipótese nula) e não existe diferença entre os gêneros (não rejeitamos a hipótese nula).\n\np1&lt;- df_teste %&gt;% \n  ggplot(aes(x = factor(case), y = mean_cca)) +\n  geom_boxplot() +\n  xlab(\"Case\") +\n  ylab(\"Mean CCA\")\n\n\n\np2&lt;- df_teste %&gt;% \n  ggplot(aes(x = factor(sex), y = mean_cca)) +\n  geom_boxplot() +\n  xlab(\"sex\") +\n  ylab(\"Mean CCA\")\n\n\nplot_combinado2 &lt;- p1 + p2 + plot_layout(ncol = 1)\n\nprint(plot_combinado2)\n\n\n\n\n\n\n\n\nOs perfís variam sim entre individuos com e sem EM, e não variam entre pacientes de gênero distintos.\n\n\n\nDescreva as principais características dos perfis diários de temperatura.\n\n\n\n\n\n\n\n\n\nTodas as curvas tem um comportamento muito parecido, com um aparente comportamento ciclico (só temos um ano no gráfico) inverno-verão. Contudo apresentam um range de temperaturas dependendo da localidade da estação. Localidades mais quentes apresentam em média, durante o inverno valores um pouco abaixo de zero e um pouco maiores que 20° no verão. Equanto localidades mais frias apresentam valores inferiores a -30° no inversno e ~5°/10° graus no verão.\n\n\n\nQual é a associação entre a precipitação média anual e a temperatura diária em todo o Canadá?\nUm bom modo de responder essa questão sería através da utilização da técnica de regressão de escalar em função (SoF/prf) que ainda não vimos. Em breve voltaremos a essa questão.\n\n\n\nE quanto à associação entre as precipitações diárias e os perfis de temperatura?\nAssim como na questão anterior essa questão demanda uma técnica que ainda não foi apresentada, nesse caso Regressão de componentes principais funcionais (FPC) e Função em função - FoF (pffr).\n\n\n\nComo as contagens de CD4 variam ao longo do tempo na população, bem como no nível individual?\n\n\n\n\n\n\n\n\n\nEssa pergunta busca compreender como as contagens de células CD4 mudam ao longo do tempo em indivíduos com HIV em tratamento. As contagens de CD4 são um indicador-chave da saúde imunológica das pessoas infectadas pelo HIV, e monitorar essas contagens ao longo do tempo pode fornecer informações valiosas sobre a progressão da doença e a eficácia do tratamento.\nEm geral os dados apresentam um comportamento muito menos errático após o mes 0. Contudo, em nível individual o comportamento possui um range de valores bem grande.\n\n\n\nDescreva as principais direções nas quais as contagens de CD4 variam.\nNão entendi bem essa pergunta…\nOs dados apresentam comportamento muito diferentes entre os indivíduos.\n\n\n\nDescreva os gráficos - como a estratégia de corrida difere entre os corredores?\ncriar os gráficos e colocar um link para eles\n\n\n\nComo a velocidade varia ao longo de um percurso de uma maratona?\n\n\n\nQual é a relação entre o comportamento da velocidade durante a maratona e o tempo de chegada?\n\n\n\nQual é a melhor estratégia para terminar a maratona entre os cinco primeiros colocados?\n\n\n\nDescreva a variação do poluente em todo os Estados Unidos. (ainda não tem um dados para substituir)\n\n\n\nDescreva os principais modos de variação. (ainda não tem um dados para substituir)\n\n\n\nComo a altura varia em meninos e meninas?\n\n\n\nO gênero tem impacto no processo de crescimento de uma criança?\n\n\n\nPlote a taxa de crescimento de meninos e meninas e descreva-a.\n\n\n\nExplore os dados das curvas de crescimento de Berkeley\n\nPlote/descreva a função média.\nPlote/descreva a função de covariância.\nAs funções média e de covariância diferem entre mulheres e homens?\n\n\n\n\nReproduza os gráficos para os conjuntos de dados discutidos na terceira página. (Maratona,poluição,crescimento)"
  },
  {
    "objectID": "respostas.html#dia-02",
    "href": "respostas.html#dia-02",
    "title": "Resposta dos exercícios propostos",
    "section": "",
    "text": "Tente executar o código acima várias vezes. Experimente também executar o código com diferentes números de funções de base, por exemplo, nb &lt;- 5, nb &lt;- 15, nb &lt;- 30. (colocar o link para a pagina onde tem o código mncionado)\n\n\n\nAgora tente executar o código com nbasis = 13, nbasis = 27 e nbasis = 365. Qual é o efeito de usar diferentes números de bases? (colocar o link onde tem o código mencionado)\n\n\n\nTente este exercício com as medidas feitas em Quebec (l = 10). Você ainda escolhe K = 13 como o número ótimo de funções de base? E em relação a outras localidades? (colocar o link para a pagina onde está o código refetrente a pergunta)\n\n\n\nExperimente diferentes valores de λ (λ = 0.0001, λ = 100 e λ = 108) e discuta os resultados! (colocar o link do código)\n\n\n\nExistem várias maneiras de selecionar o λ ótimo; por exemplo, validação cruzada\n(colocar o link para a pagina onde tem a compração entre cross-validation e generalized cross-validation)\n\n\n\nExplorar as funcções do do pacote mgcv\nFunção ksmooth (alisador de regressão de kernel) Exemplo (atividade para fazer em casa): experimente um kernel gaussiano (“normal”) e diferentes larguras de banda (0.1, 10, 30).\n\n#fit = gam(y ~ s(t.day, k = 30, bs = \"cr\"), method = 'REML')\n# fit = ksmooth(t.day, y, kernel = c(\"box\"), bandwidth = 10)\n\n\n\n\nPágina 2: Experimente diferentes números de funções de base e ajuste OLS (atividade em grupo).\n\n\n\nPágina 3: Exercício de validação cruzada com medidas de precipitação transformadas em logaritmo feitas em Quebec.\n\n\n\nPágina 4: Experimente as funções gam e ksmooth; especialmente a função gam, pois a usaremos nos próximos capítulos!"
  },
  {
    "objectID": "respostas.html#dia-03",
    "href": "respostas.html#dia-03",
    "title": "Resposta dos exercícios propostos",
    "section": "",
    "text": "O que encontramos a partir dos resultados desta fPCA (análise de componentes principais funcionais)?\n\n\n\nQuanta variabilidade dos dados foi explicada pelo primeiro fPC (componente principal funcional)?\n\n\n\nQue característica das curvas o primeiro fPC (componente principal funcional) explica?\n\n\n\nTente fazer os gráficos para o segundo e terceiro fPCs (componentes principais funcionais) e seus efeitos em relação à média geral.\n\n\n\nDiscuta e interprete os resultados da fPCA (análise de componentes principais funcionais).\n\n\n\nQuantos CPs (componentes principais) são necessários para explicar 95% das variabilidades nos dados?\n\n\n\nWhat features of the underlying curves were captured by these PCs?\n\n\n\nExplore os gráficos interativos.\n\n\n\nAnalisar os dados da Maratona (dois aspectos: tempo decorrido transformado em logaritmo e tempo por milha)\n\nPlote / descreva os dados.\nPlote / discuta as estimativas das funções média e de covariância.\nUse gráficos interativos para explorar os resultados da fPCA e resuma suas descobertas.\n\n\n\n\nAnalisar os dados do Poluente (foco no nível de sulfato transformado em logaritmo)\nPlote / descreva os dados.\nPlote / discuta as estimativas das funções média e de covariância.\nUse gráficos interativos para explorar os resultados da fPCA e resuma suas descobertas."
  },
  {
    "objectID": "respostas.html#dia-04",
    "href": "respostas.html#dia-04",
    "title": "Resposta dos exercícios propostos",
    "section": "",
    "text": "Experimente o gráfico interativo dos resultados da fPCA (plot_shiny(fpca_res)) e interprete. (Módulo 3-3)\n\n\n\nVoltando a questão do início…\nQual é a associação entre a precipitação total anual e a curva diária de temperatura?\n\n\n\nExplore os gr´sficos interativos (colocar o link para a perguinta)\n\n\n\nQual é a associação entre os perfis de FA e as pontuações do teste PASAT em pacientes com esclerose múltipla em sua primeira visita? (regressão escalar em função)\n\n\n\nO conjunto de dados também inclui rcst - perfis de FA coletados do trato corticoespinhal direito. Como essas medidas se relacionam com as medidas de FA ao longo da CCA?\n\n\n\nAjuste um modelo linear funcional com FA ao longo da CCA como resposta e pontuações do teste PASAT como covariável; tente diferentes funções R. Discuta gráficos interativos dos resultados da regressão função-em-escalar.\n\n\n\nUsaremos o conjunto de dados de DTI para ilustrar dados funcionais observados longitudinalmente amanhã. Tente plotar vários perfis observados de um paciente com esclerose múltipla selecionado aleatoriamente (algo semelhante ao exemplo abaixo). (ainda tem que criar os gráficos)"
  },
  {
    "objectID": "respostas.html#dia-05",
    "href": "respostas.html#dia-05",
    "title": "Resposta dos exercícios propostos",
    "section": "",
    "text": "Plote a trajetória predita de FA para um paciente com esclerose múltipla diferente / um tempo de visita diferente.\n\n\n\nExplore os gráficos interativos."
  }
]